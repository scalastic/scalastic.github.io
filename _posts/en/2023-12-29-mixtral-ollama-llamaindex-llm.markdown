---
layout: post
title: "Mixtral 8x7B Locally: Train Your LLM with Your Own Data"
date: 2023-12-30 09:26:00 +0100
description: "Explore Mixtral 8x7B on MacOS with Ollama and LlamaIndex for personalized AI, covering installation, testing, and specialization."
img: mixtral-ollama-llamaindex-llm.jpg
fig-caption: Mistral AI in the spotlight according to <a href="#">DALLâ€¢E</a>
tags: [AI, LLM, Mixtral, Llama, Privacy]
lang: en
permalink: /mixtral-ollama-llamaindex-llm/
status: finished
---

The newly established French company Mistral AI has managed to position itself as a leading player in the world of
Artificial Intelligence. With its Large Language Model (LLM), Mixtral 8x7B, based on an innovative concept
of Mixture of Experts (MoE), it competes with giants like Meta and its Llama 2 70B model, as well as OpenAI and its famous ChatGPT
3.5. Mistral AI's adoption of the open-source Apache 2.0 license democratizes access to this cutting-edge technology,
allowing a broad range of users and developers to exploit and customize this technology according to
their specific needs.

Let's get hands-on with this model and see how to make the most of Mixtral 8x7B by customizing a LLM model
with our own data, locally, to preserve its confidentiality. This approach finally offers unprecedented flexibility and
reliability for developers and businesses seeking to integrate AI into their projects, all the while
maintaining complete control over their data.

<hr class="hr-text" data-content="Summary">

* TOC
{:toc}

<hr class="hr-text" data-content="Terminology">

## Understanding AI Jargon

Before diving further into our approach, it may be helpful to understand the terms we will use that
are at the core of the currently popular AI models:

- **LLM (Large Language Models)**: These are AI models designed to understand and generate natural language. They
  are based on vast datasets, with perhaps the most well-known being OpenAI's ChatGPT. However, there are many others 
  like Google's BERT, Meta's Llama, Hugging Face's BLOOM, Technology Innovation Institute's Falcon, and the one of our 
  interest today, Mixtral by Mistral AI.

- **RAG (Retrieval-Augmented Generation)**: This is a means of adding new information to an LLM or specializing it in a
  specific domain. It requires vectorial databases that allow the LLM to use these new pieces of information and provide
  more contextual responses.

- **LangChain**: This is a development framework dedicated to LLMs. It allows for the combination of a wide variety of
  language models including LLMs with external sources or user input components. It has become de facto the most used 
  open source framework in applications utilizing LLMs.

- **Token**: This represents the basic unit in language processing by AI models. It can represent a
  word, character, or part of a word like a phoneme, for example. Thus, it is this abstraction that LLM models 
  manipulate, and its size influences their capacity to analyze and generate language.

- **Mixture-of-Experts (MoE)**: This is a technique where an AI model is divided into specialized 'experts,' each
  handling a different part of the information. Depending on the context of the request, the most relevant expert is
  solicited, which allows for a more precise and tailored response. This approach improves the quality of
  the information generated by leveraging the specific skills of each expert.


<hr class="hr-text" data-content="Concepts">

## The Concepts Behind Mixtral 8x7B

**Mixtral 8x7B** is a Large Language Model (LLM) of the Mixture-of-Experts (MoE) type. It operates by directing each 
token to 2 out of 8 groups of experts that make up the model. The outputs from these experts are then combined to 
produce the final result, thus optimizing the processing and generation of the response.

Each expert within the Mixtral 8x7B model has about 7 billion parameters, which explains the model's name. When 
processing a request, Mixtral 8x7B uses only 12.6 billion parameters (approximately 2x7B), which speeds up
its processing and reduces the necessary resources. The specialization of each expert allows Mixtral 8x7B to
outperform larger models like Llama 2 70B (70 billion parameters), while being six times faster.
Moreover, it equals or surpasses GPT3.5 on most standard benchmarks.

Licensed under Apache 2.0, Mixtral 8x7B can be reused by developers, researchers, and companies,
thus fostering innovation and collaboration in the field of AI. This open license allows for extensive adaptation
and customization of the model, making the technology modifiable for a wide range of applications.


<hr class="hr-text" data-content="Installation">

## Installing Mixtral 8x7B

### Step 1: Installing Ollama

Previously, installing and operating an AI model on one's computer was a very complex task. However,
the introduction of Ollama, an open-source software, has significantly simplified this process. Indeed, Ollama allows users to
easily run advanced models such as Mixtral 8x7B, directly on their own systems, paving the way for
the democratization of these technologies.

To install Ollama on your computer:

- Go to the GitHub project and follow the instructions [https://github.com/jmorganca/ollama](https://github.com/jmorganca/ollama){:target="_blank" rel="noopener noreferrer nofollow"}.
- Or download the Ollama installation binary directly from [https://ollama.ai/download](https://ollama.ai/download){:target="_blank" rel="noopener noreferrer nofollow"}
  and start the installation on your computer.

### Step 2: Starting Mixtral 8x7B

To activate the Mixtral 8x7B neural network, run this command in your terminal:

{% highlight shell %}
ollama run mixtral
{% endhighlight %}

- During the first execution, Ollama will download the Mixtral 8x7B model, which is 26 GB in size. The download time
  will depend on your internet connection.
- It is necessary for your system to have at least 48 GB of RAM to efficiently run Mixtral 8x7B.

<hr class="hr-text" data-content="Benchmark Test">

## Testing the Intrinsic Capabilities of Mixtral 8x7B

In this first test, we will examine Mixtral's ability to generate Java code using the Spring
Boot 3.2 framework. This test will serve as a benchmark before specializing our LLM specifically for Spring Boot 3.2, thus
providing a point of comparison to evaluate the improvements made by specialization.

### Optional Step: Create a Python Virtual Environment

Depending on your preferences, you may create a virtual environment to isolate the test program and its dependencies.
There are several tools for creating Python virtual environments, including:

- **venv**: Integrated into Python 3.3 and later, it allows the creation of lightweight virtual environments.

- **virtualenv**: An older, independent tool that offers additional features compared to venv.

- **conda**: Particularly useful for managing environments that require complex dependencies, including
  non-Python libraries.

- **pipenv**: Combines pip and virtualenv for an improved dependency management workflow.

- **poetry**: Manages dependencies and virtual environments, focusing on ease of use and
  reproducibility.

With **_conda_**, to create a virtual environment named **_mixtral_ollama_** under **_Python 3.11_**, execute the commands:

{% highlight shell %}
conda create --name mixtral_ollama python=3.11
conda activate mixtral_ollama
{% endhighlight %}

### Step 3: Implementing the Test Application

This step involves creating two files. The first, `requirements.txt`, contains the necessary dependencies
for the application. The second, `reference_test.py`, is a Python script that sends a request to Mixtral 8x7B. We
will use the LlamaIndex library to facilitate the use of LLMs and, subsequently, the integration
of different data sources.

- **requirements.txt** :

{% highlight python %}
llama-index
qdrant_client
torch
transformers
{% endhighlight %}

- **reference_test.py** :

{% highlight python %}
from llama_index.llms import Ollama

llm = Ollama(model="mixtral")

prompt = (
"Create a basic Java class to start a Spring Boot 3.2 application. "
"The class should include the necessary annotations to define an entry point "
"for the Spring Boot application and should have a main method. "
"Also add a comment to briefly explain each annotation used."
)

response = llm.complete(prompt)
print(response)
{% endhighlight %}

### Step 4: Querying the Model

{% highlight bash %}
pip install requirements.txt
python reference_test.py
{% endhighlight %}

We receive the following response from Mixtral:

{% highlight bash %}
 ```java
package com.example;

import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;

@SpringBootApplication // (1)
public class Application {

    public static void main(String[] args) {
        SpringApplication.run(Application.class, args); // (2)
    }
}

// (1) @SpringBootApplication is a composite annotation consisting of three other annotations:
// - @Configuration indicates that the class contains bean definitions and configurations 
// for the Spring context.
// - @EnableAutoConfiguration allows Spring Boot to automatically configure the application 
// based on the dependencies present on the classpath.
// - @ComponentScan searches for and registers all components (classes annotated with @Component,
// @Service, @Controller etc.) in the current package and its sub-packages.
//
// (2) run(...) is a static method of SpringApplication that starts the execution of 
// the Spring Boot application using the specified configuration.
```

**Note:** Note: Spring Boot 3.2 has not yet been released, so this refers to a future version
and the annotations may change in future versions of Spring Boot. The above code
is based on the current version of Spring Boot (2.6.x).
{% endhighlight %}

Mixtral 8x7B generated code based on Spring Boot 2.6.x as this is the version available in the data that trained the 
model. Since Spring Boot version 3.2 was only recently released, it was not included in these data. This explains why 
the model has no knowledge of Spring Boot in version 3.x.

<hr class="hr-text" data-content="Specialization Test">

## Specializing Our Mixtral Model

Now, we will specialize our model. To do this, we will provide it with PDF documents containing
specific information related to the new targeted context, in our example, the [Spring Boot 3.2 reference documentation](https://docs.spring.io/spring-boot/docs/current/reference/htmlsingle/){:target="_blank" rel="noopener noreferrer nofollow"}.

### Step 5: Implementing the Model Specialization

- We will place the PDF file of the Spring Boot reference documentation in a `./data` subdirectory:

{% highlight bash %}
mkdir ./data
curl -o ./data/spring-boot-reference.pdf https://docs.spring.io/spring-boot/docs/3.2.1/reference/pdf/spring-boot-reference.pdf
{% endhighlight %}

- Add new dependencies to our project in the `requirements.txt` file:

{% highlight python %}
llama-index
qdrant_client
torch
transformers
pypdf
{% endhighlight %}

- Implement the specialization script to inject the PDF into the MixTral 8x7B model, `specialized_test.py`.
  The vector database, Qdrant, acts as a RAG for our model:

{% highlight python %}
from pathlib import Path
import qdrant_client
from llama_index import VectorStoreIndex, ServiceContext, SimpleDirectoryReader
from llama_index.llms import Ollama
from llama_index.storage.storage_context import StorageContext
from llama_index.vector_stores.qdrant import QdrantVectorStore

# Loading the documents from the disk
documents = SimpleDirectoryReader("./data").load_data()

# Initializing the vector store with Qdrant
client = qdrant_client.QdrantClient(path="./qdrant_data")
vector_store = QdrantVectorStore(client=client, collection_name="springboot")
storage_context = StorageContext.from_defaults(vector_store=vector_store)

# Initializing the Large Language Model (LLM) with Ollama
llm = Ollama(model="mixtral")
service_context = ServiceContext.from_defaults(llm=llm, embed_model="local")

# Creating the index, which includes embedding the documents into the vector store
index = VectorStoreIndex.from_documents(documents, service_context=service_context, storage_context=storage_context)

# Querying the index with a specific question
query_engine = index.as_query_engine()
prompt = (
"Create a basic Java class to start a Spring Boot 3.2 application. "
"The class should include the necessary annotations to define an entry point "
"for the Spring Boot application and should have a main method. "
"Also add a comment to briefly explain each annotation used."
)
response = query_engine.query(prompt)
print(response)
{% endhighlight %}

### Step 6: Querying the New Model

{% highlight bash %}
pip install requirements.txt
python specialized_test.py
{% endhighlight %}

We obtain a completely different response from Mixtral:

{% highlight bash %}
Here's a basic Java class for starting a Spring Boot 3.2 application with the necessary annotations and
a `main` method. I've also added comments to briefly explain each annotation used:

```java
package com.example.myapplication;

import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;

/**
 * This is the main class for our Spring Boot application. It includes the following annotations:
 * 1. @SpringBootApplication: This is a convenience annotation that combines the functionality of
 *    @Configuration, @EnableAutoConfiguration, and @ComponentScan. It implicitly defines a base
 *    "search package" for certain items, such as Spring Data JPA entities.
 * 2. @EnableAutoConfiguration: This annotation is used to trigger the auto-configuration mechanism.
 *    It guesses and creates appropriate beans based on the classpath and the configuration settings.
 * 3. @ComponentScan: This annotation tells Spring to scan the package of this class for any Spring
 *    components, services, repositories, or configurations and register them as beans in the application context.
 */
@SpringBootApplication
public class MyApplication {

    /**
     * The main method is the entry point of our application. When we run the Java compiled bytecode,
     * it's actually executing this method which will initialize and run our Spring Boot application.
     * @param args command-line arguments passed to the application
     */
    public static void main(String[] args) {
        SpringApplication.run(MyApplication.class, args);
    }
}
```

Replace `com.example.myapplication` with your desired package structure for your project. You can 
also customize other settings and add additional beans or configuration based on your needs.
{% endhighlight %}

The response from the specialized model now provides a basic Java class for starting a
Spring Boot 3.2 application.


<hr class="hr-text" data-content="Conclusion">

## Conclusion

The combination of Mixtral 8x7B, Ollama, and LlamaIndex marks a significant advancement in customizing AI models
and the development of tailor-made applications, by merging technical power with ease of use. This synergy
not only enhances the protection of private data but also benefits from an open and
free license, thereby encouraging collaboration and innovation. This makes artificial intelligence more accessible and
adaptable to a variety of projects and users, democratizing its use in diverse contexts.
