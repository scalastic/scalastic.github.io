[
  
    {
      "title"    : "Edgar Morin and DevOps: Towards a More Complex Vision of Systems",
      "url"      : "/en/edgar-morin-devops-complexity-thinking/",
      "img"      : "PHBpY3R1cmU+CiAgPHNvdXJjZSBzcmNzZXQ9Ii9hc3NldHMvaW1nL2VkZ2FyLW1vcmluLWRldm9wcy1jb21wbGV4aXR5LXRoaW5raW5nLTMwMC1lY2YyOGE1MzkuYXZpZiAxLjB4LCAvYXNzZXRzL2ltZy9lZGdhci1tb3Jpbi1kZXZvcHMtY29tcGxleGl0eS10aGlua2luZy02MDAtZWNmMjhhNTM5LmF2aWYgMi4weCIgdHlwZT0iaW1hZ2UvYXZpZiI+CiAgPHNvdXJjZSBzcmNzZXQ9Ii9hc3NldHMvaW1nL2VkZ2FyLW1vcmluLWRldm9wcy1jb21wbGV4aXR5LXRoaW5raW5nLTMwMC01MDJkOGIyNzAuanBnIDEuMHgsIC9hc3NldHMvaW1nL2VkZ2FyLW1vcmluLWRldm9wcy1jb21wbGV4aXR5LXRoaW5raW5nLTYwMC01MDJkOGIyNzAuanBnIDIuMHgiIHR5cGU9ImltYWdlL2pwZWciPgogIDxpbWcgc3JjPSIvYXNzZXRzL2ltZy9lZGdhci1tb3Jpbi1kZXZvcHMtY29tcGxleGl0eS10aGlua2luZy0zMDAtNTAyZDhiMjcwLmpwZyIgYWx0PSJFZGdhciBNb3JpbiBhbmQgRGV2T3BzOiBUb3dhcmRzIGEgTW9yZSBDb21wbGV4IFZpc2lvbiBvZiBTeXN0ZW1zIj4KPC9waWN0dXJlPgo=",
      "isencoded" : "y",
      "tags"     : "DevOps, Complexity Thinking, Edgar Morin, Complex Systems, Systems Thinking",
      "content"  : "Over the past few years, DevOps has become an essential approach in the IT field. It aims to bridge the gap between development and operations to improve agility, quality, and the speed of software deliveries. However, in many organizations, it is still reduced to a mere implementation of tools such as continuous integration and deployment (CI/CD), orchestration with Kubernetes, or infrastructure as code (IaC). This technical and mechanistic vision of DevOps overlooks a crucial dimension: the complexity of human and technical dynamics within DevOps. Indeed, a DevOps transformation is not just about automation or tools but also about team organization, culture, interaction management, and adaptation to unforeseen events. A simplistic approach can therefore lead to resistance to change, hidden silos persisting, or even SRE teams burning out due to unrealistic expectations regarding stability and speed. To better address these challenges, it is necessary to adopt a more global and systemic approach. Complexity thinking, developed by Edgar Morin, provides a framework for better understanding DevOps dynamics by considering interactions, uncertainties, and feedback loops. In this article, we will explore the fundamental principles of complexity thinking and see how they can enhance and improve DevOps practices by bringing a more holistic and adaptive vision of systems. Complexity Thinking: A Necessity for DevOps Why Simplistic Thinking Fails in DevOps A Brief Introduction to Complexity Thinking What Complexity Thinking Can Bring to DevOps Dialogical Principle: Reconciling DevOps Tensions The Contribution of the Included Third Principle Organizational Recursion Principle: A DevOps Learning Loop Hologrammatic Principle: Each DevOps Team Influences the Whole Managing Uncertainty: DevOps as an Adaptive System Transversality and Interdisciplinarity: Redefining DevOps Culture Towards a DevOps More Aware of Its Complexity Conclusion and Perspectives Towards a More Evolving DevOps Culture And You? References Complexity Thinking: A Necessity for DevOps DevOps is often seen as a simple solution to complex problems. Many companies adopt it, thinking that simply setting up CI/CD pipelines, automating deployments, and using Kubernetes will yield all the benefits. This reductionist view of DevOps, while effective in the short term, obscures the real complexity of IT and organizational environments. Why Simplistic Thinking Fails in DevOps An overly simplistic approach to DevOps is based on three cognitive traps that can limit its effectiveness: Reductionism: Many reduce DevOps to tools and automated processes: “DevOps is just CI/CD.” In reality, DevOps is much more than a set of technical practices. It involves organizational transformation, a culture of collaboration, and efficient management of information flows between teams. Binary Division: Some still see DevOps as a strict alternative to traditional methods: “We either do DevOps or stick with traditional Ops.” This perspective rigidly opposes approaches that, in reality, often coexist. In many companies, DevOps practices are implemented progressively, integrating with existing models. The Illusion of Total Control: With the rise of automation and observability, one might believe that everything can be controlled: “Everything is automated, so everything will go smoothly.” However, even with well-established processes, uncertainty remains. Systems are dynamic, unpredictable, and interdependent. An approach that fails to account for this complexity often leads to disappointment when incidents occur despite all precautions. A Brief Introduction to Complexity Thinking To overcome these limitations, it is essential to adopt a more global and interconnected mindset. This is precisely what Edgar Morin, philosopher and sociologist, proposes through his theory of complexity thinking. Complexity thinking is based on two major principles: Interconnection: A phenomenon cannot be understood in isolation; it is part of a network of interactions where each element influences the others. Multidimensionality: A system cannot be reduced to a single explanation or discipline; it must be analyzed from multiple perspectives (technical, human, organizational, etc.). By applying these principles to DevOps, we understand that it is not just a technical methodology, but a living ecosystem, constantly evolving. In the next section, we will explore how the key concepts of complexity thinking can enhance DevOps and help better address its challenges. What Complexity Thinking Can Bring to DevOps Applying Edgar Morin’s complexity thinking to DevOps allows for a more nuanced and effective vision of IT and organizational systems. Rather than excessively simplifying DevOps dynamics, the goal is to recognize and leverage the richness of interactions within it. Here’s how several principles of complexity thinking can enhance DevOps practices. Dialogical Principle: Reconciling DevOps Tensions DevOps is based on collaboration between development (Dev) and operations (Ops) teams, which have historically had opposing objectives: Developers strive to deliver new features quickly, Ops teams aim to ensure system stability and reliability. These two approaches may seem contradictory, but in reality, they are complementary. What complexity thinking brings: Edgar Morin’s dialogical thinking teaches us that instead of merging these two logics into one, it is more relevant to acknowledge their coexistence and learn to orchestrate them. DevOps should not be about completely merging roles but about intelligently managing the tensions between speed and stability. The Contribution of the Included Third Principle A key concept that extends the dialogical principle is the thinking of the included third. Unlike binary logic, which opposes two conflicting elements, the thinking of the included third proposes a third way, which integrates these tensions to generate a new model. Why is this important in DevOps?In DevOps, this logic is essential because it helps us understand that automation does not replace humans but redefines their role. Rather than viewing automation as the elimination of manual tasks, it should be seen as a transformation of human work towards higher-value-added tasks. Concrete example: In Site Reliability Engineering (SRE) culture, there is a concept of “toil reduction”: Automation reduces repetitive tasks (toil) but does not make Ops teams obsolete. It enables engineers to focus on continuous improvement, optimization, and solving more complex problems. Thus, Dev and Ops are not simply two opposing or merging forces, but they generate a new hybrid model where each adopts new responsibilities. How to improve DevOps? Encourage a culture of learning in tension, where teams accept that their objectives may sometimes be opposed but are always interdependent, recognizing that compromises may be necessary between speed and reliability. Implement regular post-mortems, where every incident becomes an opportunity for collective improvement. Promote constant feedback between Dev and Ops to continuously adjust practices, learn from failures, and refine the balance between Dev and Ops. Integrate the thinking of the included third into how roles evolve with automation and new DevOps practices. Organizational Recursion Principle: A DevOps Learning Loop In a complex system, each element influences the whole and is, in turn, influenced by it. In DevOps, this means that practices constantly evolve based on observed outcomes. What complexity thinking brings: DevOps should be seen as a continuous learning cycle. A production incident, for example, should not be treated as a mere anomaly to fix but as a source of improvement for development and operations processes. Another key concept of complexity thinking is auto-eco-organization. Edgar Morin explains that a living system does not simply follow external rules but self-organizes in response to its environment. Why is this important in DevOps?Auto-eco-organization directly applies to DevOps: teams should not simply follow rigid top-down processes but rather adapt naturally to real-world constraints. The best DevOps organization is one that emerges organically from the teams themselves through experimentation and continuous learning, rather than a predefined structure imposed without consideration for local specificities. How to improve DevOps? Establish short feedback loops, where each encountered issue feeds into a continuous improvement cycle. Foster a culture of self-organization within teams, where decisions are made based on past experiences rather than fixed rules. Promote frequent retrospectives on deployments and incidents to adjust practices in real time. Encourage an organic adaptation of processes, giving teams the freedom to experiment and optimize their workflows based on real-world needs. Hologrammatic Principle: Each DevOps Team Influences the Whole A complex system cannot be reduced to the sum of its parts: each element contains a representation of the whole and can have a global impact. This means that a single team or a single technical decision can have repercussions far beyond its immediate scope, as every part of the system influences the whole. What complexity thinking brings: In a DevOps organization, the choices of a single team can affect the entire system. A persistent silo within one team can slow down the entire delivery pipeline, and a poorly implemented tool like Kubernetes can introduce large-scale issues. How to improve DevOps? Encourage a collective vision through cross-functional teams where everyone understands their role and the broader impact of their decisions. Develop and reinforce a culture of transparency and communication, where technical choices are shared and debated collectively to avoid hidden silos. Involve developers, Ops, and SREs in global decision-making rather than confining them to their specific areas. Managing Uncertainty: DevOps as an Adaptive System Modern systems cannot be controlled rigidly. Their unpredictability requires adaptive and resilient strategies. Incidents and failures are inevitable. What complexity thinking brings: Complexity thinking encourages embracing uncertainty as a fundamental characteristic rather than an anomaly to eliminate. It pushes for the development of systems that do not aim to avoid failures but rather resist and adapt quickly when they occur. How to improve DevOps? Foster a culture of resilience, where teams see failures as learning opportunities. Adopt Chaos Engineering practices to test system resilience by simulating failures. Organize Game Days, where teams face crisis scenarios to better anticipate the unexpected. Promote a blameless culture, where post-mortems focus on learning rather than assigning blame. Transversality and Interdisciplinarity: Redefining DevOps Culture DevOps is sometimes reduced to a technical methodology, but it is above all a cultural transformation that involves human, organizational, and cognitive aspects just as much as tools. It blends technology, organization, psychology, and change management. What complexity thinking brings: Complexity thinking emphasizes that a system cannot be understood from a single discipline or perspective. Applying this approach to DevOps helps avoid an overly technocentric view and considers human dynamics, organizational constraints, and the impact of technical decisions on company culture. How to improve DevOps? Train teams in the principles of systems thinking and complex interactions, alongside technical skills. Encourage regular exchanges between Dev, Ops, Sec, and Business teams to better understand each other’s expectations and constraints. Value soft skills, such as communication and collaborative problem-solving, as much as technical skills. Towards a DevOps More Aware of Its Complexity Applying complexity thinking to DevOps helps overcome simplistic and mechanistic views that often limit its effectiveness. Rather than seeking ready-made solutions, it encourages considering all interactions, embracing uncertainty, and fostering a culture of continuous learning. Conclusion and Perspectives DevOps is much more than just a technical approach focused on automation and tools. It is a dynamic ecosystem where technical, organizational, and human aspects intersect. Yet, it is often approached too simplistically, which can limit its effectiveness and lead to failures. In reality, Edgar Morin’s complex thinking does not fundamentally change DevOps but rather reveal and strengthen a dimension that is already intrinsic to it. By nature, DevOps relies on team interconnection, continuous adaptation, and uncertainty management, all of which resonate with complexity thinking. However, in practice, these aspects are sometimes overlooked or neglected in favor of a more mechanistic view focused solely on tools and automation. Refocusing on this complexity allows for a more holistic and effective approach to DevOps systems. It also helps avoid common pitfalls, such as reducing DevOps to a mere technology stack, rigidly separating roles, or maintaining the illusion of total control over infrastructure and processes. Towards a More Evolving DevOps Culture To fully embrace complexity thinking, organizations must foster a culture of continuous learning, where experimentation, adaptation, and collaboration are at the heart of practices. This involves: Integrating feedback loops at all levels (development, infrastructure, security, users). Accepting uncertainty as a normal component of the system, with practices like Chaos Engineering and blameless post-mortems. Breaking down silos by encouraging regular exchanges between Dev, Ops, Sec, and Business teams for better mutual understanding. And You? How do you apply complexity thinking in your DevOps practices? What mechanisms have you implemented to manage uncertainty, encourage collaboration, and avoid the pitfalls of a rigid approach? By applying Edgar Morin’s insights on complexity thinking, we can strengthen DevOps’ ability to become a truly adaptive system, capable of continuously evolving in response to technological and organizational challenges. References Introduction à la pensée complexe Edgar Morin Collection Points Essais, Éditions Points 2014, 160 pages, Paperback, French ISBN: 978-2757842003 On Complexity Edgar Morin Hampton Press 2008, 127 pages, Paperback, English ISBN: 978-1572738010 Edgar Morin French philosopher and sociologist, born in Paris in 1921 under the name Edgar Nahoum, into a family of Greek Jewish origin from Salonika. He adopted the pseudonym Morin in 1942 when he joined the communist resistance. The following year, he became a commander of the Fighting French Forces. After the war, he distanced himself from the Communist Party and joined the CNRS in 1950. As an emeritus research director and honorary doctor of twenty-seven universities worldwide, he is the author of a widely commented and translated transdisciplinary work, including the ambitious Method, a six-volume series published by Seuil."
    } ,
  
    {
      "title"    : "Enough is Enough? Quit X, Switch to BlueSky",
      "url"      : "/en/quit-x-twitter-install-bluesky/",
      "img"      : "PHBpY3R1cmU+CiAgPHNvdXJjZSBzcmNzZXQ9Ii9hc3NldHMvaW1nL3F1aXQteC10d2l0dGVyLWluc3RhbGwtYmx1ZXNreS0zMDAtZGYyYTRhYmI2LmF2aWYgMS4weCwgL2Fzc2V0cy9pbWcvcXVpdC14LXR3aXR0ZXItaW5zdGFsbC1ibHVlc2t5LTYwMC1kZjJhNGFiYjYuYXZpZiAyLjB4IiB0eXBlPSJpbWFnZS9hdmlmIj4KICA8c291cmNlIHNyY3NldD0iL2Fzc2V0cy9pbWcvcXVpdC14LXR3aXR0ZXItaW5zdGFsbC1ibHVlc2t5LTMwMC1lZDI2NDQ1MmYuanBnIDEuMHgsIC9hc3NldHMvaW1nL3F1aXQteC10d2l0dGVyLWluc3RhbGwtYmx1ZXNreS02MDAtZWQyNjQ0NTJmLmpwZyAyLjB4IiB0eXBlPSJpbWFnZS9qcGVnIj4KICA8aW1nIHNyYz0iL2Fzc2V0cy9pbWcvcXVpdC14LXR3aXR0ZXItaW5zdGFsbC1ibHVlc2t5LTMwMC1lZDI2NDQ1MmYuanBnIiBhbHQ9IkVub3VnaCBpcyBFbm91Z2g/IFF1aXQgWCwgU3dpdGNoIHRvIEJsdWVTa3kiPgo8L3BpY3R1cmU+Cg==",
      "isencoded" : "y",
      "tags"     : "HelloQuitX, Bluesky, X, Twitter, Tutorial, Personal Data Server, We'reNotSheep, EnoughIsEnough",
      "content"  : "Since the acquisition of Twitter by Elon Musk in October 2022, renamed X, the platform has undergone profound structural and strategic changes. Among these are drastic staff reductions, particularly impacting teams responsible for content moderation and security. This lack of oversight has led to a significant rise in problematic content, seemingly amplified by new algorithms. These include hate speech, misinformation, and conspiracy theories. The situation worsened during Donald Trump’s presidential campaign in November 2024. Musk, who publicly supported Trump by investing $118 million in his campaign, became a prominent figure promoting far-right ideologies and misinformation. As a result, many public figures, media outlets, and institutions decided to leave the platform, condemning its amplification of harmful content and an environment increasingly at odds with their values. Meta (Facebook, Instagram, WhatsApp, and Threads)The CEO of Meta, Mark Zuckerberg, also announced the end of the fact-checking program in the United States, replacing it with a community notes system similar to X’s. Additionally, Zuckerberg expressed a desire to reintroduce more “masculine energy” in the workplace, criticizing what he perceived as excessive diversity and inclusion policies. These initiatives seem to align Meta with the policies of the Trump administration, signaling closer ties with the government. Alternatives: Bluesky and Mastodon Bluesky: Initiated in 2019 by Jack Dorsey, co-founder of Twitter, Bluesky became an independent company in 2021. It is built on the AT Protocol, designed to create a decentralized social network while offering a unified user experience. Unlike Mastodon, Bluesky offers a more centralized interface with plans for future federation. This approach has attracted many users: the platform gained over 5 million sign-ups within five days of Trump’s election, reaching 27.6 million users by January 2025. Mastodon: Launched in 2016 by Eugen Rochko, Mastodon is a decentralized microblogging platform. It operates on the ActivityPub protocol, allowing users to join or create independent “instances” (servers) with their own rules and moderation. This structure fosters specific communities and greater user autonomy. A Growing Collective Trend The hashtag #HelloQuitX, along with variants like #HelloQuitteX in France, became a collective rallying cry to leave X following Trump’s election. Supported by academics, associations, and public figures, this movement has driven awareness campaigns encouraging users to migrate to more ethical social networks. A clear trend is emerging: many users leaving X are transitioning to Bluesky, a platform reminiscent of Twitter’s user experience before its recent changes. Bluesky also benefits from its strategic positioning as a decentralized alternative, thanks to its adoption of the AT Protocol. While temporarily maintaining a centralized structure to simplify the onboarding process, this compromise appeals to users deterred by the perceived complexity of Mastodon or Meta’s new policies. Additionally, Bluesky attracts users with its stricter moderation and a growing community that values respectful and inclusive interactions. Alternatives: Bluesky and Mastodon A Growing Collective Trend Bluesky PDS: A Technical Solution for Experts What is Bluesky PDS? Who is Bluesky PDS for? Step-by-Step Installation Guide for Bluesky PDS Prerequisites Step 1: DNS Configuration Step 2: Preparing the Server Step 3: Download the Installation Script Step 4: Run the Installation Script Step 5: Verifying the Installation 5.1. Check Docker Images 5.2. Check Server Logs 5.3. Verifying the PDS Service Startup Step 6: Verifying the Connection 6.1. Opening HTTP and HTTPS Ports 6.2. Checking the Server Health 6.3. Verifying WebSockets (Optional) Step 7: Creating a User Account Step 8: Configuring Email Sending 8.1. Choose an Email Sending Service 8.2. Configure Your PDS to Use SMTP 8.3. Ensure Network Access 8.4. Restart Your PDS Responsibilities as an Administrator 1. Keep Your PDS Updated 2. Keep the Operating System Updated 3. Monitor and Manage Performance 4. Ensure Server Security 5. Legal Responsibilities Conclusion Bluesky PDS: A Technical Solution for Experts Bluesky stands out for its innovative approach to decentralization, built on its AT Protocol (Authenticated Transfer). At the core of this architecture lies the Bluesky PDS (Personal Data Server), offering advanced users the ability to directly control their data and online presence. What is Bluesky PDS? The Bluesky PDS is a solution designed to independently host and manage your data while interacting with the Bluesky network. Unlike relying on a centralized instance, as is common with most social networks, Bluesky PDS allows technical users and organizations to: Control their personal data: By hosting a PDS server, users retain full ownership of their content and metadata, reducing reliance on a single platform. Customize their experience: PDS administrators can tailor moderation settings, dissemination algorithms, and interaction policies to meet their specific needs. Federate with other instances: By integrating the AT Protocol, each PDS can seamlessly interact with other servers, creating a decentralized yet interoperable ecosystem. Who is Bluesky PDS for? Bluesky PDS is primarily aimed at technical experts, such as developers, system administrators, and organizations seeking full control over their data and online interactions. Setting up a PDS requires technical expertise in deployment, hosting, and maintenance. A Simpler Alternative for Casual UsersFor users who prefer not to manage the technical aspects of a PDS, the easiest solution is to create an account directly on the official Bluesky platform via https://bsky.app/. This option provides an intuitive, ready-to-use experience, ideal for exploring the network with minimal effort. For advanced users or technical professionals interested in deepening their use of Bluesky and actively contributing to the decentralized ecosystem, this article will explore the steps necessary to install and configure a Bluesky PDS on your own infrastructure. Step-by-Step Installation Guide for Bluesky PDS Here is a step-by-step guide to installing Bluesky PDS on your server, based on the official Bluesky PDS repository: bluesky-social/pds&amp;nbsp;Public Prerequisites To install and configure Bluesky PDS, you will need the following: A server with a public IPv4 address or a public DNS name accessible from the Internet. Server administration access with root or sudo privileges. A valid domain name under your control, configured to point to your server. Basic knowledge of system administration and command-line tools. Recommended Hardware Resources Operating System Ubuntu 20.04/22.04 or Debian 11/12. Memory (RAM) 1 GB CPU Cores 1 Storage 20 GB SSD Architectures amd64, arm64 Number of Users 1-20 This minimal configuration is sufficient to host a small Bluesky PDS instance for personal or small community use. Step 1: DNS Configuration Point your domain name to your server’s IP address by setting up the appropriate DNS records with your domain registrar. Example: Suppose I have a domain example.com hosting multiple applications. I will create a subdomain pds.example.com to host my Bluesky PDS. For this, I will create the following DNS records pointing to my Bluesky PDS server’s IP address, e.g., 12.34.56.78: Name Type Value TTL pds.example.com A 12.34.56.78 600 *.pds.example.com A 12.34.56.78 600 Step 2: Preparing the Server Connect to your server via SSH: ssh &amp;lt;user&amp;gt;@&amp;lt;server_ip_address&amp;gt; Bash Update existing packages: sudo apt update &amp;amp;&amp;amp; sudo apt upgrade -y Bash Step 3: Download the Installation Script To download the Bluesky PDS installation script, run the following commands on your server via SSH. You can use either wget or curl to retrieve the installation script from the official Bluesky repository. Using wget: wget https://raw.githubusercontent.com/bluesky-social/pds/main/installer.sh Bash Or using curl: curl https://raw.githubusercontent.com/bluesky-social/pds/main/installer.sh &amp;gt;installer.sh Bash Step 4: Run the Installation Script Once the script is downloaded, run it as an administrator using the following command: sudo bash installer.sh Bash This script will automatically install all necessary dependencies and configure the required services to run your Bluesky PDS instance: Installation of Docker and Docker Compose. Downloading the Docker images for Bluesky PDS (3 images). Starting the Docker containers. Setting up systemd services to enable auto-start on server reboot. Configuring a Let’s Encrypt TLS certificate with automatic renewal via the Caddy web server. During execution, the script will prompt you for certain inputs: Enter your public DNS address: Following our example, you should enter pds.example.com. Adapt this to your specific domain name. Enter an admin email address: Provide a valid email address. This will be used by Let’s Encrypt to manage your TLS certificate (it can be from any domain, e.g., gmail.com, proton.me, etc.). Create a PDS user account? (y/N): Answer N. We will perform this action later. You should see the following output in the logs: /pds.service. * Downloading pdsadmin =============================================================================== PDS installation successful! ------------------------------------------------------------------------------- Check service status : sudo systemctl status pds Watch service logs : sudo docker logs -f pds Backup service data : /pds PDS admin command : pdsadmin Output Step 5: Verifying the Installation After completing the installation of Bluesky PDS, it is important to ensure your server is online and functioning correctly. Follow these steps to confirm your setup is operational: 5.1. Check Docker Images View the Docker images that were downloaded by running the following command: sudo docker ps Bash This should return output similar to: CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 36924479e72e caddy:2 &quot;caddy run --config …&quot; 23 hours ago Up 23 hours caddy a13a320572eb ghcr.io/bluesky-social/pds:0.4 &quot;dumb-init -- node -…&quot; 23 hours ago Up 23 hours pds 67b03b48e7ea containrrr/watchtower:latest &quot;/watchtower&quot; 23 hours ago Up 23 hours (healthy) watchtower Plaintext 5.2. Check Server Logs To diagnose potential issues or confirm proper functionality, you can view the Docker logs for your PDS server with the following command: sudo docker logs -f pds Bash The output should display logs similar to the following: {&quot;level&quot;:30,&quot;time&quot;:1737107986429,&quot;pid&quot;:7,&quot;hostname&quot;:&quot;PDS&quot;,&quot;name&quot;:&quot;pds&quot;,&quot;req&quot;:{&quot;id&quot;:372,&quot;method&quot;:&quot;GET&quot;,&quot;url&quot;:&quot;/xrpc/chat.bsky.convo.getLog&quot;,&quot;query&quot;:{},&quot;params&quot;:{},&quot;headers&quot;:{&quot;user-agent&quot;:&quot;Mozilla/5.0&quot;,&quot;accept&quot;:&quot;*/*&quot;,&quot;accept-encoding&quot;:&quot;gzip, deflate&quot;,&quot;accept-language&quot;:&quot;fr-FR&quot;,&quot;dnt&quot;:&quot;1&quot;,&quot;origin&quot;:&quot;https://bsky.app&quot;}},&quot;res&quot;:{&quot;statusCode&quot;:200,&quot;headers&quot;:{&quot;cache-control&quot;:&quot;private&quot;,&quot;content-length&quot;:&quot;36&quot;,&quot;content-type&quot;:&quot;application/json; charset=utf-8&quot;}},&quot;responseTime&quot;:2771,&quot;msg&quot;:&quot;request completed&quot;} {&quot;level&quot;:30,&quot;time&quot;:1737110436039,&quot;pid&quot;:7,&quot;hostname&quot;:&quot;PDS&quot;,&quot;name&quot;:&quot;pds&quot;,&quot;req&quot;:{&quot;id&quot;:381,&quot;method&quot;:&quot;GET&quot;,&quot;url&quot;:&quot;/&quot;,&quot;query&quot;:{},&quot;params&quot;:{},&quot;headers&quot;:{&quot;user-agent&quot;:&quot;Mozilla/5.0&quot;,&quot;accept&quot;:&quot;*/*&quot;,&quot;accept-encoding&quot;:&quot;gzip&quot;}},&quot;res&quot;:{&quot;statusCode&quot;:200,&quot;headers&quot;:{&quot;cache-control&quot;:&quot;private&quot;,&quot;content-length&quot;:&quot;126&quot;,&quot;content-type&quot;:&quot;text/plain; charset=utf-8&quot;}},&quot;responseTime&quot;:2,&quot;msg&quot;:&quot;request completed&quot;} Plaintext 5.3. Verifying the PDS Service Startup Check the status of the Bluesky PDS service to confirm it is running: sudo systemctl status pds Bash This should return: ● pds.service - Bluesky PDS Service Loaded: loaded (/etc/systemd/system/pds.service; enabled; preset: enabled) Active: active (exited) since Thu 2025-01-16 11:58:41 UTC; 24h ago Docs: https://github.com/bluesky-social/pds Process: 138062 ExecStart=/usr/bin/docker compose --file /pds/compose.yaml up --detach (code=exited, status=0/SUCCESS) Main PID: 138062 (code=exited, status=0/SUCCESS) CPU: 173ms Jan 16 11:58:34 PDS docker[138074]: Container caddy Creating Jan 16 11:58:34 PDS docker[138074]: Container watchtower Created Jan 16 11:58:36 PDS docker[138074]: Container caddy Created Jan 16 11:58:36 PDS docker[138074]: Container pds Starting Jan 16 11:58:36 PDS docker[138074]: Container watchtower Starting Jan 16 11:58:38 PDS docker[138074]: Container watchtower Started Jan 16 11:58:39 PDS docker[138074]: Container pds Started Jan 16 11:58:39 PDS docker[138074]: Container caddy Starting Jan 16 11:58:41 PDS docker[138074]: Container caddy Started Jan 16 11:58:41 PDS systemd[1]: Finished pds.service - Bluesky PDS Service. Plaintext Step 6: Verifying the Connection 6.1. Opening HTTP and HTTPS Ports If not already configured, ensure that ports 80/tcp (HTTP) and 443/tcp (HTTPS) are open to allow access from the Internet. You can adjust these settings via your firewall or cloud provider. 6.2. Checking the Server Health Test the availability and health of your server by accessing the health check endpoint. Open a web browser and visit the following URL (adjust to your domain name): https://&amp;lt;pds.example.com&amp;gt;/xrpc/_health Bash If everything is functioning correctly, you should see a JSON response with the server version, for example: {&quot;version&quot;:&quot;0.4.74&quot;} Json 6.3. Verifying WebSockets (Optional) To ensure the Bluesky network can synchronize and retrieve content from your PDS, WebSockets must be functional. Here’s how to test them: Install a WebSocket testing tool, such as wsdump. Test WebSocket connectivity by running the following command: wsdump &quot;wss://&amp;lt;pds.example.com&amp;gt;/xrpc/com.atproto.sync.subscribeRepos?cursor=0&quot; Bash If WebSockets are correctly configured but no content has been created in your PDS yet, the command will keep running without displaying output. This indicates that the system is ready but currently has no content to transmit. Step 7: Creating a User Account To set up a user account on your PDS server, follow these steps via SSH: Run the following command: sudo pdsadmin account create Bash Enter the requested information at the prompt: Enter an email address (e.g. alice@pds.example.com): Enter a valid email address to be used for password recovery. This can belong to any domain (e.g., gmail.com, proton.me). Enter a handle (e.g. alice.pds.example.com): Enter a fully qualified username that will be linked to a subdomain of your PDS. The handle must end with your PDS server’s domain, such as .pds.example.com. Confirmation of creation: If everything is properly configured, you will see output similar to the following: Enter an email address (e.g. alice@pds.example.com): jeanjerome@my-email.com Enter a handle (e.g. alice.pds.example.com): jeanjerome.pds.example.com Account created successfully! ----------------------------- Handle : jeanjerome.pds.example.com DID : did:plc:livr8wtor8vyxsa4w064e4fs Password : xuNPXGl8d86Lmky750r2EozC ----------------------------- Save this password, it will not be displayed again. Plaintext ImportantImmediately save your password, as it will not be displayed again after account creation. In case of an errorIf you encounter an error like ERROR: Reserved handle, this indicates the chosen handle contains a reserved keyword blocked to prevent conflicts or abuse. You can find the full list of reserved keywords in the AT Protocol source file: reserved.ts. Step 8: Configuring Email Sending To enable your PDS to send emails (e.g., for verifying user email addresses or sending other notifications), you must configure an SMTP server. 8.1. Choose an Email Sending Service You can use a reliable email sending service, such as Resend or SendGrid. These services offer straightforward configuration and a ready-to-use API. Create an account with one of these services. Generate an API key, which will be used to authenticate email sending from your PDS. 8.2. Configure Your PDS to Use SMTP Edit the configuration file located at /pds/pds.env to include the following information (example using Resend): PDS_EMAIL_SMTP_URL=smtps://resend:&amp;lt;your API key&amp;gt;@smtp.resend.com:465/ PDS_EMAIL_FROM_ADDRESS=jeanjerome@my-email.com Plaintext PDS_EMAIL_SMTP_URL: The SMTP URL of the email sending service, including your API key. PDS_EMAIL_FROM_ADDRESS: The email address used as the sender for emails sent by your PDS. 8.3. Ensure Network Access Verify that your server allows outbound connections on the ports required by the SMTP service (typically port 465 for a secure connection). 8.4. Restart Your PDS After updating the configuration, restart your PDS to apply the changes: sudo systemctl restart pds Bash Responsibilities as an Administrator As the administrator of a PDS server, you are responsible for keeping your system up-to-date, monitoring its performance, and ensuring the security of your infrastructure. Below are the key tasks you should perform regularly. 1. Keep Your PDS Updated Bluesky regularly provides updates to fix bugs, enhance performance, and improve security. These updates are crucial for ensuring the smooth operation of your server. Update your PDS using the pdsadmin tool: sudo pdsadmin update Bash 2. Keep the Operating System Updated It is important to keep your server’s operating system up-to-date to benefit from the latest improvements and prevent security vulnerabilities. Apply system updates with the following command: sudo apt update &amp;amp;&amp;amp; sudo apt upgrade -y Bash 3. Monitor and Manage Performance To detect potential issues, regularly monitor your server’s logs and status. View the PDS service logs with systemd: sudo journalctl -u pds Bash View the PDS application logs: sudo docker logs -f pds Bash 4. Ensure Server Security Perform regular backups of critical data and configuration files located in /pds/. Restrict access to the server by configuring a firewall and using secure connections (SSH): Install tools such as ufw and fail2ban for additional protection. Monitor SSL/TLS certificate renewals to secure communications between users and your PDS. 5. Legal Responsibilities As a host, you are responsible for the content published through your server. Ensure compliance with local and international laws, particularly those related to data protection and combating illegal or harmful content. Conclusion Social networks have become central to our digital interactions, shaping communication, opinions, and the collective perception of information. The rise of alternatives like BlueSky, emphasizing ethics and decentralization, provides tangible solutions to issues such as toxicity and the concentration of digital power. However, these initiatives raise new questions: How can responsible use of these networks be ensured, especially when they are decentralized? Where does freedom of expression end, and how can it be reconciled with combating misinformation and hate speech? What regulations should be implemented to govern these new networks while respecting fundamental rights? How should responsibilities be distributed between users, administrators, and platform creators? What economic models can sustain these platforms while adhering to their ethical values? How can privacy and user control over data be guaranteed? Exploring these issues compels us to rethink our use of social networks and envision a more balanced digital ecosystem—one that respects users and defends democratic values currently under threat from autocratic regimes and opportunistic entrepreneurs."
    } ,
  
    {
      "title"    : "Day 2: Discovering Kubernetes and Setting Up Your First Cluster for the CKA Certification",
      "url"      : "/en/cka-certification-day-2-kubernetes/",
      "img"      : "PHBpY3R1cmU+CiAgPHNvdXJjZSBzcmNzZXQ9Ii9hc3NldHMvaW1nL2NrYS1kYXkyLWt1YmVybmV0ZXMtYXJjaGl0ZWN0dXJlLTMwMC1hZTAyZGM3NjAuYXZpZiAxLjB4LCAvYXNzZXRzL2ltZy9ja2EtZGF5Mi1rdWJlcm5ldGVzLWFyY2hpdGVjdHVyZS02MDAtYWUwMmRjNzYwLmF2aWYgMi4weCIgdHlwZT0iaW1hZ2UvYXZpZiI+CiAgPHNvdXJjZSBzcmNzZXQ9Ii9hc3NldHMvaW1nL2NrYS1kYXkyLWt1YmVybmV0ZXMtYXJjaGl0ZWN0dXJlLTMwMC0zYjFkYTFhNDkuanBnIDEuMHgsIC9hc3NldHMvaW1nL2NrYS1kYXkyLWt1YmVybmV0ZXMtYXJjaGl0ZWN0dXJlLTYwMC0zYjFkYTFhNDkuanBnIDIuMHgiIHR5cGU9ImltYWdlL2pwZWciPgogIDxpbWcgc3JjPSIvYXNzZXRzL2ltZy9ja2EtZGF5Mi1rdWJlcm5ldGVzLWFyY2hpdGVjdHVyZS0zMDAtM2IxZGExYTQ5LmpwZyIgYWx0PSJEYXkgMjogRGlzY292ZXJpbmcgS3ViZXJuZXRlcyBhbmQgU2V0dGluZyBVcCBZb3VyIEZpcnN0IENsdXN0ZXIgZm9yIHRoZSBDS0EgQ2VydGlmaWNhdGlvbiI+CjwvcGljdHVyZT4K",
      "isencoded" : "y",
      "tags"     : "Kubernetes, CKA, Cluster, DevOps, Certification, Kind, Kubectl, Tutorial",
      "content"  : "Kubernetes is the most widely used tool for orchestrating containers in Cloud Native and DevOps environments. It enables the management of application deployments while ensuring scalability, resilience, and optimized resource management. To lay a solid foundation for the CKA certification, it is essential to understand Kubernetes architecture and learn to configure your own cluster. This will help you master its components, practice and experiment, and meet the exam requirements. In this article, we will cover the fundamentals of Kubernetes and its use. You will learn to configure a Multi-node cluster with Kind, install and use the Kubernetes client, kubectl, and execute your first commands to interact with a cluster. These skills are an important first step in preparing for the certification. Day 2Welcome to the second step of your journey towards the Certified Kubernetes Administrator (CKA) certification. In this article, we will explore: The architecture of Kubernetes and the role of its main components. The specifics of the CKA exam, including available resources and the Kubernetes version used. The setup of a Multi-node cluster with Kind and the use of the kubectl tool. Initial practical commands to interact with your Kubernetes cluster. As with Day 1, this tutorial is designed to combine theory and practice to strengthen your technical skills and build confidence before the exam. 1. Introduction to Kubernetes Architecture 1.1. Main Components of the Master Node 1.2. Components of Worker Nodes 2. Overview of the Certified Kubernetes Administrator (CKA) Exam 2.1. Exam Structure and Objectives 2.2. Kubernetes Version Used 2.3. Accessible Documentation During the Exam 3. Configuring a Multi-node Kubernetes Cluster with Kind 3.1. Prerequisites 3.2. Installing Kind 3.2. Examples of Installation Based on Your Operating System 3.2.1. On Linux 3.2.2. On macOS 3.2.3. On Windows 3.2.4. Testing the Installation 3.3. Creating the Multi-node Cluster Step 1: Create a Configuration File for the Cluster Step 2: Determine the Kubernetes Image to Use Step 3: Create the Cluster from the Configuration File Step 4: Verify the Cluster Creation 1. List Active Kind Clusters 2. Display Node Details 3. Check Cluster Logs 4. Getting Started with kubectl 4.1. Download and Installation 4.1.1. Installation on Linux 4.1.2. Installation on macOS 4.1.3. Installation on Windows 4.1.4. Verify and Configure kubectl for the Kind Cluster 4.2. Basic Commands 4.2.1. Working with Contexts 4.2.1.1. Display the Current Configuration 4.2.1.2. Add a New Context 4.2.1.3. Delete an Existing Context 4.2.1.4. List Available Contexts 4.2.1.5. Display the Active Context 4.2.1.6. Set a Default Context 4.2.2. Checking Cluster Status 4.2.3. List Cluster Nodes 4.2.4. Display Available Namespaces 4.2.5. List Pods in a Namespace 4.2.6. Get Details of a Specific Resource 5. Conclusion Next Step 1. Introduction to Kubernetes Architecture Kubernetes Cluster Architecture The architecture of Kubernetes is designed to orchestrate containers by dividing responsibilities between two main types of nodes: Master Nodes and Worker Nodes. Master Nodes are responsible for managing and controlling the entire cluster, while Worker Nodes run containerized workloads. This role separation ensures system availability, resilience, and scalability. 1.1. Main Components of the Master Node The Master Node (also known as the Control Plane) is the brain of the Kubernetes cluster. It oversees all cluster operations and manages the desired states of deployed applications. Here are its main components: API Server: The entry point for all interactions with the cluster. Receives requests via the REST interface and forwards them to other components for processing. Also used by the kubectl client to interact with Kubernetes. etcd: A distributed key-value database that stores the cluster’s state, including configurations, metadata, and information about Kubernetes objects. Crucial for ensuring consistency and recovering the state in case of failures. Controller Manager: Consolidates multiple controllers responsible for the automated management of Kubernetes objects (e.g., ensuring a minimum number of pod replicas are running). Monitors the current cluster state and acts to maintain the desired state. Scheduler: Assigns newly created pods to available nodes based on defined resources and constraints (e.g., CPU, RAM, or node labels). Optimizes the utilization of cluster resources. 1.2. Components of Worker Nodes Worker Nodes are the machines where containers are executed. Each Worker Node contains several components responsible for managing local resources and running workloads: Kubelet: An agent that runs and monitors the pods on its node. Communicates with the API Server to receive instructions and report the status of the pods. Kube Proxy: A networking component that ensures communication between services and pods. Manages routing rules and load balancing for exposed services. Container Runtime: Software responsible for running containers on the node. Kubernetes supports various runtimes, including Docker, containerd, and CRI-O (any engine compatible with the Container Runtime Interface or CRI). Ensures efficient isolation and execution of containers. Pods: Pods are the basic units of deployment in Kubernetes and group one or more containers that share the same network and volumes. Role of Kubelet: Kubelet ensures that the pods scheduled on the node are functioning correctly, interacts with the runtime to start/stop containers, and reports pod status to the API Server. Role of Kube Proxy: Kube Proxy manages network connectivity between pods and routes requests to the appropriate pods, balancing the load when necessary. What Does a Node Run On?Kubernetes nodes can be deployed on a variety of platforms, including physical machines, virtual machines, or containerized environments. Kubernetes is designed to be infrastructure-agnostic, enabling users to deploy their cluster in a heterogeneous environment. How Is High Availability Achieved in a Kubernetes Cluster?High Availability (HA) in Kubernetes is achieved through several strategies: Replication of Master Nodes: Configuring multiple Master Nodes (a minimum of 3) reduces the risk of a single point of failure. These Master Nodes share responsibilities using etcd as a distributed database. Management of Worker Nodes: Kubernetes automatically detects Worker Node failures and redistributes the affected pods to other functional nodes. Load Balancing and Heartbeats: Mechanisms such as load balancers and “liveness probes” ensure that traffic is directed to operational components. 2. Overview of the Certified Kubernetes Administrator (CKA) Exam The Certified Kubernetes Administrator (CKA) exam assesses candidates’ practical skills in administering and managing Kubernetes clusters. It is designed to validate operational expertise in a real-world environment. 2.1. Exam Structure and Objectives The exam lasts 2 hours and focuses on practical scenarios. Candidates must solve real-world problems and perform tasks related to Kubernetes cluster management. The primary objectives include: Managing and configuring clusters. Networking and connectivity. Security, including Role-Based Access Control (RBAC). Resource management and application maintenance. Proper preparation and regular practice of commands are essential for success in this practice-oriented format. Complete InformationDetailed information can be found on the Linux Foundation’s dedicated page: https://training.linuxfoundation.org/certification/certified-kubernetes-administrator-cka-2/. 2.2. Kubernetes Version Used The Certified Kubernetes Administrator (CKA) exam relies on a specific Kubernetes version. It is highly recommended to verify this version on the official website before starting your preparation. This ensures your knowledge and practical exercises align with the features and behaviors of the version used in the exam. Since Kubernetes evolves rapidly, commands or functionalities may vary between versions, making this verification essential. Kubernetes Version for CKA 2025As of early 2025, and at least until February 10, the version used for the CKA exam is v1.31. To learn about the specific changes for the CKA 2025 exam, visit the dedicated page: Program Changes - CKA 2025. 2.3. Accessible Documentation During the Exam During the exam, you have access to specific online resources to assist you: Kubernetes Documentation: The official documentation, essential for looking up information about Kubernetes concepts, commands, and objects. Kubernetes Blog: Useful for news or relevant articles about Kubernetes, though rarely needed during the exam. Kubectl Reference: An indispensable resource for quickly finding command options and syntaxes for kubectl. These resources allow you to confirm or supplement your knowledge during the exam, but they must be used efficiently since time is limited. It is also worth noting that you will access this documentation, along with the exam content, through a virtualized environment. Preparing for this workflow is essential. 3. Configuring a Multi-node Kubernetes Cluster with Kind Kind (Kubernetes IN Docker) is a lightweight tool for deploying Kubernetes clusters for local development and testing. As its name suggests, Kind runs Kubernetes nodes (Master Nodes and Worker Nodes) as Docker containers on your machine. This eliminates the need for separate physical or virtual machines, making setup quick, portable, and ideal for test environments. This section provides a step-by-step guide to creating a Multi-node cluster using Kind. 3.1. Prerequisites Before starting, ensure that Docker is installed and that the Docker daemon is running on your machine. 3.2. Installing Kind There are several simple ways to install Kind on your machine: Using binaries: https://kind.sigs.k8s.io/docs/user/quick-start/#installing-from-release-binaries. Using a package manager: https://kind.sigs.k8s.io/docs/user/quick-start/#installing-with-a-package-manager. 3.2. Examples of Installation Based on Your Operating System 3.2.1. On Linux # For AMD64 / x86_64 [ $(uname -m) = x86_64 ] &amp;amp;&amp;amp; curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.26.0/kind-linux-amd64 # For ARM64 [ $(uname -m) = aarch64 ] &amp;amp;&amp;amp; curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.26.0/kind-linux-arm64 chmod +x ./kind sudo mv ./kind /usr/local/bin/kind Bash 3.2.2. On macOS brew install kind Bash ==&amp;gt; Downloading https://ghcr.io/v2/homebrew/core/kind/manifests/0.26.0 ############################################################################################################################################ 100.0% ==&amp;gt; Fetching kind ==&amp;gt; Downloading https://ghcr.io/v2/homebrew/core/kind/blobs/sha256:472a0a175ae63c92c8975fc202905dad51e248b4f398eed975df307f0bd14c5e ############################################################################################################################################ 100.0% ==&amp;gt; Pouring kind--0.26.0.arm64_sequoia.bottle.tar.gz ==&amp;gt; Caveats zsh completions have been installed to: /opt/homebrew/share/zsh/site-functions ==&amp;gt; Summary 🍺 /opt/homebrew/Cellar/kind/0.26.0: 9 files, 9MB ==&amp;gt; Running `brew cleanup kind`... Disable this behaviour by setting HOMEBREW_NO_INSTALL_CLEANUP. Hide these hints with HOMEBREW_NO_ENV_HINTS (see `man brew`). Plaintext 3.2.3. On Windows curl.exe -Lo kind-windows-amd64.exe https://kind.sigs.k8s.io/dl/v0.26.0/kind-windows-amd64 Move-Item .--kind-windows-amd64.exe c:--some-dir-in-your-PATH--kind.exe Powershell 3.2.4. Testing the Installation kind --version Bash You should see the installed version displayed: kind version 0.26.0 Plaintext 3.3. Creating the Multi-node Cluster Once Kind is installed, follow these steps to configure a Kubernetes cluster with multiple nodes. Step 1: Create a Configuration File for the Cluster A YAML configuration file is required to define your cluster topology. Below is an example configuration for a cluster with 1 Master Node and 2 Worker Nodes: Create a file named kind-cluster-config.yaml: kind: Cluster apiVersion: kind.x-k8s.io/v1alpha4 nodes: - role: control-plane - role: worker - role: worker Yaml Step 2: Determine the Kubernetes Image to Use By default, Kind uses the latest Kubernetes version packaged for the tool. However, for our CKA exam preparation, it is critical to use the same version as the one specified for the exam. Check the Kind release page to identify the appropriate image version. Look for the section listing pre-built images for the release. Example: Images pre-built for this release: - v1.32.0: kindest/node:v1.32.0@sha256:c48c62eac5da28cdadcf560d1d8616cfa6783b58f0d94cf63ad1bf49600cb027 - v1.31.4: kindest/node:v1.31.4@sha256:2cb39f7295fe7eafee0842b1052a599a4fb0f8bcf3f83d96c7f4864c357c6c30 - v1.30.8: kindest/node:v1.30.8@sha256:17cd608b3971338d9180b00776cb766c50d0a0b6b904ab4ff52fd3fc5c6369bf Markdown For this article, the version specified for the CKA exam in early 2025 is v1.31. We will use the following image: kindest/node:v1.31.4@sha256:2cb39f7295fe7eafee0842b1052a599a4fb0f8bcf3f83d96c7f4864c357c6c30 Plaintext Step 3: Create the Cluster from the Configuration File Use the following command to create the cluster, specifying the configuration file and the image for the selected version: kind create cluster -- --config kind-cluster-config.yaml -- --image kindest/node:v1.31.4@sha256:2cb39f7295fe7eafee0842b1052a599a4fb0f8bcf3f83d96c7f4864c357c6c30 -- --name multi-node-cluster Bash Explanation of the options: --config kind-cluster-config.yaml: Defines the cluster topology based on the configuration file. --image: Specifies the Docker image containing the exact Kubernetes version to use. --name multi-node-cluster: Assigns a name to the cluster for easier management. You should see the following logs displayed: Creating cluster &quot;multi-node-cluster&quot; ... ✓ Ensuring node image (kindest/node:v1.31.4) 🖼 ✓ Preparing nodes 📦 📦 📦 ✓ Writing configuration 📜 ✓ Starting control-plane 🕹️ ✓ Installing CNI 🔌 ✓ Installing StorageClass 💾 ✓ Joining worker nodes 🚜 Set kubectl context to &quot;kind-multi-node-cluster&quot; You can now use your cluster with: kubectl cluster-info --context kind-multi-node-cluster Thanks for using kind! 😊 Plaintext Your Multi-node Kubernetes cluster is now set up with the exact version used in the CKA exam, in this case, v1.31.4. Let’s now explore how to interact with the cluster to validate its functionality. Step 4: Verify the Cluster Creation At this point, you only need two tools to perform initial verifications: kind and the docker client. Since Kind uses Docker containers to run Kubernetes nodes, these tools will allow you to quickly check if the cluster is operational. 1. List Active Kind Clusters Use the following command to display the clusters created with Kind: kind get clusters Bash You should see output like this: multi-node-cluster Plaintext This confirms that your “multi-node-cluster” is active. 2. Display Node Details To verify the created nodes and their roles, execute: docker ps --filter &quot;name=multi-node-cluster&quot; Bash This command lists all Docker containers corresponding to the cluster nodes, showing their roles (control-plane or worker) and statuses. Example output: CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES e035ef879a00 kindest/node:v1.31.4 &quot;/usr/local/bin/entr…&quot; About an hour ago Up About an hour 127.0.0.1:51483-&amp;gt;6443/tcp multi-node-cluster-control-plane 6abe3d65c3a4 kindest/node:v1.31.4 &quot;/usr/local/bin/entr…&quot; About an hour ago Up About an hour multi-node-cluster-worker e7e120b8205a kindest/node:v1.31.4 &quot;/usr/local/bin/entr…&quot; About an hour ago Up About an hour multi-node-cluster-worker2 Plaintext Each container represents a node in your Kubernetes cluster. In this example, one control-plane node (control-plane) and two worker nodes (worker, worker2) have been created. 3. Check Cluster Logs You can also review Kind logs to diagnose potential issues during cluster startup: kind export logs --name multi-node-cluster Bash Example output: Exporting logs for cluster &quot;multi-node-cluster&quot; to: /private/var/folders/j9/bnfyqnns4tzccm9jnvrxzx100000gn/T/436485870 Plaintext This command exports the cluster logs to a local directory for detailed analysis: % ls -al /private/var/folders/j9/bnfyqnns4tzccm9jnvrxzx100000gn/T/436485870 total 16 drwx------@ 7 jeanjerome staff 224 3 jan 11:47 . drwx------@ 161 jeanjerome staff 5152 3 jan 11:47 .. -rw-r--r--@ 1 jeanjerome staff 1286 3 jan 11:47 docker-info.txt -rw-r--r--@ 1 jeanjerome staff 34 3 jan 11:47 kind-version.txt drwxr-xr-x@ 12 jeanjerome staff 384 3 jan 11:47 multi-node-cluster-control-plane drwxr-xr-x@ 12 jeanjerome staff 384 3 jan 11:47 multi-node-cluster-worker drwxr-xr-x@ 12 jeanjerome staff 384 3 jan 11:47 multi-node-cluster-worker2 Bash With these checks, you can ensure that your cluster has been configured correctly. For more detailed information and full interaction with the cluster, it is now necessary to install and configure the kubectl client. 4. Getting Started with kubectl kubectl is the official Kubernetes command-line client. It allows interaction with the cluster, management of resources, and retrieval of detailed cluster state information. This section guides you through installing kubectl and introduces basic commands to explore and manage your cluster. 4.1. Download and Installation The steps below provide examples for installing kubectl on different operating systems and configuring it to interact with a Kind cluster. There are multiple methods to install kubectl depending on your operating system and the package managers available. These examples reflect current practices and may evolve over time. For up-to-date information, refer to the official documentation: https://kubernetes.io/docs/tasks/tools/install-kubectl/. Here are some examples tailored for major operating systems: 4.1.1. Installation on Linux Download the kubectl binary: curl -LO https://dl.k8s.io/release/$(curl -Ls https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl Bash Make it executable: chmod +x ./kubectl Bash Move it to a directory included in your PATH: sudo mv ./kubectl /usr/local/bin/kubectl Bash 4.1.2. Installation on macOS Run the installation command: brew install kubectl Bash 4.1.3. Installation on Windows Check the latest stable version of the kubectl binary from https://dl.k8s.io/release/stable.txt. Download it, for example: curl -LO https://dl.k8s.io/release/v1.32.0/bin/windows/amd64/kubectl.exe Powershell Add the binary to your PATH. 4.1.4. Verify and Configure kubectl for the Kind Cluster Verify the installation in your shell: kubectl version --client Shell You should see output similar to the following (versions may vary): Client Version: v1.32.0 Kustomize Version: v5.5.0 Plaintext Kind automatically configures the kubeconfig file to allow kubectl to interact with the cluster. To verify that kubectl is properly configured: kubectl cluster-info Shell If the configuration is correct, you will see information about the cluster’s API Server and Control Plane: Kubernetes control plane is running at https://127.0.0.1:51483 CoreDNS is running at https://127.0.0.1:51483/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy To further debug and diagnose cluster problems, use &#39;kubectl cluster-info dump&#39;. Plaintext 4.2. Basic Commands Once kubectl is installed and configured, you can use the following commands to explore and interact with your cluster. 4.2.1. Working with Contexts For the CKA ExamIn Kubernetes, a context is a combination of cluster, user, and namespace. It is crucial to work with the correct context, especially during the CKA exam, where context-related errors can lead to wasted time or misconfigurations. Familiarize yourself with context management to avoid these issues. Here are some useful commands for working with contexts. 4.2.1.1. Display the Current Configuration kubectl config view Shell This command displays the complete current configuration, including clusters, users, and contexts defined in the kubeconfig file. Example output: % kubectl config view apiVersion: v1 clusters: - cluster: certificate-authority-data: DATA+OMITTED server: https://127.0.0.1:51483 name: kind-multi-node-cluster contexts: - context: cluster: kind-multi-node-cluster user: kind-multi-node-cluster name: kind-multi-node-cluster current-context: kind-multi-node-cluster kind: Config preferences: {} users: - name: kind-multi-node-cluster user: client-certificate-data: DATA+OMITTED client-key-data: DATA+OMITTED Plaintext 4.2.1.2. Add a New Context To add a new context, use the following command: kubectl config set-context &amp;lt;context-name&amp;gt; -- --cluster=&amp;lt;cluster-name&amp;gt; -- --user=&amp;lt;user-name&amp;gt; -- --namespace=&amp;lt;namespace&amp;gt; Shell For example: kubectl config set-context dev-cluster -- --cluster=cluster-dev -- --user=developer -- --namespace=development Shell 4.2.1.3. Delete an Existing Context Remove a specific context with the following command: kubectl config delete-context &amp;lt;context-name&amp;gt; Shell For example: kubectl config delete-context dev-cluster Shell 4.2.1.4. List Available Contexts Use this command to list all available contexts: kubectl config get-contexts Shell Example output: % kubectl config get-contexts CURRENT NAME CLUSTER AUTHINFO NAMESPACE * kind-multi-node-cluster kind-multi-node-cluster kind-multi-node-cluster production-cluster prod-cluster prod-user default dev-environment dev-cluster dev-user development Plaintext 4.2.1.5. Display the Active Context kubectl config current-context Shell Example output: % kubectl config current-context kind-multi-node-cluster Plaintext 4.2.1.6. Set a Default Context kubectl config use-context kind-multi-node-cluster Shell Example output: % kubectl config use-context kind-multi-node-cluster Switched to context &quot;kind-multi-node-cluster&quot;. Plaintext By configuring the context correctly, you ensure that all kubectl commands point to the intended cluster. 4.2.2. Checking Cluster Status Display general information about a specific cluster: kubectl cluster-info --context kind-multi-node-cluster Shell Example output: % kubectl cluster-info --context kind-multi-node-cluster Kubernetes control plane is running at https://127.0.0.1:51483 CoreDNS is running at https://127.0.0.1:51483/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy To further debug and diagnose cluster problems, use &#39;kubectl cluster-info dump&#39;. Plaintext 4.2.3. List Cluster Nodes Display the nodes in the cluster and their statuses: kubectl get nodes Shell Example output: % kubectl get nodes NAME STATUS ROLES AGE VERSION multi-node-cluster-control-plane Ready control-plane 139m v1.31.4 multi-node-cluster-worker Ready &amp;lt;none&amp;gt; 139m v1.31.4 multi-node-cluster-worker2 Ready &amp;lt;none&amp;gt; 139m v1.31.4 Plaintext 4.2.4. Display Available Namespaces List all namespaces in the cluster: kubectl get namespaces Shell Example output: % kubectl get namespaces NAME STATUS AGE default Active 140m kube-node-lease Active 140m kube-public Active 140m kube-system Active 140m local-path-storage Active 140m Plaintext 4.2.5. List Pods in a Namespace Check for any deployed pods in the default namespace: kubectl get pods Shell Example output: % kubectl get pods No resources found in default namespace. Plaintext Check if there are any pods deployed in the kube-system namespace: kubectl get pods -n kube-system Shell Example output: % kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE coredns-7c65d6cfc9-76brc 1/1 Running 0 143m coredns-7c65d6cfc9-dwrlq 1/1 Running 0 143m etcd-multi-node-cluster-control-plane 1/1 Running 0 143m kindnet-g4x7d 1/1 Running 0 143m kindnet-mzgc6 1/1 Running 0 143m kindnet-vjp2j 1/1 Running 0 143m kube-apiserver-multi-node-cluster-control-plane 1/1 Running 0 143m kube-controller-manager-multi-node-cluster-control-plane 1/1 Running 0 143m kube-proxy-cpbqm 1/1 Running 0 143m kube-proxy-kdnl2 1/1 Running 0 143m kube-proxy-lb2z2 1/1 Running 0 143m kube-scheduler-multi-node-cluster-control-plane 1/1 Running 0 143m Plaintext 4.2.6. Get Details of a Specific Resource Retrieve detailed information about a specific node: kubectl describe node &amp;lt;node-name&amp;gt; Shell Example output: % kubectl describe node worker-node-1 Name: worker-node-1 Roles: &amp;lt;none&amp;gt; Labels: beta.kubernetes.io/arch=amd64 beta.kubernetes.io/os=linux kubernetes.io/arch=amd64 kubernetes.io/hostname=worker-node-1 kubernetes.io/os=linux Annotations: kubeadm.alpha.kubernetes.io/cri-socket: unix:///run/containerd/containerd.sock node.alpha.kubernetes.io/ttl: 0 volumes.kubernetes.io/controller-managed-attach-detach: true CreationTimestamp: Fri, 01 Jan 2025 10:00:00 +0100 Taints: &amp;lt;none&amp;gt; Unschedulable: false Lease: HolderIdentity: worker-node-1 AcquireTime: &amp;lt;unset&amp;gt; RenewTime: Fri, 01 Jan 2025 14:00:00 +0100 Conditions: Type Status LastHeartbeatTime LastTransitionTime Reason Message ---- ------ ----------------- ------------------ ------ ------- MemoryPressure False Fri, 01 Jan 2025 14:00:00 +0100 Fri, 01 Jan 2025 10:00:00 +0100 KubeletHasSufficientMemory kubelet has sufficient memory available DiskPressure False Fri, 01 Jan 2025 14:00:00 +0100 Fri, 01 Jan 2025 10:00:00 +0100 KubeletHasNoDiskPressure kubelet has no disk pressure PIDPressure False Fri, 01 Jan 2025 14:00:00 +0100 Fri, 01 Jan 2025 10:00:00 +0100 KubeletHasSufficientPID kubelet has sufficient PID available Ready True Fri, 01 Jan 2025 14:00:00 +0100 Fri, 01 Jan 2025 10:00:37 +0100 KubeletReady kubelet is posting ready status Addresses: InternalIP: 192.168.1.10 Hostname: worker-node-1 Capacity: cpu: 4 ephemeral-storage: 500Gi hugepages-1Gi: 0 hugepages-2Mi: 0 hugepages-32Mi: 0 hugepages-64Ki: 0 memory: 8192Mi pods: 200 Allocatable: cpu: 4 ephemeral-storage: 500Gi hugepages-1Gi: 0 hugepages-2Mi: 0 hugepages-32Mi: 0 hugepages-64Ki: 0 memory: 8192Mi pods: 200 System Info: Machine ID: a1b2c3d4e5f67890abcdef1234567890 System UUID: 12345678-90ab-cdef-1234-567890abcdef Boot ID: 87654321-fedc-ba98-7654-3210fedcba98 Kernel Version: 6.1.0-20-generic OS Image: Ubuntu 22.04 LTS Operating System: linux Architecture: amd64 Container Runtime Version: containerd://1.6.12 Kubelet Version: v1.31.4 Kube-Proxy Version: v1.31.4 PodCIDR: 10.100.0.0/24 PodCIDRs: 10.100.0.0/24 ProviderID: kind://docker/cluster/worker-node-1 Non-terminated Pods: (2 in total) Namespace Name CPU Requests CPU Limits Memory Requests Memory Limits Age --------- ---- ------------ ---------- --------------- ------------- --- kube-system kindnet-abcdef 100m (5%) 100m (5%) 50Mi (2%) 50Mi (2%) 3h32m kube-system kube-proxy-xyz123 0 (0%) 0 (0%) 0 (0%) 0 (0%) 3h32m Allocated resources: (Total limits may be over 100 percent, i.e., overcommitted.) Resource Requests Limits -------- -------- ------ cpu 100m (5%) 100m (5%) memory 50Mi (2%) 50Mi (2%) ephemeral-storage 0 (0%) 0 (0%) hugepages-1Gi 0 (0%) 0 (0%) hugepages-2Mi 0 (0%) 0 (0%) hugepages-32Mi 0 (0%) 0 (0%) hugepages-64Ki 0 (0%) 0 (0%) Events: &amp;lt;none&amp;gt; Plaintext Retrieve details about a pod: kubectl describe pod &amp;lt;pod-name&amp;gt; Shell Example output: % kubectl describe pod kube-proxy-abcde -n kube-system Name: kube-proxy-abcde Namespace: kube-system Priority: 2000001000 Priority Class Name: system-node-critical Service Account: kube-proxy Node: worker-node-1/192.168.1.5 Start Time: Fri, 01 Jan 2025 09:00:00 +0100 Labels: controller-revision-hash=abcdef12 k8s-app=kube-proxy pod-template-generation=1 Annotations: &amp;lt;none&amp;gt; Status: Running IP: 192.168.1.5 IPs: IP: 192.168.1.5 Controlled By: DaemonSet/kube-proxy Containers: kube-proxy: Container ID: containerd://a1b2c3d4e5f67890abcdef1234567890abcdef1234567890abcdef1234567890 Image: registry.k8s.io/kube-proxy:v1.31.4 Image ID: docker.io/library/import-2024-12-16@sha256:0123456789abcdef0123456789abcdef0123456789abcdef0123456789abcdef Port: &amp;lt;none&amp;gt; Host Port: &amp;lt;none&amp;gt; Command: /usr/local/bin/kube-proxy --config=/var/lib/kube-proxy/config.conf --hostname-override=$(NODE_NAME) State: Running Started: Fri, 01 Jan 2025 09:00:01 +0100 Ready: True Restart Count: 0 Environment: NODE_NAME: (v1:spec.nodeName) Mounts: /lib/modules from lib-modules (ro) /run/xtables.lock from xtables-lock (rw) /var/lib/kube-proxy from kube-proxy (rw) /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-xyz12 (ro) Conditions: Type Status PodReadyToStartContainers True Initialized True Ready True ContainersReady True PodScheduled True Volumes: kube-proxy: Type: ConfigMap (a volume populated by a ConfigMap) Name: kube-proxy Optional: false xtables-lock: Type: HostPath (bare host directory volume) Path: /run/xtables.lock HostPathType: FileOrCreate lib-modules: Type: HostPath (bare host directory volume) Path: /lib/modules HostPathType: kube-api-access-xyz12: Type: Projected (a volume that contains injected data from multiple sources) TokenExpirationSeconds: 3607 ConfigMapName: kube-root-ca.crt ConfigMapOptional: &amp;lt;nil&amp;gt; DownwardAPI: true QoS Class: BestEffort Node-Selectors: kubernetes.io/os=linux Tolerations: op=Exists node.kubernetes.io/disk-pressure:NoSchedule op=Exists node.kubernetes.io/memory-pressure:NoSchedule op=Exists node.kubernetes.io/network-unavailable:NoSchedule op=Exists node.kubernetes.io/not-ready:NoExecute op=Exists node.kubernetes.io/pid-pressure:NoSchedule op=Exists node.kubernetes.io/unreachable:NoExecute op=Exists node.kubernetes.io/unschedulable:NoSchedule op=Exists Events: &amp;lt;none&amp;gt; Plaintext Ces commandes constituent une première base pour explorer et gérer votre cluster Kubernetes. Une maîtrise approfondie de ces outils vous aidera à progresser rapidement dans votre apprentissage et à éviter des erreurs majeures lors de l’examen. 5. Conclusion In this article, we explored the basics of configuring and interacting with a Kubernetes cluster. We covered key concepts such as Kubernetes architecture, setting up a cluster, installing the kubectl client, and using basic commands. These skills provide a foundation to deepen your understanding of Kubernetes and succeed in the CKA exam. Next Step The next article in this series will focus on Pods in Kubernetes, the smallest deployable unit in the system. You will learn what a Pod is, how to create, manage, and interact with it, while also discovering its relationships with other cluster resources. To progress in your learning, it is essential to practice regularly. Feel free to reproduce the commands and configurations from this article in a test environment. Continuous practice will enhance your mastery of the concepts and build your confidence for the exam. Happy learning, and see you soon for the next chapter in this series!"
    } ,
  
    {
      "title"    : "Day 1: Docker and Containers for the CKA Certification",
      "url"      : "/en/cka-certification-day-1-docker/",
      "img"      : "PHBpY3R1cmU+CiAgPHNvdXJjZSBzcmNzZXQ9Ii9hc3NldHMvaW1nL2NrYS1kb2NrZXItZnVuZGFtZW50YWxzLTMwMC05ODYwNzMxNTAuYXZpZiAxLjB4LCAvYXNzZXRzL2ltZy9ja2EtZG9ja2VyLWZ1bmRhbWVudGFscy02MDAtOTg2MDczMTUwLmF2aWYgMi4weCIgdHlwZT0iaW1hZ2UvYXZpZiI+CiAgPHNvdXJjZSBzcmNzZXQ9Ii9hc3NldHMvaW1nL2NrYS1kb2NrZXItZnVuZGFtZW50YWxzLTMwMC1lZTAwNDcyYmIuanBnIDEuMHgsIC9hc3NldHMvaW1nL2NrYS1kb2NrZXItZnVuZGFtZW50YWxzLTYwMC1lZTAwNDcyYmIuanBnIDIuMHgiIHR5cGU9ImltYWdlL2pwZWciPgogIDxpbWcgc3JjPSIvYXNzZXRzL2ltZy9ja2EtZG9ja2VyLWZ1bmRhbWVudGFscy0zMDAtZWUwMDQ3MmJiLmpwZyIgYWx0PSJEYXkgMTogRG9ja2VyIGFuZCBDb250YWluZXJzIGZvciB0aGUgQ0tBIENlcnRpZmljYXRpb24iPgo8L3BpY3R1cmU+Cg==",
      "isencoded" : "y",
      "tags"     : "Docker, Kubernetes, CKA, Containers, DevOps, Certification, Orchestration, Tutorial",
      "content"  : "Docker serves as the foundation upon which Kubernetes orchestrates containerized applications at scale. Mastering Docker is therefore an essential first step. Day 1Welcome to this series dedicated to preparing for the Certified Kubernetes Administrator (CKA) certification. This step-by-step training is designed to guide you in learning and mastering the foundational concepts and practical skills essential for earning this globally recognized certification. Throughout this series, you will explore key topics such as: Containerization with Docker and its role in the Kubernetes ecosystem. The basics of Kubernetes, including managing pods, services, and deployments. Kubernetes cluster administration, with a focus on high availability and security. Solving common challenges faced by Kubernetes administrators in production environments. Each tutorial is structured to provide a clear progression, blending theory and practice to strengthen your technical skills. Whether you are a beginner or already experienced, this series will equip you with the tools needed to excel in your CKA exam and your professional projects. Take the time to experiment with each step, and feel free to revisit key concepts if needed. Best of luck with your preparation and success on your journey toward the CKA certification! 1. Introduction to Docker 1.1. What is Docker? 1.2. Understanding Containers vs. Virtual Machines 1.3. Containers vs. Virtual Machines: An Analogy 1.4. Challenges of Non-Containerized Applications 1.5. How Docker Addresses These Challenges 1.6. A Simple Docker Workflow 1.7. Docker Architecture 2. Installing Docker 2.1. Using Docker Desktop (Comprehensive but Heavy Solution) 2.2. Installing Docker Components Individually (Recommended) 2.2.1. For Windows with WSL2 2.2.2. For Linux 2.2.3. For macOS with Docker CLI and Colima 2.3. Testing the Installation 3. Using Docker: Step-by-Step Tutorial 3.1. The Dockerfile (Basic Approach) 3.2. Issues with the Basic Approach 3.3. Multi-Stage Builds with Docker 3.3.1. Advantages of Multi-Stage Builds 3.3.2. How Multi-Stage Builds Work 3.3.3. Practical Implementation 3.4. Summary of Multi-Stage Build Advantages 4. Conclusion 4.1. When to Use Kubernetes 4.2. When Not to Use Kubernetes 1. Introduction to Docker 1.1. What is Docker? Docker is a powerful open-source platform designed to simplify the development, deployment, and execution of applications through containerization. Containers provide lightweight, portable, and isolated environments that package an application along with its dependencies. This ensures consistent behavior across various computing environments. Docker has become indispensable for modern software systems, offering unmatched speed, efficiency, and scalability. The simplicity and power of Docker have made it a revolutionary tool for development teams. It enables consistent workflows, whether you are coding on your laptop, testing in a CI/CD pipeline, or deploying to production clusters. 1.2. Understanding Containers vs. Virtual Machines Architecture of VMs vs Containers (source: Open Telekom) Both containers and virtual machines (VMs) provide isolated application environments, but their underlying mechanisms differ fundamentally: Virtual Machines (VMs): Operate via a hypervisor layer, such as VMware or VirtualBox, running a complete guest operating system (OS). Consume significant resources, as each VM includes its own OS kernel. Have relatively long startup times due to the full OS initialization process. Containers: Share the host OS kernel, eliminating the overhead of an additional guest OS. Lightweight, containing only the application and its dependencies. Feature fast startup times and minimal resource consumption, making them more efficient. 1.3. Containers vs. Virtual Machines: An Analogy To clarify the difference, consider the analogy of residential buildings: Virtual Machines: These are like standalone houses. Each house has its own foundation, walls, plumbing, and electrical system, much like a complete operating system. While isolated, they require significant resources to construct and maintain. Containers: These are akin to apartments in a shared building. Apartments share a common infrastructure (foundation, walls, utilities), similar to the host OS kernel. Each apartment is self-contained but lighter and faster to construct than a standalone house. 1.4. Challenges of Non-Containerized Applications Before the advent of containerization, applications faced several inherent challenges: Dependency Conflicts: Different applications often required incompatible versions of the same libraries or runtime environments, causing conflicts. Environmental Inconsistencies: Applications frequently worked on a developer’s local machine but failed in production due to environmental differences. Resource Inefficiency: Running multiple applications on the same machine required resource-intensive virtual machines or complex configurations. Scalability Limitations: Scaling or updating applications was labor-intensive, error-prone, and time-consuming. 1.5. How Docker Addresses These Challenges Docker effectively solves these systemic challenges: Isolation: Containers encapsulate all necessary dependencies, eliminating conflicts between applications. Portability: Containers ensure consistent application behavior across different environments—from development to testing to production. Efficiency: Sharing the host OS kernel reduces resource consumption and accelerates startup times. Scalability: Docker simplifies horizontal scaling by enabling rapid deployment of multiple containers derived from the same image. Now, let’s move from theory to practice and explore how to use Docker effectively. 1.6. A Simple Docker Workflow To understand Docker’s utility, let’s explore its basic workflow: Write a Dockerfile: Create a Dockerfile to define your container’s blueprint, specifying the base image, application code, and dependencies. Build an Image: Use docker build to compile an image from the Dockerfile. Run a Container: Use docker run to instantiate and run a container from the image, creating an isolated runtime environment. Push to a Registry: Save and share the image by pushing it to a container registry (e.g., Docker Hub) with docker push. Pull and Deploy: Download and deploy the image on another system using docker pull, ensuring consistent application behavior. 1.7. Docker Architecture Docker Architecture (source: Official Docker Documentation) The Docker architecture consists of several key components: Docker Client: The user interface to interact with Docker. Commands like docker build and docker run are sent to the Docker Daemon. Docker Daemon (dockerd): The core service that manages images, containers, and their lifecycle. It processes client requests and coordinates operations. Images: Immutable templates used to create containers. They encapsulate the application’s environment and dependencies. Containers: Lightweight instances of images that provide runtime environments for running applications. Docker Registry: A centralized repository to store and distribute Docker images (e.g., Docker Hub or private registries). Container Runtime: The underlying engine responsible for running containers. Docker’s runtime, such as containerd, ensures efficient container execution. This guide provides a foundational understanding of Docker’s capabilities and its transformative role in modern software development. As you delve deeper, experiment with Docker’s features to master containerization and simplify application deployment. 2. Installing Docker Before containerizing a project, ensure Docker is installed on your system. Here are the two main options: 2.1. Using Docker Desktop (Comprehensive but Heavy Solution) Advantages: Easy to install, with a graphical user interface and integrated tools. Disadvantages: Can be resource-heavy and installs components often unnecessary for CKA preparation. Licensing may also pose challenges in enterprise environments. Steps: Download Docker Desktop from docker.com. Install and launch Docker Desktop. 2.2. Installing Docker Components Individually (Recommended) Advantages: Lightweight installation, tailored for CKA preparation. Disadvantages: Requires a few extra steps on macOS and Windows. 2.2.1. For Windows with WSL2 Step 1 - Open PowerShell and install WSL2 by following the official Microsoft documentation. wsl --install Powershell Step 2 - Open the Ubuntu Linux distribution that has been installed and follow the Linux installation steps below. 2.2.2. For Linux Step 1 - Update your repositories: sudo apt update Bash Step 2 - Install Docker: sudo apt install docker.io Bash Step 3 - Enable and start Docker: sudo systemctl enable docker sudo systemctl start docker Bash Step 4 - Add your user to the Docker group (to avoid using sudo): sudo usermod -aG docker $USER Bash Restart your terminal to apply the changes. 2.2.3. For macOS with Docker CLI and Colima Step 1 - Install the Docker client using Homebrew: brew install docker Bash Step 2 - Install Colima using Homebrew: brew install colima Bash Why Colima? macOS requires Colima to run Docker, as its Darwin (BSD-based) kernel does not natively support Linux containerization features like namespaces and cgroups. Colima is a lightweight tool based on Lima that creates an optimized Linux virtual machine for running the Docker daemon, providing a compatible environment. Step 3 - Start Colima to initialize the Docker daemon: colima start -f Bash 2.3. Testing the Installation Run the following Docker command to test your installation: docker run hello-world Bash You should see a message indicating that everything is working, as shown below: Hello from Docker! This message shows that your installation appears to be working correctly. To generate this message, Docker took the following steps: 1. The Docker client contacted the Docker daemon. 2. The Docker daemon pulled the &quot;hello-world&quot; image from the Docker Hub. (arm64v8) 3. The Docker daemon created a new container from that image which runs the executable that produces the output you are currently reading. 4. The Docker daemon streamed that output to the Docker client, which sent it to your terminal. To try something more ambitious, you can run an Ubuntu container with: $ docker run -it ubuntu bash Share images, automate workflows, and more with a free Docker ID: https://hub.docker.com/ For more examples and ideas, visit: https://docs.docker.com/get-started/ Output By following these steps, you will have a fully configured and operational Docker environment, ready for local testing, development, or orchestration preparation with Kubernetes. Alternatives to DockerWhile Docker remains the go-to tool for containerization, many alternatives are now available. Among the main solutions that can replace either the Docker daemon or client, notable options include: Podman: A daemonless alternative with a Docker-compatible interface. Containerd: A lightweight container runtime used by Docker itself, which can also be used directly. CRI-O: A container runtime optimized for Kubernetes, adhering to the CRI interface. However, for CKA certification preparation, it is recommended to use Docker to ensure familiarity with the fundamentals and tools frequently mentioned in the documentation. 3. Using Docker: Step-by-Step Tutorial This tutorial will guide you through the process of containerizing a project with Docker. By the end of this guide, you will know how to create a Dockerfile, build and push a Docker image, and work with containers. 3.1. The Dockerfile (Basic Approach) Let’s start with a simple approach. This will help us understand the limitations, which we can compare to a more optimized approach later. Step 1 - Create a directory for your project: mkdir docker-c-app cd docker-c-app Bash Step 2 - Create a main.c file containing: #include &amp;lt;stdio.h&amp;gt; int main() { printf(&quot;Welcome to your Docker application!--n&quot;); return 0; } C Step 3 - Create an initial Dockerfile: # Use an image with GCC to compile and run the application FROM gcc:latest # Set the working directory WORKDIR /app # Copy the source file COPY main.c . # Compile the application RUN gcc -o app main.c # Define the default command CMD [&quot;./app&quot;] Dockerfile Dockerfile SyntaxFor a deeper understanding of Dockerfile syntax and concepts, refer to the official documentation: Dockerfile Reference. Step 4 - Build the Docker image: docker build -t c-app-basic:1.0.0 . Bash Command Explanation: docker build: This command creates a Docker image based on the instructions defined in the Dockerfile located in the current directory. -t c-app-basic: The -t option assigns a name (c-app-basic) and a tag (1.0.0) to the image. If no explicit tag is specified, Docker defaults to latest. .: This period indicates that the build context is the current directory, where Docker looks for the Dockerfile and other files needed to build the image. Once this command is executed, Docker reads the instructions in the Dockerfile, builds the image, and saves it locally on your system. Step 5 - Verify that the image was created: docker images Bash This command displays the list of Docker images available on your local system: % docker images REPOSITORY TAG IMAGE ID CREATED SIZE c-app-basic 1.0.0 70543dee1b46 4 minutes ago 1.39GB gcc latest d18c3b309576 5 months ago 1.39GB hello-world latest ee301c921b8a 20 months ago 9.14kB Output Look for the name c-app-basic in the list to confirm that the image was successfully created. Step 6 - Run the container: docker run --rm c-app-basic:1.0.0 Bash Command Explanation: docker run: Starts a new container from the specified image, in this case, c-app-basic:1.0.0. --rm: Indicates that the container will be automatically removed after execution. This helps keep your environment clean and prevents unused containers from accumulating. c-app-basic:1.0.0: The name and tag of the image from which the container is created. When you execute this command, Docker creates and starts a container based on the c-app-basic image with the 1.0.0 tag. Once the program inside the container completes, you will see the following message in your terminal: Welcome to your Docker application! Output After execution, the container will be automatically deleted due to the --rm option. Docker CLI Command ReferenceTo explore and master essential Docker CLI commands, check out the official cheat sheet: Docker CLI Cheat Sheet. 3.2. Issues with the Basic Approach The basic approach, while functional, has several major drawbacks: Excessive Image Size: The final image includes all tools required for compilation, such as gcc, along with other unnecessary libraries and dependencies. This significantly increases the image size, making it heavier to transfer and deploy. Increased Attack Surface: Compilation tools, while useful during the build process, are unnecessary in the final image. Their presence expands the potential attack surface, exposing the image to unnecessary vulnerabilities in a production environment. Not Optimized for Production: Best practices in production recommend including only the files necessary to run the application. In this approach, temporary build files and compilation tools add unnecessary complexity. To check the size of the image generated with this basic approach, use the following command: docker images | grep c-app-basic Bash The large size of this image (1.39GB) highlights the importance of optimizing the build process using techniques such as multi-stage builds, which will be explored in the next section. 3.3. Multi-Stage Builds with Docker Multi-stage builds are a technique that allows for the creation of optimized Docker images by separating the application build process from the final image creation. Each stage uses a specific image and contributes incrementally to the application build, but only the necessary components are included in the final image. 3.3.1. Advantages of Multi-Stage Builds Reduced Image Size: Compilation tools and other temporary files remain in intermediate stages and are not included in the final image. Improved Security: Excluding unnecessary tools like gcc or build libraries reduces the potential attack surface. Production Optimization: The final image is minimal, containing only what is necessary to run the application. 3.3.2. How Multi-Stage Builds Work The Dockerfile is structured into multiple stages. One stage can rely on another using the COPY --from=&amp;lt;stage_name&amp;gt; directive. For example: An initial stage, referred to as the build stage, includes all the tools needed to compile or assemble the project. A subsequent stage, called the final stage, retrieves only the files necessary to run the application and uses a lightweight base image, such as alpine or chainguard, to minimize the final image size. 3.3.3. Practical Implementation To address the issues mentioned with the basic approach, let’s use a multi-stage build for our application: Step 1 - Modify the Dockerfile to include multiple stages: # Build Stage FROM gcc:latest as builder # Set the working directory WORKDIR /app # Copy the source file COPY main.c . # Compile the application RUN gcc -o app main.c # Final Stage FROM chainguard/glibc-dynamic # Set the working directory WORKDIR /app # Copy the executable from the build stage COPY --from=builder /app/app . # Define the default command CMD [&quot;./app&quot;] Dockerfile Step 2 - Build the multi-stage image: docker build -t c-app-optimized:1.0.0 . Bash Step 3 - Run the container: docker run --rm c-app-optimized:1.0.0 Bash You will see the same output in the terminal: Welcome to your Docker application! Step 4 - Compare the Image Sizes: docker images | grep c-app Bash In my case, the output is: % docker images | grep c-app c-app-optimized 1.0.0 cd510de25230 17 minutes ago 9.56MB c-app-basic 1.0.0 70543dee1b46 About an hour ago 1.39GB Output You’ll notice that the optimized image is significantly smaller (99% smaller for this simple application) due to the use of a lightweight image (chainguard/glibc-dynamic) and the elimination of unnecessary compilation tools. 3.4. Summary of Multi-Stage Build Advantages Reduced Image Size: The optimized image is much smaller, making it faster to deploy. Improved Security: Compilation tools and unnecessary files are excluded from the final image. Modern Practices: Multi-stage builds represent a better approach for production environments. By comparing the two approaches, you can see how a multi-stage build simplifies container management while reducing risks and resource usage. 4. Conclusion Standalone containers, while useful for simple applications, pose challenges in terms of scalability, resource management, and orchestration when deployed in more complex systems. Managing networking, load balancing, and ensuring high availability manually becomes increasingly difficult as application complexity grows. Kubernetes addresses these challenges by automating the deployment, scaling, and management of containerized applications. It offers features like self-healing, load balancing, resource optimization, and declarative configuration, making it ideal for managing production environments. 4.1. When to Use Kubernetes Applications with a microservices architecture requiring orchestration. Environments demanding high scalability and fault tolerance. CI/CD pipelines for consistent testing and deployments. Multi-cloud or hybrid deployments to distribute workloads. Applications with dynamic traffic and resource demands. 4.2. When Not to Use Kubernetes Small, single-container applications. Prototypes or learning projects where simplicity is key. Systems with limited budgets or resources. Static workloads that do not require frequent updates or scalability. Real-time systems requiring ultra-low latency, where Kubernetes abstraction might introduce delays. By integrating tools like Docker with Kubernetes, you can create scalable and efficient environments tailored to your application’s needs. However, always evaluate the trade-offs to ensure Kubernetes aligns with your project’s scale and complexity."
    } ,
  
    {
      "title"    : "The Ultimate Guide to Mastering Hexagonal Architecture: Focus on the Domain",
      "url"      : "/en/hexagonal-architecture-domain/",
      "img"      : "PHBpY3R1cmU+CiAgPHNvdXJjZSBzcmNzZXQ9Ii9hc3NldHMvaW1nL2hleGFnb25hbC1hcmNoaXRlY3R1cmUtZG9tYWluLTMwMC1lYTAzN2VmZDYuYXZpZiAxLjB4LCAvYXNzZXRzL2ltZy9oZXhhZ29uYWwtYXJjaGl0ZWN0dXJlLWRvbWFpbi02MDAtZWEwMzdlZmQ2LmF2aWYgMi4weCIgdHlwZT0iaW1hZ2UvYXZpZiI+CiAgPHNvdXJjZSBzcmNzZXQ9Ii9hc3NldHMvaW1nL2hleGFnb25hbC1hcmNoaXRlY3R1cmUtZG9tYWluLTMwMC01MDVmMDI0MGUuanBnIDEuMHgsIC9hc3NldHMvaW1nL2hleGFnb25hbC1hcmNoaXRlY3R1cmUtZG9tYWluLTYwMC01MDVmMDI0MGUuanBnIDIuMHgiIHR5cGU9ImltYWdlL2pwZWciPgogIDxpbWcgc3JjPSIvYXNzZXRzL2ltZy9oZXhhZ29uYWwtYXJjaGl0ZWN0dXJlLWRvbWFpbi0zMDAtNTA1ZjAyNDBlLmpwZyIgYWx0PSJUaGUgVWx0aW1hdGUgR3VpZGUgdG8gTWFzdGVyaW5nIEhleGFnb25hbCBBcmNoaXRlY3R1cmU6IEZvY3VzIG9uIHRoZSBEb21haW4iPgo8L3BpY3R1cmU+Cg==",
      "isencoded" : "y",
      "tags"     : "Hexagonal-Architecture, Domain, Software-Design, Clean-Architecture, Craftsmanship",
      "content"  : "Although it has existed for many years, Hexagonal Architecture has been experiencing significant growth recently. At the core of this architecture is the Domain: it plays a central role by encapsulating business logic and ensuring a clear separation between functional and technical concerns. This article aims to guide you step-by-step in setting up the domain part of hexagonal architecture. We will address essential questions that every developer must ask to build a solid application domain: How to structure inbound and outbound ports? What is the role of business services and entities? How to handle exceptions and return types? What are the best practices for data validation and how can DTOs be used? By exploring these topics, we will present appropriate solutions and implementation choices that will allow you to build your domain with the right tools while adhering to the state of the art. This guide will provide you with the keys to mastering the design of an efficient and coherent domain within your application. 1. Definition of Inbound and Outbound Ports Inbound Ports (Application Interfaces) Outbound Ports (Technical Interfaces) Importance of These Distinctions Reason for Choosing This Structure 2. Exception Handling in the Domain The Domain and Business Errors Can the Domain Focus Solely on Business Errors? Domain’s Responsibility Regarding Errors in APIs and SPIs At the Inbound Ports (API) Level At the Outbound Ports (SPI) Level In Summary 3. Business Services in Hexagonal Architecture Positioning of Business Services within APIs and SPIs What Business Services Can Do What Business Services Should Not Do Advantages of Business Services 4. Using Business Entities Business Entities Different Possible Implementations 1. Java POJO (Plain Old Java Object) 2. Java Records 3. Lombok Recommendations Data Validation in Business Entities 5. Choosing Return Types for Methods Illustrative Scenarios Benefits of This Approach Best Practices 6. Data Validation Input Adapters (REST, UI, etc.) Business Service (Domain) Impacts for Other Components Advantages and Disadvantages 7. Role of DTOs in Hexagonal Architecture Why Use DTOs? Benefits of DTOs Disadvantages of DTOs In Summary 8. Organization of Domain Packages An Example of Package Structure for the “user” Use Case Class and Interface Details Conclusion - Beyond the Hexagon 1. Definition of Inbound and Outbound Ports In hexagonal architecture, ports define the interaction points between the domain’s business logic and the external layers. They are divided into two main categories: inbound ports and outbound ports. Inbound Ports (Application Interfaces) Inbound ports, represented by interfaces like UserApiPort, expose the operations the application offers to external layers. These ports define the use cases or application services that the system provides, such as createUser, findUserById, updateUser, and deleteUser. public interface UserApiPort { User createUser(User user); User findUserById(Long id); User updateUser(Long id, User user); void deleteUser(Long id); } Java Use of inbound port calls: Inbound ports act as application contracts between the domain and external adapters (e.g., REST controllers, user interfaces). They allow external layers to invoke business operations without needing to know the internal implementation details. By focusing on the application’s functional needs, they provide a clear interface for implementing defined use cases. Naming and responsibility distinctions: Inbound interfaces can be named with the ApiPort suffix, reflecting their role as application interfaces (API) for the provided operations. They focus on the functional logic and services the application provides to users. Handling return values and exceptions: Inbound port methods return business objects, such as User, or raise business exceptions when issues arise (e.g., ResourceNotFoundException, BusinessRuleViolationException). This allows external adapters to appropriately handle errors and provide clear responses to the application’s clients. Note The findUserById(Long id) method returns a User or raises a ResourceNotFoundException if the user does not exist. The createUser(User user) method raises a BusinessRuleViolationException if the user’s name is empty or null. Advantages: Functional Decoupling: Inbound ports isolate business logic from the technical details of external layers. Service Clarity: They explicitly define the available operations, making the application easier to understand and use. Disadvantages: Complex Initial Design: This requires a good understanding of use cases to define relevant interfaces. Outbound Ports (Technical Interfaces) Outbound ports, such as UserSpiPort, define how the domain interacts with external systems. They focus on the technical aspects necessary to carry out business operations, like database access or external services. public interface UserSpiPort { User saveUser(User user); Optional&amp;lt;User&amp;gt; findUser(Long userId); User updateUser(User user); void deleteUser(Long userId); } Java Use of outbound port calls: Outbound ports act as technical interfaces that the domain uses to perform its tasks, without concerning itself with concrete implementations. They allow delegation of technical operations to specialized adapters, while keeping the domain independent of specific technologies. Naming and responsibility distinctions: Outbound interfaces can be named with the SpiPort suffix, indicating their role as Service Provider Interface or SPI. They focus on the technical details required for the domain to function, without involving business logic. Handling return values and exceptions: Outbound port methods often return Optional&amp;lt;User&amp;gt;, reflecting technical uncertainty about the existence of a resource. They do not raise business exceptions, leaving it to the domain to decide how to handle cases where data is not available. NoteThe findUserById(Long id) method returns an Optional&amp;lt;User&amp;gt;, indicating that the user may or may not exist in the external system. Advantages: Technical Flexibility: Makes it easier to change the implementation of technical services without affecting the domain. Testability: Outbound ports can be easily mocked during unit tests, thereby isolating the business logic. Disadvantages: Need for Adequate Abstraction: Ports must be generic enough to avoid introducing technological dependencies into the domain. Importance of These Distinctions Consistent Error Handling: By separating responsibilities, the domain can decide how to handle cases of missing data (e.g., raising a business exception), while outbound ports manage technical uncertainties. Code Clarity: Developers can quickly understand the role of each interface based on its name and location within the project. Maintainability: This organization simplifies future modifications, whether adding new features or changing the technical implementation. Reason for Choosing This Structure Strong Decoupling: By clearly distinguishing between inbound and outbound ports, hexagonal architecture ensures decoupling between the application’s functional logic and technical implementation details. Adaptability: Allows for technical adapters to be modified or replaced without impacting the domain or application services. Consistency in Communication: External adapters interact with the domain via clear functional interfaces, while the domain uses well-defined technical interfaces to access external resources. 2. Exception Handling in the Domain The domain is meant to be independent of technical details and should focus on business logic. This raises the following question: should the domain handle only business exceptions, or can it also be concerned with certain technical errors? The Domain and Business Errors The domain is responsible for business logic and must handle situations where business rules are violated. To do this, specific domain exceptions should be defined, such as: ResourceNotFoundException: Thrown when a requested resource (such as a user) does not exist. BusinessRuleViolationException: Thrown when a business rule is violated, for instance, when a user attempts to register with an already-used email address. public class ResourceNotFoundException extends RuntimeException { public ResourceNotFoundException(String message) { super(message); } } Java public class BusinessRuleViolationException extends RuntimeException { public BusinessRuleViolationException(String message) { super(message); } public BusinessRuleViolationException(String message, Throwable cause) { super(message, cause); } } Java These exceptions allow the domain to clearly signal to the calling layers that a business rule violation has occurred, without exposing internal technical details. NoteThe use of RuntimeException (unchecked exceptions) simplifies the code by avoiding the explicit declaration of exceptions while allowing their automatic propagation to the adapters for centralized handling of business errors. Can the Domain Focus Solely on Business Errors? Ideally, the domain should concentrate exclusively on business errors. Technical errors, such as database, network, or input/output exceptions, should be handled by technical adapters (implementations of SPI ports). However, in practice, some technical errors can impact business logic and cannot be completely ignored by the domain. Examples of cases where the domain must consider technical errors: Unavailability of an essential external service: If a business operation depends on an external service (such as a payment system) and that service is unavailable, the domain must decide how to respond, for example by canceling the transaction and informing the user. Violations of technical constraints reflecting business rules: For example, a database uniqueness constraint violation might reflect a business rule of uniqueness that was not enforced earlier. Domain’s Responsibility Regarding Errors in APIs and SPIs At the Inbound Ports (API) Level Inbound ports, such as UserApiPort, define the use cases that the domain exposes to external adapters (such as REST controllers). Domain’s responsibilities: Raise business exceptions: When business rules are violated, the domain raises specific exceptions like BusinessRuleViolationException or ResourceNotFoundException. Provide clear returns: API port methods return business objects or raise business exceptions, allowing external adapters to handle errors appropriately. NoteThe domain should not propagate technical exceptions through inbound ports, but it may raise business exceptions (such as ResourceNotFoundException or BusinessRuleViolationException). External adapters capture these business exceptions and translate them into appropriate responses for clients (e.g., HTTP status codes like 404 Not Found or 409 Conflict in the case of a REST API). At the Outbound Ports (SPI) Level Outbound ports, such as UserSpiPort, define how the domain interacts with external systems (e.g., a database). Domain’s responsibilities: Handle technical uncertainties: SPI port methods may return Optional&amp;lt;User&amp;gt; to indicate that the user might not exist, without raising technical exceptions. Avoid managing technical exceptions: Adapters that implement the SPI should capture technical exceptions (such as SQLException or ConstraintViolationException) and transform them into results the domain can understand (e.g., Optional.empty()). NoteThe domain should be protected from technical exceptions arising from SPI adapters to maintain its independence from technical details. In Summary The domain: Manages business errors by raising specific exceptions. Must be informed of critical technical errors impacting business logic, without handling technical details. Does not propagate technical exceptions to external adapters. Technical adapters (SPI): Capture technical errors and transform them into results the domain can understand (e.g., Optional.empty()). Do not propagate technical exceptions to the domain. External adapters (API): Receive business exceptions from the domain and transform them into appropriate responses for clients (e.g., HTTP error codes). By adhering to these principles, exception handling in the domain remains consistent with the goals of hexagonal architecture: maintaining a clear separation between business logic and technical details, while ensuring robustness and resilience of the application in the face of various errors. 3. Business Services in Hexagonal Architecture In hexagonal architecture, business services encapsulate the application’s business logic. They orchestrate the operations necessary to carry out the defined use cases, relying on ports and adapters to interact with external systems and infrastructure layers. Positioning of Business Services within APIs and SPIs public class UserApiService implements UserApiPort { private final UserSpiPort userSpiPort; ... @Override public User addUser(User user) { return userSpiPort.saveUser(user); } @Override public User getUser(Long userId) { return userSpiPort.findUser(userId) .orElseThrow(() -&amp;gt; new ResourceNotFoundException(&quot;User not found: &quot; + userId)); } ... } Java Business services sit at the heart of the domain and interact with inbound (API) and outbound (SPI) ports: Inbound Ports (API): Business services implement the interfaces defined by API ports. These interfaces represent the use cases the application exposes to external adapters (e.g., REST controllers). Example: The UserApiService implements the UserApiPort interface, which defines operations such as createUser, findUserById, updateUser, and deleteUser. Outbound Ports (SPI): Business services use the interfaces defined by SPI ports to interact with external systems (such as data persistence). They delegate technical operations to adapters that implement these ports. Example: UserApiService uses UserSpiPort to access methods like saveUser and findUserById, without worrying about where and how this data will be stored. What Business Services Can Do Encapsulate Business Logic: They are responsible for enforcing business rules, specific validations, and orchestrating the operations necessary to execute a use case. Example: Verifying that a user does not already exist before creating them, or ensuring that the provided data meets business constraints. Raise Business Exceptions: In case of a business rule violation, services can raise specific exceptions to signal the issue to higher layers. Example: Raising a BusinessRuleViolationException if an email address is already in use. Use SPI Ports: They delegate technical operations to adapters via SPI ports, ensuring decoupling between business logic and technical details. Example: Calling userSpiPort.saveUser(user) to persist a user without knowing the details of the database. What Business Services Should Not Do Manage Technical Details: They should not include logic related to specific technologies, such as direct database interactions, network protocols, or external frameworks. Explanation: This would violate the principle of separation of concerns and make the domain dependent on technical details. Manipulate Technical Objects: Business services should not directly manipulate technical objects (e.g., JPA entities or framework-specific DTOs). Explanation: They should work with pure business objects to maintain domain independence. Handle Technical Exceptions: They should not handle exceptions related to technical layers (such as SQLException). These exceptions should be captured and handled by the technical adapters. Explanation: The domain should remain agnostic of technical details to ensure its portability and testability. Advantages of Business Services Centralization of Business Logic: By grouping rules and business processes within services, system maintenance and evolution become easier. Layer Decoupling: Business services interact with ports, ensuring a clear separation between the domain and technical layers. Improved Testability: By isolating business logic, services can be tested independently from external infrastructure. By following these guidelines, business services contribute to a clear, modular architecture that adheres to DevOps and craftsmanship principles. 4. Using Business Entities Within hexagonal architecture, business entities represent the primary objects of the domain, encapsulating both the state and behavior associated with them. They are central to business logic and must be designed to ensure consistency, maintainability, and independence from technical layers. Business Entities Business entities are objects that model key elements of the application domain, such as Users, orders, or products. They contain essential data and methods that manipulate this data according to the defined business rules. public class User { private Long id; private String name; private String email; private boolean active; public User(Long id, String name, String email) { validateName(name); validateEmail(email); this.id = id; this.name = name; this.email = email; this.active = false; } // Business methods public void activateAccount() { this.active = true; } public void changeEmail(String newEmail) { validateEmail(newEmail); this.email = newEmail; } // Internal validations private void validateName(String name) { if (name == null || name.isEmpty()) { throw new BusinessRuleViolationException(&quot;Name cannot be null or empty.&quot;); } } private void validateEmail(String email) { if (email == null || !email.contains(&quot;@&quot;)) { throw new BusinessRuleViolationException(&quot;Invalid email address.&quot;); } } // Getters and setters // ... } Java Key characteristics of business entities: Encapsulation of state and behavior: Entities group together their attributes (data) and methods (behaviors). Technological independence: They are not dependent on frameworks, libraries, or specific technologies, allowing the domain to remain independent of external layers. Consistency of business rules: They ensure that domain constraints and invariants are respected. Different Possible Implementations Several approaches can be used to implement business entities in Java: 1. Java POJO (Plain Old Java Object) POJOs are classic Java classes without dependencies on any particular framework. They contain private attributes and public methods to access and modify these attributes. Advantages: Simplicity and clarity: Easy to understand and maintain. Full control: Allows complete customization of behavior. Disadvantages: Verbosity: Requires manually writing repetitive code (constructors, getters, setters). 2. Java Records Introduced in Java 14, records are concise, immutable classes designed to hold data. public record User(Long id, String name, String email, boolean active) { public User { validateName(name); validateEmail(email); } // Business methods returning new objects due to immutability public User activateAccount() { return new User(id, name, email, true); } public User changeEmail(String newEmail) { validateEmail(newEmail); return new User(id, name, newEmail, active); } // Internal validations private static void validateName(String name) { if (name == null || name.isEmpty()) { throw new BusinessRuleViolationException(&quot;Name cannot be null or empty.&quot;); } } private static void validateEmail(String email) { if (email == null || !email.contains(&quot;@&quot;)) { throw new BusinessRuleViolationException(&quot;Invalid email address.&quot;); } } } Java Advantages: Conciseness: Reduces boilerplate code. Immutability: Encourages data safety and consistency. Disadvantages: Limitation of mutations: Every modification creates a new instance, which may be less performant. Availability: Requires Java 14 or higher. 3. Lombok Lombok is a library that automatically generates repetitive code through annotations. @Data @AllArgsConstructor public class User { private Long id; private String name; private String email; private boolean active; // Business methods public void activateAccount() { this.active = true; } public void changeEmail(String newEmail) { if (newEmail == null || !newEmail.contains(&quot;@&quot;)) { throw new BusinessRuleViolationException(&quot;Invalid email address.&quot;); } this.email = newEmail; } } Java Advantages: Reduces repetitive code: Automatically generates getters, setters, constructors, etc. Improved readability: More concise source code. Disadvantages: External dependency: Introduces an additional dependency. Hidden magic: The generated code is not visible, which can complicate debugging. Recommendations After evaluating the different options, here are clear recommendations: Favor POJOs for Complete Control Why: They offer great flexibility and independence from Java versions or external dependencies. Best Practices: Use private attributes with public methods for access. Include validations in constructors and setters. Avoid exposing too much of the internal state (encapsulation principle). Use Records for Immutable Entities Why: If the business entity is naturally immutable, records offer a concise and safe syntax. Best Practices: Include validations in the compact constructor. Handle mutations by returning new instances. Use Lombok with Caution Why: Lombok can speed up development but may introduce complexity. Best Practices: Ensure the team is comfortable with Lombok. Clearly document the use of annotations. Limit Lombok to cases where the benefit is significant. Data Validation in Business Entities Data validation is essential to maintain domain integrity. Implementing Validation: In constructors and methods: Integrate validation for each attribute during creation or modification. Raise business exceptions: Use specific exceptions to signal business rule violations. public class User { // Private attributes public User(Long id, String name, String email) { validateName(name); validateEmail(email); // Initialize attributes } public void changeEmail(String newEmail) { validateEmail(newEmail); this.email = newEmail; } private void validateName(String name) { if (name == null || name.isEmpty()) { throw new BusinessRuleViolationException(&quot;Name cannot be null or empty.&quot;); } } private void validateEmail(String email) { if (email == null || !email.contains(&quot;@&quot;)) { throw new BusinessRuleViolationException(&quot;Invalid email address.&quot;); } } // Other methods and getters/setters } Java 5. Choosing Return Types for Methods In a hexagonal architecture, the choice of return types for domain, SPI, and API methods is crucial. This decision directly affects the roles and capabilities of each component and must be made carefully to maintain a clear separation between business logic, technical details, and external communication. Return types act as interface points between the domain, SPI, and API. By wisely defining these types, each layer can perform its specific function without overstepping the responsibilities of others. For example: The domain can focus on business logic by returning clear business objects or raising appropriate business exceptions. The SPI handles technical details and external system uncertainties, using technical return types such as Optional or error codes. The API interacts with external clients by translating domain results into appropriate responses and adhering to standard communication protocols. NoteIntroduced in Java 8, Optional is a container class that may or may not contain a non-null value. It is used to explicitly represent the possible absence of a value, thereby avoiding issues related to NullPointerException. Illustrative Scenarios To better understand how this separation works in practice, here are some concrete scenarios showing the interactions between SPI, domain, and API. Scenario SPI Domain API (e.g., REST) 1. Searching for a non-existent user Returns Optional.empty() Raises a ResourceNotFoundException business exception. Captures the exception and returns an HTTP 404 Not Found response to the client. 2. Creating an already existing user Captures the technical uniqueness constraint exception. Before saving, the domain checks if the user already exists. If they do, it raises a BusinessRuleViolationException. Captures the exception and returns an HTTP 409 Conflict response to the client. 3. Updating a non-existent resource Returns a boolean indicating whether the update succeeded. If the update fails (returns false), the domain raises a ResourceNotFoundException. Captures the exception and returns an HTTP 404 Not Found response to the client. 4. Database connection error Captures the technical DatabaseConnectionException. May raise a ServiceUnavailableException or handle the error according to business rules. Captures the exception and returns an HTTP 503 Service Unavailable response. 5. Empty resource list when fetching users Returns a list, which may be empty. The empty list is considered a valid response and is returned as-is. Returns an HTTP 200 OK response with an empty list to the client. Benefits of This Approach Decoupling Layers: Each layer has a well-defined responsibility, making maintenance and scalability easier. Clarity in Error Handling: Technical errors do not cross layers, and clients receive consistent messages. Flexibility: The technical implementation of the SPI can be changed without impacting the domain or API. Best Practices Do Not Expose SPI Technical Types to the Domain: The domain should work with business objects and not depend on specific technical types. Use Business Exceptions in the Domain: To signal issues related to business rules. Translate Business Exceptions into Appropriate HTTP Codes: The API should map exceptions to standard HTTP codes for clear communication with the client. Handle Technical Exceptions in the SPI: The SPI should capture technical exceptions and provide results that the domain can interpret. 6. Data Validation In hexagonal architecture, data validation can occur at multiple levels, but the business service is primarily responsible for business validations. However, input adapters (e.g., REST controllers or application services) can also play a role by validating the syntax and structure of data before passing it to the domain. Here is the breakdown of responsibilities. Input Adapters (REST, UI, etc.) They can check that the received data respects the expected syntax and format (e.g., required fields, valid date formats, etc.). These adapters can use validation libraries like Hibernate Validator (which follows the Bean Validation standard) to validate DTOs before passing them to the domain. This helps filter errors before the data reaches the business service, reducing the complexity of error handling within the domain. @PostMapping(&quot;/users&quot;) public ResponseEntity&amp;lt;UserDto&amp;gt; createUser(@Valid @RequestBody UserDto userDto) { // If validation fails, a 400 Bad Request will be returned automatically User createdUser = userService.createUser(userDtoMapper.toDomain(userDto)); return new ResponseEntity&amp;lt;&amp;gt;(userDtoMapper.toDto(createdUser), HttpStatus.CREATED); } Java Business Service (Domain) It is responsible for business validations that are specific to the domain. This involves verifying, for example, that a user does not already exist, or that a specific business rule is respected (e.g., the user must be of legal age). The domain uses standard language instructions to encapsulate these validations in business objects. Business exceptions are raised if any rules are violated. Business validation ensures that business rules are upheld. This helps maintain data integrity within the domain. public User createUser(User user) { if (userRepository.findUserByEmail(user.getEmail()).isPresent()) { throw new BusinessRuleViolationException(&quot;User already exists.&quot;); } return userRepository.saveUser(user); } Java Impacts for Other Components Input Adapters: By ensuring that received data is valid at the reception point, input adapters help reduce complexity and error handling in the domain. If validation fails, the adapters directly return a 400 Bad Request with an explanatory message. Business Service: If validation fails within the business service (e.g., due to a business rule violation), a specific exception (such as a BusinessRuleViolationException) is raised and captured by the input adapter to return a 409 Conflict or another appropriate HTTP code. This ensures that business rules remain centralized within the domain and not within the infrastructure. Advantages and Disadvantages Advantages: Separation of responsibilities: Structure and syntax validations are handled at the adapter level, while business validations are concentrated in the business service. Clear error handling: Errors related to business rule violations or incorrect formats are clearly identified and returned with appropriate HTTP codes (400, 409, etc.). Disadvantages: Potential duplication: In some cases, the same validation might be needed in both the adapter (for structure reasons) and in the domain (for business reasons), which could lead to duplication. Additional complexity: Although this approach is highly modular and decoupled, it can sometimes make the system more complex to implement and maintain. In summary, within hexagonal architecture, data validation is split between input adapters and the business service, with a clear separation between structure/syntax validations and business validations. This decoupling enhances modularity but requires careful attention to avoid validation duplication. 7. Role of DTOs in Hexagonal Architecture In hexagonal architecture, DTOs (Data Transfer Objects) serve to transfer data between the various layers of the application, especially between external adapters (such as REST controllers) and the domain. They ensure strict decoupling between business logic and external interfaces, while facilitating adaptation to the specific data formats needed by each layer. Why Use DTOs? Separation of Concerns: DTOs allow separation between data representation in external interfaces (REST API, UI) and the business objects in the domain. This ensures that business logic encapsulated in domain objects is not directly exposed to external adapters. Example: A UserDto used to transfer user data via a REST API contains only the necessary information (ID, name, address), while the User domain object encapsulates more complex behaviors and business rules. public class User { private Long id; private String name; private String email; private Address address; // Class that contains the user&#39;s address information private List&amp;lt;Order&amp;gt; orders; // List of orders placed by the user // Constructors, getters, and setters... } Java public class UserDto { private Long id; private String name; private String address; // Address represented as a string (e.g., &quot;123 Main St, City, Country&quot;) // Constructors, getters, and setters... } Java Adapting to Data Formats: DTOs facilitate mapping data from a format suited to external clients’ needs (e.g., JSON for a REST API) to richer domain objects that adhere to business rules. This provides flexibility in data transformation. Example: A UserDtoMapper can convert a UserDto into a User domain object and vice versa. public class UserDtoMapper { // Method to convert a DTO into a domain object public User toDomain(UserDto dto) { Address address = parseAddress(dto.getAddress()); // Convert the address from String to an Address object return new User(dto.getId(), dto.getName(), dto.getEmail(), address, new ArrayList&amp;lt;&amp;gt;()); } // Method to convert a domain object into a DTO public UserDto toDto(User user) { String address = formatAddress(user.getAddress()); // Convert the Address object to a String return new UserDto(user.getId(), user.getName(), user.getEmail(), address); } // Utility method to transform an address string into an Address object private Address parseAddress(String address) { // Assumes the address is in the form &quot;123 Main St, City, Country&quot; String[] parts = address.split(&quot;, &quot;); return new Address(parts[0], parts[1], parts[2]); } // Utility method to format an Address object into a string private String formatAddress(Address address) { return String.format(&quot;%s, %s, %s&quot;, address.getStreet(), address.getCity(), address.getCountry()); } } Java Domain Protection: DTOs offer control over what data is exposed to external clients, filtering out sensitive or unnecessary information for the API context. This protects the integrity of domain data and prevents exposure of unnecessary technical or business details. Example: A UserDto can omit sensitive fields such as financial information or passwords. Benefits of DTOs Modularity: The decoupling between external layers and the domain enhances code modularity. Changes to DTOs do not directly affect the domain, simplifying maintenance. Reduced Dependencies: External layers do not need to be aware of internal domain details, which limits dependencies between different application layers. Adaptability and Scalability: DTOs make it easy to adapt the data format according to the needs of external interfaces (adding fields, handling API versions) without impacting business logic. Disadvantages of DTOs Additional Complexity: Using DTOs requires maintaining additional classes and mappers to transform objects between layers, which can complicate the code and increase maintenance. Potential Duplication: DTOs may sometimes duplicate information found in domain objects, leading to maintenance overhead if mappers are not well managed. In Summary The use of DTOs in a hexagonal architecture is essential to maintain the independence of the domain from external technologies. They allow for flexible data mapping between the different layers, protect business objects from direct exposure, and ensure better modularity within the application. However, this approach introduces some complexity and requires additional effort to maintain the mappers and DTOs. 8. Organization of Domain Packages A clear and well-structured organization of packages helps avoid design mistakes and clearly identify each component of the system. By isolating the domain in an independent module, we ensure that it remains free from technical dependencies or external frameworks. This separation helps maintain the integrity of the domain by protecting its business logic from technical concerns, while facilitating the evolution of the architecture over time. Within a hexagonal architecture, this modular structure ensures that responsibilities are clearly defined between the domain, the ports (inbound and outbound), and the services, thus promoting clear decoupling and coherent code organization. Package by Layer vs. Package by Feature The Package by Layer approach organizes classes by their technical role, grouping them by the architecture’s cross-cutting layers. The Package by Feature approach organizes classes by functionality or use case. For a modern architecture aimed at flexibility and the ability to evolve rapidly (such as hexagonal architecture), Package by Feature is recommended as it guarantees better separation of concerns and facilitates the transformation of features into autonomous services. An Example of Package Structure for the “user” Use Case domain/ ├── common/ │ └── exceptions/ │ ├── BusinessRuleViolationException.java │ └── ResourceNotFoundException.java │ └── user/ ├── domain/ │ └── User.java ├── port/ │ ├── inbound/ │ │ └── UserApiPort.java │ └── outbound/ │ └── UserSpiPort.java └── service/ └── UserApiService.java Ascii Class and Interface Details Package domain.common.exceptions: This package contains common business exceptions to signal rule violations or resource absence, distinct from technical exceptions. The goal is to centralize these exceptions to maintain domain consistency and encapsulation. Package domain.user: The domain.user package groups all elements related to the “user” business domain. By isolating all relevant classes, interfaces, and services within this single package, several advantages are achieved: Ease of Identification: The domain.user package brings together everything related to the “user” domain in one place. This simplifies understanding and navigating the code, as it’s easy to locate components associated with this business entity. Modularity and Reusability: By isolating the domain.user package, it becomes modular. This makes the system extensible, as new behaviors and services specific to user can be added without impacting other parts of the domain. Ease of Relocation and Maintenance: Since the domain.user package is isolated, it can easily be moved, restructured, or even extracted into another project. For instance, if the user entity were to be externalized as an independent microservice, it would be relatively simple to do so, as all related classes and interfaces are already well encapsulated within a single package. Business Context Consistency: Grouping all parts related to user within a single package helps preserve business context consistency. All objects, services, ports (inbound and outbound) remain encapsulated within a single context, helping to avoid circular dependencies and ensuring a clear separation of concerns. Package domain.user.port.inbound: The inbound ports package contains interfaces defining use cases exposed to external adapters. These interfaces serve as contracts between external layers and business logic, describing the functional operations of the domain without exposing its internal logic. Package domain.user.port.outbound: Outbound ports define technical interfaces allowing the domain to access external systems (databases, third-party services, etc.). They delegate technical tasks while maintaining the domain’s independence from underlying technologies, ensuring infrastructure flexibility. Package domain.user.service: The services package contains business implementations that orchestrate inbound and outbound port operations. These services implement inbound interfaces, ensure business logic, and delegate technical operations to outbound ports. This organization allows the code to be structured according to the principles of separation of concerns and decoupling between business and technical layers, thus ensuring a modular and easily maintainable architecture. Conclusion - Beyond the Hexagon Hexagonal architecture, with its principles of decoupling and separation of concerns, provides a robust and scalable framework for managing the complexity of a modern application. However, beyond these technical choices, other dimensions of software architecture deserve exploration. A natural step after mastering hexagonal architecture is to consider infrastructure management. The decoupling between the domain and infrastructure opens up numerous technical implementation strategies: cloud computing, container deployment, microservices… Each approach brings its own challenges and opportunities. Transitioning to architectures like microservices also raises questions about service distribution management, resilience, and the trade-offs between modularity and operational complexity. Beyond infrastructure, other architectures may also be considered. For example, layered architecture remains a viable option for simpler applications where strict separation between the domain and infrastructure is not necessary. Similarly, event-driven approaches or CQRS (Command Query Responsibility Segregation) focus on event handling and scalability for complex applications, often with very different implementation models but complementary to hexagonal architecture. Finally, the choice of tools and frameworks to support this architecture must be continuously reassessed. In conclusion, hexagonal architecture is only one piece of the puzzle. It provides a solid foundation but must be constantly reevaluated and adapted within a broader technological context. Infrastructure, tooling, and the integration of other architectural paradigms will be key to building systems that are increasingly scalable, resilient, and high-performing."
    } ,
  
    {
      "title"    : "The World in Data: A Website Dataviz to Analyze Global Activity",
      "url"      : "/en/dataviz-leaflet-engagement/",
      "img"      : "PHBpY3R1cmU+CiAgPHNvdXJjZSBzcmNzZXQ9Ii9hc3NldHMvaW1nL2RhdGF2aXotbGVhZmxldC1lbmdhZ2VtZW50LTMwMC00Y2E0OTNhNjAuYXZpZiAxLjB4LCAvYXNzZXRzL2ltZy9kYXRhdml6LWxlYWZsZXQtZW5nYWdlbWVudC02MDAtNGNhNDkzYTYwLmF2aWYgMi4weCIgdHlwZT0iaW1hZ2UvYXZpZiI+CiAgPHNvdXJjZSBzcmNzZXQ9Ii9hc3NldHMvaW1nL2RhdGF2aXotbGVhZmxldC1lbmdhZ2VtZW50LTMwMC05YWNlNjQzYjQuanBnIDEuMHgsIC9hc3NldHMvaW1nL2RhdGF2aXotbGVhZmxldC1lbmdhZ2VtZW50LTYwMC05YWNlNjQzYjQuanBnIDIuMHgiIHR5cGU9ImltYWdlL2pwZWciPgogIDxpbWcgc3JjPSIvYXNzZXRzL2ltZy9kYXRhdml6LWxlYWZsZXQtZW5nYWdlbWVudC0zMDAtOWFjZTY0M2I0LmpwZyIgYWx0PSJUaGUgV29ybGQgaW4gRGF0YTogQSBXZWJzaXRlIERhdGF2aXogdG8gQW5hbHl6ZSBHbG9iYWwgQWN0aXZpdHkiPgo8L3BpY3R1cmU+Cg==",
      "isencoded" : "y",
      "tags"     : "Dataviz, Leaflet, JavaScript",
      "content"  : "In the current digital landscape, online activities outline the contours of a new world map where people interact with technologies. This phenomenon is particularly noticeable in the field of computing, where professionals are interconnected through a global network, marked by peaks and troughs of activity that follow geographical areas and time zones. By studying the connections to our website dedicated to DevOps, we can glimpse the habits and preferences of this community of technical experts. Our analysis reveals not only the areas of high professional concentration but also offers insights into technological choices and digital access across the world. This mapping of digital rhythms allows us to observe not only where but also when and how engineers interact with our platform, thus providing an indirect measure of global activity in this sector. By diving into the analysis of these digital pulses, we embark on a journey through the daily habits of engineers, while deciphering the broader signals that these data emit in the context of the globalization of our economies. Introduction to Interactive Dataviz Geographical Distribution of Visitors Europe North America Asia Africa South America Oceania Operating System Usage Windows macOS iOS Android GNU/Linux Search Engine Preferences Google Bing Yandex, Baidu Brave, DuckDuckGo, Qwant Twitter, LinkedIn Referring Websites Ecosia, Yahoo! Horizontal Activity Analysis Conclusion Introduction to Interactive Dataviz Our data visualization tool is designed to offer an immersive experience in analyzing user engagement data from our website specialized in DevOps. Thanks to the use of the JavaScript library Leaflet, we have created an interactive map that illustrates the geographical distribution of visitors, as well as the intensity of their interaction with the site’s content. The map uses color markers and layers to represent different levels of activity, allowing users to quickly identify the hotspots of web traffic. This real-time interaction allows the user to discover activity patterns that might otherwise remain hidden in simple data tables. Moreover, our dataviz offers the possibility to filter data by operating system and search engine. This level of customization reveals technological preferences by region and can help understand how cultural, economic, and political factors influence web usage and the choice of digital tools. The zoom functionality allows examining specific areas in more detail, providing a microscopic view of specific regions and their engagement with the site. This tool is not only a means of displaying data but also a platform for exploration that invites analysis and discovery. Visualizing activity by hour of the day is another fascinating aspect of our tool. A circular graph illustrates the site’s activity over a full day, revealing a wave of engagement that spreads from east to west, following the sunrise across the meridians. This phenomenon highlights the global nature of the DevOps community and its work patterns synchronized with circadian cycles. Therefore, this dataviz is not just a means of presenting data; it acts as a visual narrative that transforms numbers into stories, allowing users to understand the complex nuances of online activity among IT professionals. Geographical Distribution of Visitors The analysis of the geographical distribution of visitors to our website reveals a mapping that goes beyond borders, unveiling peculiarities that reflect both the global adoption of technologies and specific local and regional trends. Europe Geographical engagement in Europe In Europe, we notice a strong engagement with a total of 13,000 visits. France stands out particularly with 6,500 visits, which can be attributed to the fact that half of the site’s content is available in French. Germany and the United Kingdom follow with 1,100 and 800 visits respectively, highlighting the relevance of our content in these technological hubs. Zooming in on France, it is interesting to note that the distribution of visits follows the famous “diagonal of emptiness,” a geographical area extending from the northeast to the southwest where the population density is low and economic activity is lesser. The United Kingdom presents a particular geographical distribution as well, divided in two, highlighting the economic poles and clusters of companies specialized in computing mainly concentrated in the southern half of the country. Spain is an excellent example of how computer activity is primarily concentrated in urban areas. The activity is essentially centered around a few major cities such as Madrid, Barcelona, Valencia, and Seville. This concentration indicates the location of technological hubs and innovation ecosystems in the country. In Germany, the distribution is more homogeneous, reflecting the strong industrialization and digital adoption throughout the country, with a notable concentration in regions such as Bavaria, Baden-Württemberg, and around major cities like Berlin, Munich, and Frankfurt, known for their dynamism in the tech and innovation sector. North America Geographical engagement in North America The American continent also shows a significant presence with 5,000 visits in North America, dominated by the United States with 4,000 visits and Canada with 1,000. These figures reflect the influence and adoption of computing tools in these regions. In Canada, the activity is primarily concentrated near the border with the United States, highlighting the clustering of the population in major southern cities like Toronto, Montreal, and Vancouver, which are important technology hubs. In the United States, the engagement on our site presents a geographical distribution that underscores a relatively homogeneous activity in the eastern half of the country, encompassing both major technology centers such as New York, Washington, Boston, Chicago, Atlanta, and Miami. On the other hand, the western half of the United States shows a more targeted activity, particularly concentrated on the west coast, in regions such as the Silicon Valley and Los Angeles in California, as well as Seattle in the State of Washington, and inland, near Austin, Houston, and Dallas in Texas, as well as Denver in Colorado. Asia Geographical engagement in Asia In Asia, our site recorded a total of 4,000 visits, with a notable presence in India (1,900 visits), illustrating the increasing importance of this country in the global IT sector. Russia with 550 visits, China with 400, Japan with 200, and Singapore with 170 also contribute with figures that testify to the global reach of the site, despite linguistic differences or digital barriers. Moreover, significant activity points were observed in Israel (100) and the United Arab Emirates (80), highlighting the influence and adoption of technologies in these strategic regions. Furthermore, significant activity points can be distinguished in some major cities: Taiwan, with its strong technology industry, highlighting the importance of the island in the production of computer hardware. Hong Kong, as a financial center with a strong presence of startups and financial institutions. Tel Aviv, as a center of technological innovation, where a flourishing ecosystem of startups and innovation culture prevails. Dubai, with its commitment to digital transformation and its status as a commercial hub, reflecting the city’s ambition to be a leader in technological innovation. Tehran and Moscow, despite regulatory challenges and international sanctions. Africa Geographical engagement in Africa Although Africa presents overall more modest figures, with a total of 900 visits, the engagement is nonetheless notable, particularly in Morocco with 150 visits, in Tunisia with 128, in South Africa with 75, in Senegal with 70, and in Algeria with 61 visits. These data testify to a significant adoption and a growing interest in information technology across the continent, highlighting a distinctive presence from the Maghreb to Southern Africa, crossing the regions of West Africa and the Great Lakes. The points of engagement in Africa reflect not only a growing interest in information technology but also the presence of dynamic and growing technological communities in these regions: Morocco and Tunisia are known for their offshore development centers and for their increasing investment in advanced technological education and training. South Africa, with its most diversified and advanced economy on the continent. Senegal and Algeria indicate the emergence of technological hubs and an interest in information technology. West Africa, with countries like Senegal, Ivory Coast, Benin, and Nigeria, shows the rapid evolution of technological sectors in this region. South America Geographical engagement in South America South America shows 500 visits, led by Brazil with 250 visits, followed by Colombia with 100 visits, Argentina with 65 visits, and Chile with 50 visits. This region displays growth potential given the rapid evolution of its technology sector. It is the metropolises like São Paulo, Buenos Aires, and Santiago that gather the majority of visits and illustrate the central role of these cities as regional technology hubs. Oceania Geographical engagement in Oceania In Oceania, Australia leads with 250 visits, closely followed by New Zealand with 50 visits, illustrating how the internet connects remote regions of the globe: Australia shows a notable peculiarity, with activity mainly localized on the east coast. The cities of Sydney, Melbourne, and Brisbane form a triangle of technological innovation. In New Zealand, activity is centered around Auckland and Wellington. This global overview not only demonstrates the international reach of the site but also underscores the importance of considering languages and local contexts in a content strategy. The predominance of engagement in certain countries reflects the concentrations of specialized engineering populations and local technological ecosystems, providing an updated view of global economic activities in development and operations. Operating System Usage The study of the geographical distribution of operating systems used by our visitors reveals significant trends reflecting technological preferences and usage behaviors in the field of computing. The visualized data show variable concentrations according to regions and operating systems, offering an interesting view of user engagement and their preferred work environment. Windows Engagement by Windows Operating System Users of the Windows operating system show a high density primarily in North America, Europe, Asia, and the Maghreb. This dominance could be explained by the wide adoption of Windows in business and educational environments, as well as its compatibility with a multitude of development applications. macOS Engagement by macOS Operating System The distribution of macOS users, although generally less dense than that of Windows, shows areas of intense activity, especially on the west and east coasts of the United States, as well as in parts of Europe. These hotspots could indicate a higher economic level, as Apple devices, often perceived as high-end, are more commonly adopted by users with greater financial means. iOS Engagement by iOS Operating System The use of iOS on mobile devices is significantly higher in densely populated and technologically advanced urban areas, such as major cities in the United States, Western Europe, and innovation hubs in various countries. This indicates that inhabitants of these regions again enjoy a higher standard of living, allowing the acquisition of devices considered more expensive, and reflects their interest in cutting-edge technologies. Android Engagement by Android Operating System The use of Android is characterized by its more homogeneous distribution across the world, indicating a strong market penetration in various economic and cultural contexts. It is found in regions less represented by other operating systems such as South America, Africa, and South Asia. This may indicate a prevalence of Android in regions with lower hardware costs and a greater diversity of devices. GNU/Linux Engagement by Linux Operating System The GNU/Linux system, while representing a more modest share, shows significant use in specific areas, such as research centers, universities, and technology companies, reflecting its use on servers or by developers for its robustness and adaptability for advanced technical needs. The map reveals less dense areas, which matches expectations since Linux is often preferred by a more technical audience and less by the public. This also sheds light on the location of data centers and major computing centers, illustrating how Linux is essential in critical infrastructures and intensive computing operations. In conclusion, analyzing the operating systems used to access our site highlights the diversity of work environments and user preferences. Their distribution is quite homogeneous, and this dataviz does not allow for the extraction of significant information on specific usage behaviors or regional preferences. However, some emerging trends can be highlighted: For example, the increased use of macOS and iOS in developed regions could reflect a higher purchasing power that favors Apple brand products. Moreover, the data reveal that the Android operating system, with its greater accessibility, continues to dominate in emerging markets. Its significant presence in these regions suggests a predominance in the adoption of affordable mobile technologies. As for GNU/Linux, its discreet but notable presence in specific areas denotes a preference for open and reliable operating systems, often favored in technical centers and academic institutions. Despite the apparent uniformity, it would be interesting to perform a more granular analysis. Minor fluctuations in the use of operating systems, when examined over a longer period or with more detailed demographic data, could reveal substantial information in technological preferences. Search Engine Preferences This analysis sheds light on the various ways internet users access our site, revealing preferences that significantly vary by geography and user interests. Google Engagement by Google Search Engine At the top of the list, Google overwhelmingly dominates with 17,100 visitors, confirming its status as the preferred search engine globally. Its ability to attract such a considerable number of visitors underscores the importance of being well-indexed on this platform. Bing Engagement by Bing Search Engine Bing, with 350 visitors, shows popularity spread across Europe, India, and North America, illustrating its solid presence as an alternative to Google, often due to its integration into Microsoft products. Yandex, Baidu Engagement by Yandex and Baidu Search Engines Yandex, attracting 320 visitors, stands out for its strong presence in Russia, but also receives some visits from Belarus, Lithuania, the Netherlands, and Washington in the United States. This reflects its reach within Russian territory despite a few exceptions. Baidu, the Chinese search engine, attracts 80 visitors, a modest figure considering the large Chinese population, suggesting that Baidu significantly favors local content, potentially limiting the visibility of international sites among Chinese users. This trend could reflect internet censorship practices or local cultural preferences, influencing how information is consumed in China. Brave, DuckDuckGo, Qwant Privacy-focused search engines like Brave (220 visitors) and DuckDuckGo (230 visitors) show a notable preference in North America and Europe, reflecting a growing awareness in these regions of the importance of personal data protection. Qwant, with its 40 visitors, focused in France, indicates there is still work to be done before it can compete with Google. Twitter, LinkedIn Twitter, with 150 visitors, demonstrates its utility as a social networking platform for reaching users worldwide. LinkedIn, with 65 visitors, illustrates the usefulness of professional networks even though it is more focused on Europe and the United States. Referring Websites Engagement by Referring Websites Referring websites also play a key role, with 400 visitors coming from Europe, the United States, India, and China, highlighting the importance of external links and online partnerships. Ecosia, Yahoo! Ecosia (25 visitors), mainly used in France, shows the beginning of interest in eco-responsible search engines. Finally, Yahoo!, with its 30 visitors spread between France, India, and the United States, indicates that it struggles to regain a significant place in the market. This analysis reveals the multiple ways users access our site, highlighting preferences marked by geographical differences and specific interests. Google stands out significantly with over 90% of site visitors, affirming its global hegemony and the crucial importance of adopting a specific SEO strategy for this platform, to reach a wide and varied audience. Twitter, compared to LinkedIn, appears more effective for reaching users on social networks. Bing, Brave, and DuckDuckGo, on the other hand, position themselves as alternatives capturing a more modest market share, dominated by the giant Google. Horizontal Activity Analysis The horizontal analysis of activities on our site dedicated to DevOps reveals user engagement patterns throughout the day, which follow work rhythms and time zones across the world. A wave moving from east to west with the rhythm of the sun emerges. NoteTo launch the visualization, click on the Visitor Time button. A clock appears, tracing the hours of the day. Click on it again to stop the animation. Conclusion By highlighting activity by geographical area, operating system, referral source, and hour, we have revealed insights into the digital behaviors of internet users and global activity zones, revealing how consumption trends and online interactions can influence and be influenced by economic and political dynamics. This exploration is not limited to a better understanding of visitor engagement; it also raises important questions about our relationship with technology and its footprint in our daily lives. In an era where digital shapes our interactions, leisure, and work habits, the data we have examined invites broader reflection on our digital life. Ultimately, the digital rhythms of our website are but a reflection of broader trends, inviting us to reflect on the place we wish to give to technology in our lives. As we continue to navigate this ever-changing digital space, let’s take the time to consider not only how we can effectively reach our audience but also how we can contribute to a more conscious and respectful digital ecosystem."
    } ,
  
    {
      "title"    : "CrewAI, Ollama, Mixtral: An Agile Squad in Your Computer",
      "url"      : "/en/crewai-mixtral-agile-team/",
      "img"      : "PHBpY3R1cmU+CiAgPHNvdXJjZSBzcmNzZXQ9Ii9hc3NldHMvaW1nL2NyZXdhaS1taXh0cmFsLWFnaWxlLXRlYW0tMzAwLTI2ZGJmZjA5ZS5hdmlmIDEuMHgsIC9hc3NldHMvaW1nL2NyZXdhaS1taXh0cmFsLWFnaWxlLXRlYW0tNjAwLTI2ZGJmZjA5ZS5hdmlmIDIuMHgiIHR5cGU9ImltYWdlL2F2aWYiPgogIDxzb3VyY2Ugc3Jjc2V0PSIvYXNzZXRzL2ltZy9jcmV3YWktbWl4dHJhbC1hZ2lsZS10ZWFtLTMwMC0yMjhhNjA3ZmEuanBnIDEuMHgsIC9hc3NldHMvaW1nL2NyZXdhaS1taXh0cmFsLWFnaWxlLXRlYW0tNjAwLTIyOGE2MDdmYS5qcGcgMi4weCIgdHlwZT0iaW1hZ2UvanBlZyI+CiAgPGltZyBzcmM9Ii9hc3NldHMvaW1nL2NyZXdhaS1taXh0cmFsLWFnaWxlLXRlYW0tMzAwLTIyOGE2MDdmYS5qcGciIGFsdD0iQ3Jld0FJLCBPbGxhbWEsIE1peHRyYWw6IEFuIEFnaWxlIFNxdWFkIGluIFlvdXIgQ29tcHV0ZXIiPgo8L3BpY3R1cmU+Cg==",
      "isencoded" : "y",
      "tags"     : "AI, Agile, LLM, Mixtral, Development, DevOps",
      "content"  : "In the IT field, Artificial Intelligence (AI) presents itself as a revolutionary phenomenon, carrying both promises and challenges. At the heart of professional discussions, two schools of thought stand out distinctly. On one side, there is a population of skeptical developers, doubting strongly that AI could ever replace the human expertise and creative intuition necessary for their work. On the other, a second population considers this possibility with some resignation, wondering more “when” this will happen rather than “if” it is possible. This article aims to evaluate the advancements of AI in the field of software development, using a typical case found in dev teams: an Agile squad tasked with delivering a software product. Through this approach, we wish to offer IT professionals a clear and updated view on the real capabilities of AI and its potential to transform the software development landscape. Without claiming to provide a definitive answer to the question of replacing developers with AI, our goal is to present concrete data and tangible results that illuminate the debate and invite reflection on the future of our profession in a world where AI is increasingly taking a more significant role. The Tools at Our Disposal AI Architecture AI Model System Design Guiding Principles Source Code Autonomous Agents The Product Owner Agent The Developer Agent The Reviewer Agent The Tasks The Task Assigned to the Product Owner The Task Assigned to the Developer The Task Assigned to the Reviewer Forming the Squad Results The Product Owner’s Response The Developer’s Response The Reviewer’s Response Analysis of the Results Strengths Evaluation Conclusion Pros Cons Applications The Tools at Our Disposal To implement our project, we will use existing AI solutions that are available to the general public. There’s no need for a server farm calculating for weeks to come up with a potentially revolutionary new model. AI Architecture One of the latest advances in AI is the use of multi-agent architectures, an approach where multiple intelligent agents operate autonomously while collaborating to achieve a common goal. This technique surpasses traditional single-model AI approaches by replicating complex thought and decision-making processes that typically characterize human intelligence. Several solutions exist, the most notable at the moment are: AutoGen from Microsoft and CrewAI, easier to use because it is based on the LangChain framework. Therefore, our choice will be CrewAI. Its integration with LangChain allows it to benefit from the incorporation of all the Open Source models from Ollama and to operate them very simply on a local computer. Moreover, it simplifies the simulation of different roles and interactions between agents. AI Model To simulate our virtual Agile development team, we will use LLM models to fill the roles of each of its members. Several choices are available to us thanks to the different models supported by Ollama. We will use the latest model from MistralAI, Mixtral8x7B, a next-generation LLM model based on a Mixture-of-Experts (MoE) architecture. With this combination of technologies, we will be able to push the boundaries of what is possible in software development and explore innovative approaches to project management and team collaboration. System Design Our system must replicate a complete Agile team, where each AI agent has a specific role: The Product Owner who defines the user stories, The Developer who translates them into code, The Reviewer in charge of ensuring the quality of the produced code. NoteFor this demonstration, there is no need for a Scrum Master. Indeed, from the perspective of a neural network, their role as a facilitator is completely unnecessary. Each component of our architecture is designed to interact coherently with the others, with the goal of successfully delivering the product. This approach allows us to test the effectiveness of AI in traditionally human roles and to assess its potential to improve productivity and software development quality. Guiding Principles As a reminder, our project is guided by 3 key principles: We aim to demonstrate that AI can not only automate tasks but also significantly contribute to creative and decision-making processes. We strive to maintain as authentic an Agile approach as possible, even in a context of advanced automation. We place particular importance on responsibility in the use of AI, ensuring that our project positively contributes to the evolution of software development. We hope to show new perspectives on the integration of AI in software development. This approach allows us to question and redefine the boundaries between Artificial Intelligence and Human Intelligence, demonstrating how AI can complement and enrich human capabilities rather than merely replace them. Source Code You can find the source code for this project in this GitHub repo: jeanjerome/ai-agile-team&amp;nbsp;Public Autonomous Agents Our project relies on the use of CrewAI, a framework for orchestrating autonomous AI agents. An Agent should be seen as a team member, with specific skills and a particular task to accomplish. It can execute tasks, make decisions, or communicate with other agents. In CrewAI, an agent is characterized by three main attributes: A Role which specifies its function within the team and identifies the tasks for which it is particularly qualified. A Goal representing the end that the agent strives to achieve, thus guiding its choices and actions. A Backstory providing context for the agent’s role and goal, thereby improving interaction and collaboration dynamics. In our case of an Agile squad, we will create 3 types of Agents: The Product Owner, The Developer, The Reviewer. The Product Owner Agent From this Agent, we expect the automated generation of User Stories (US): the AI analyzes the project’s needs and generates coherent US, allowing the Developer to have a quick understanding and efficient implementation of the required functionalities. Here’s how to define it with CrewAI: po = Agent( role=&quot;Product Owner&quot;, goal=&quot;Ensure the detailed drafting of user stories&quot;, backstory=&quot;&quot;&quot;As the Product Owner of an Agile team, you excel at comprehending market demands, identifying the target audience, and analyzing the competition. You are skilled at devising strategies to appeal to the widest possible audience, ensuring the product aligns with user stories and meets market expectations. &quot;&quot;&quot;, [.../...] ) Python The Developer Agent We want this Agent to implement the product features described in the form of User Stories (US): the Developer write the necessary code to meet the requirements of the US. Let’s define it in turn: developer = Agent( role=&quot;Bash Scripting Expert&quot;, goal=&quot;Implement the requirements outlined in each user story through coding&quot;, backstory=&quot;&quot;&quot;You are a master of Bash scripting, with a profound knowledge of Unix-based systems.&quot;&quot;&quot;, [.../...] ) Python The Reviewer Agent This agent performs code reviews, ensuring the quality and compliance with the standards of the produced code. Let’s examine its definition in more detail: reviewver = Agent( role=&quot;Reviewer&quot;, goal=&quot;Review the code to assess the quality, maintainability, and alignment with state-of-the-art and best practices&quot;, backstory=&quot;&quot;&quot;You are a guardian of code quality, with a sharp eye for detail in code review. You are adept at ensuring that developments not only function as intended but also adhere to state-of-the-art-standards. With a deep appreciation for collaborative development, you provide constructive feedback, guiding contributors towards best practices and fostering a culture of continuous improvement. &quot;&quot;&quot;, [.../...] ) Python The Tasks In the context of CrewAI, tasks are individual missions that agents accomplish. They encapsulate the necessary information for execution, including a description, an assigned agent, and the required tools, offering flexibility for various action complexities. These tasks can also be designed to require collaboration between agents. This collaborative approach can be defined in the task properties. The Task Assigned to the Product Owner Here is the definition of the tasks of a Product Owner. This is where a certain level of detail is required: task1 = Task( description=&quot;&quot;&quot;Develop user stories for a Bash script wrapper function designed to : - Execute commands with parameters, - Log execution information to a file, - Manage errors by logging detailed stack trace, - Output result to stdout when available. This tool aims to enhance operational efficiency and reliability. Your user stories should clearly articulate the needs and expectations of the users, focusing on how they will interact with the wrapper to perform tasks more effectively. Include scenarios covering a range of use cases, from simple command execution to complex workflows involving error handling and output management. Ensure that each user story is detailed, specifying the context, the user&#39;s goal, and the desired outcome, to guide the development team in creating a solution that meets users&#39; needs. &quot;&quot;&quot;, expected_output=&quot;a title and a definition of done&quot;, agent=po, ) Python NoteDepending on the LLM used, it might be necessary to provide technical instructions to the model to generate its response. Indeed, an Agent’s response must be usable as input for another Agent, and a certain format must be respected. Since LLMs can take certain liberties, it is sometimes necessary to add the instructions below to the description, so that the Agent adheres to the expected formatting: description=&quot;&quot;&quot; [.../...] These keywords must never be translated and transformed: - Action: - Thought: - Action Input: because they are part of the thinking process instead of the output. Action Input should be formatted as coworker|task|context. &quot;&quot;&quot; Python The Task Assigned to the Developer Here is the developer’s task with all the necessary details on how to accomplish it: task2 = Task( description=&quot;&quot;&quot;Using the user stories provided, develop a robust and efficient tool. Your code should follow the stdout is for output, the stderr is for messaging principal. You ensure your code is clean, simple, and adheres to best practices for shell script development. &quot;&quot;&quot;, expected_output=&quot;markdown&quot;, agent=developer, ) Python The Task Assigned to the Reviewer Below is a detailed description of a reviewer’s tasks: task3 = Task( description=&quot;&quot;&quot;Ensure the quality of the code, the adherence to the SOLID principals, and the respect of the specifications included in the user stories. Provide detailed feedback to developers, highlighting areas for improvement, potential bugs, and suggestions for optimization. Collaborate with the development team to achieve high-quality software delivery in the project. &quot;&quot;&quot;, expected_output=&quot;Full report in bullet points&quot;, agent=reviewver, ) Python Forming the Squad A crew in CrewAI represents a collaborative group of agents working together to accomplish a set of tasks. Each crew defines the strategy for task execution, collaboration between agents, and the overall workflow. In our case, the formation of the crew will proceed as follows: crew = Crew( agents=[po, developer, reviewer], tasks=[task1, task2, task3], verbose=2, process=Process.sequential, ) Python Finally, we will start the Sprint by executing the instructions: result = crew.kickoff() print(&quot;######################&quot;) print(result) Python Results CrewAI does not guarantee that your process will successfully reach completion, nor that the LLM models you have chosen are relevant for accomplishing the tasks you have submitted to them. Between model hallucinations, response format errors, and the maximum number of possible iterations, there may be a need for adaptation and tuning. However, after an hour of trial and error in my case, I was able to obtain satisfactory responses from my virtual Agile Squad. The Product Owner’s Response Here is what the “Product Owner” Agent wrote upon completing its task: Final Answer: As a busy system administrator, I want to be able to execute simple commands with parameters using the wrapper function so that I can save time and reduce errors in command line input. Context: System administrator needs to run “ls -l” frequently on various servers to check file permissions. As a developer, I want the wrapper function to log execution information to a file so that I can keep track of all commands executed and their outcomes for debugging purposes. Context: Developer wants to monitor long-running scripts and detect any issues during execution. As a QA engineer, I want the wrapper function to manage status codes effectively so that I can identify and handle errors in a consistent manner, ensuring reliable test automation. Context: QA engineer is executing a suite of integration tests and wants to ensure proper handling of failed tests. As a DevOps engineer, I want the wrapper function to output results to stdout when available so that I can quickly view command outputs without having to check log files. Context: DevOps engineer is executing multiple commands in parallel and needs real-time feedback on their execution status. As a data analyst, I want the wrapper function to handle complex workflows involving error handling and output management so that I can automate my daily tasks more efficiently and focus on data analysis instead of command line management. Context: Data analyst is running several data processing scripts and needs to ensure they execute correctly, log errors, and manage outputs in a consistent manner. The Developer’s Response The Developer Agent, in turn, responded: Final Answer: To implement the user stories, I will create a Bash function called execute_command that takes a command and parameters as input. This function will address all the requirements outlined in the user stories. It will log execution information to a file, manage status codes effectively, output results to stdout when available, and handle complex workflows involving error handling and output management. Here’s the Bash function: #!/bin/bash execute_command() { # Parse command and parameters local command=$1 shift local params=(&quot;$@&quot;) # Log execution information to a file echo &quot;$(date) - Executing: $command ${params[@]}&quot; &amp;gt;&amp;gt; ~/execution.log # Output results to stdout when available &quot;$command&quot; &quot;${params[@]}&quot; | while read line; do echo &quot;[$command]: $line&quot; done # Manage status codes effectively local status=$? if [ $status -ne 0 ]; then echo &quot;Error: $command exited with status $status&quot; &amp;gt;&amp;gt; ~/execution.log fi return $status } Bash This function can be used to execute simple commands with parameters, like the ls -l example mentioned in the user story. The output will be sent both to stdout and a log file, allowing for easy tracking of executed commands and their outcomes. Error handling is built into the function, ensuring reliable execution and consistent error management. Example usage: execute_command ls -l /var/log Bash The Reviewer’s Response The reviewer finalized the process by returning: Final Answer: The Bash function execute_command is well-written and adheres to the specified requirements. It effectively logs execution information, manages status codes, and outputs results to stdout when available. Additionally, it handles complex workflows involving error handling and output management. However, there are a few recommendations to improve code quality, maintainability, and alignment with team’s best practices: Add comments describing the function’s purpose and its behavior to make it easier for other developers to understand. Use lowercase and underscores for local variable names to follow Bash scripting conventions (e.g., command should be command_). Separate logging, execution, and error handling into different functions to improve readability and maintainability. Add input validation to ensure the function receives valid inputs, such as checking if the command provided exists. Implement a more sophisticated logging mechanism, for example using the tee command or Bash’s built-in exec command, to enable real-time log monitoring during development and debugging. Add error messages to the log when the status code is not zero, explaining what went wrong. This will make it easier to diagnose issues when they occur. Consider adding a mechanism to pass flags for enabling or disabling logging or outputting results to stdout based on user requirements. Ensure that the function handles special characters and edge cases in command names and parameters, such as spaces or quotes. Test the function with various commands, input types, and error scenarios to ensure it works as expected. Analysis of the Results The results of this demonstration are overwhelmingly positive, even startling, showing that AI agents can effectively simulate the roles and responsibilities of a software development team with great efficiency. Strengths Among the strengths observed, we can note: Efficiency in Generating User Stories: The AI agents demonstrated an impressive ability to generate clear and relevant User Stories, allowing for a quick understanding of the project’s needs. Quality of Code Development: The code produced by the developer AI agents adheres to high quality standards and excellent alignment with the project requirements. Improvement of Code Review Processes: The AI agents responsible for code review performed this task with increased precision and efficiency, thereby contributing to improving the overall quality of the code. Evaluation We have here a solution that could enhance the efficiency of any Agile team. Whether it’s automating the generation of the Backlog, implementing a first version of the code, or checking compliance with standards, all these preparatory tasks could be automated. Teams could then focus on tasks with higher added value. It’s also a simple and cost-effective way to increase their velocity. Conclusion The results obtained illustrate how the adoption of AI in software development could revolutionize the way teams design and conduct their projects. They highlight AI’s capacity to act as an innovation driver in this field, offering both cutting-edge automation and opportunities to enhance efficiency and the quality of collaborative work. These findings encourage further exploration of AI’s application in software development, including experimenting with new configurations of intelligent agents, LLM models, and solving more complex problems. However, leveraging these advantages involves addressing certain challenges, such as the need to train professionals in AI technologies and finding the right balance between automation and human interaction. In conclusion, the future of software development with AI looks promising. It is now up to development teams to seize these opportunities while wisely navigating through the challenges. Pros Significant improvement in team efficiency Stimulation of creativity and innovation Ability to quickly solve complex problems Cons Necessity of an initial investment in time and resources for training Challenges related to maintaining the balance between automation and human intervention Applications The implications of integrating AI into software development go beyond simply automating repetitive tasks. They include improving decision-making, innovating in product design, and personalizing user experiences. As technology evolves, it’s crucial for IT professionals to stay informed and open to exploring these new tools, while carefully considering the benefits and challenges they present."
    } ,
  
    {
      "title"    : "How to Install the New Apple Ferret LLM on Your Mac",
      "url"      : "/en/ferret-apple-mac-llm/",
      "img"      : "PHBpY3R1cmU+CiAgPHNvdXJjZSBzcmNzZXQ9Ii9hc3NldHMvaW1nL2ZlcnJldC1hcHBsZS1tYWMtbGxtLTMwMC1jZGM2NjdiM2UuYXZpZiAxLjB4LCAvYXNzZXRzL2ltZy9mZXJyZXQtYXBwbGUtbWFjLWxsbS02MDAtY2RjNjY3YjNlLmF2aWYgMi4weCIgdHlwZT0iaW1hZ2UvYXZpZiI+CiAgPHNvdXJjZSBzcmNzZXQ9Ii9hc3NldHMvaW1nL2ZlcnJldC1hcHBsZS1tYWMtbGxtLTMwMC1iMTI0ZTdhZDQuanBnIDEuMHgsIC9hc3NldHMvaW1nL2ZlcnJldC1hcHBsZS1tYWMtbGxtLTYwMC1iMTI0ZTdhZDQuanBnIDIuMHgiIHR5cGU9ImltYWdlL2pwZWciPgogIDxpbWcgc3JjPSIvYXNzZXRzL2ltZy9mZXJyZXQtYXBwbGUtbWFjLWxsbS0zMDAtYjEyNGU3YWQ0LmpwZyIgYWx0PSJIb3cgdG8gSW5zdGFsbCB0aGUgTmV3IEFwcGxlIEZlcnJldCBMTE0gb24gWW91ciBNYWMiPgo8L3BpY3R1cmU+Cg==",
      "isencoded" : "y",
      "tags"     : "AI, LLM, Ferret, MacOS",
      "content"  : "Developed in collaboration with Cornell University, Apple has very discreetly presented on GitHub, its very first LLM model, Ferret. Well after OpenAI, Meta, and Google, Apple is, in turn, entering the LLM race. However, the approach is different. Open source and multimodal, this model combines computer vision and natural language processing, offering unique capabilities in terms of understanding and analyzing text and images. More powerful than OpenAI’s GPT-4, according to Apple, this advance promises to enrich the company’s devices, especially in improving data interpretation and perhaps even Siri. Ironically, although Apple stopped using and supporting NVIDIA products since 2016, its Ferret model was developed using NVIDIA’s highly performant graphics cards, the A100s. The source code available on GitHub therefore does not work on Apple products. Let’s see how to remedy this and test the capabilities and responsiveness of this very first version of Ferret on our machines “Designed by Apple”. CUDA, MPS, and Prerequisites Ferret Installation Step 1: Configure Git Step 2: Download Ferret’s Source Code Step 3: Create a Python Virtual Environment Step 4: Install the Vicuna Model Step 5: Download Ferret Weights Step 6: Transform Vicuna into Ferret Launching Ferret Demo Step 7: First Terminal Step 8: Second Terminal Step 9: Third Terminal Step 10: Access the Demo Demo Testing Test 1: The Dog and the Ferrets Test 2: The Jetty on the Lake Test 3: The Living Room Test Summary Optimizing the Ferret Model for Apple Devices Model Quantization Model Sparsification and Pruning Model Distillation Split Deployment Advanced Use of Flash Memory Conclusion The Pros The Cons Uses CUDA, MPS, and Prerequisites The greatest adherence of Ferret’s code lies in its use of CUDA, NVIDIA’s GPU framework. Fortunately, the library used is PyTorch, which has been ported and optimized for Apple Silicon GPUs. The porting to Apple’s Metal API and its Metal Performance Shaders (MPS) framework will be all the simpler. Another point to note is the basic documentation on installing and using Ferret on GitHub, proof, if any were needed, that Apple reserves its LLM model solely for researchers as specified in its terms of use. So let’s find out together how to run this Ferret on our Macs. For this, keep in mind that a substantial amount of GPU memory is required. Our tests were conducted on a MacBook Pro M1 Max with 64 GB of memory. Ferret Installation Step 1: Configure Git Start by installing Git Large File Storage (LFS) to manage the large file sizes we’re going to need: brew install git-lfs git lfs install Shell Step 2: Download Ferret’s Source Code I’ve adapted the Ferret code for Silicon processors and Apple’s Metal Performance Shaders (MPS) framework. It is available on this repo: jeanjerome/ml-ferret&amp;nbsp;Public The main branch contains the original code from Apple. The silicon branch contains my adapted version. This structuring makes it easier to compare the two versions. To download the code: git clone https://github.com/jeanjerome/ml-ferret cd ml-ferret git switch silicon Shell Step 3: Create a Python Virtual Environment Ferret uses Python, so let’s create a virtual environment with Conda to isolate dependencies: conda create -n ferret python=3.10 -y conda activate ferret Shell Then, install the necessary dependencies: pip install --upgrade pip pip install -e . pip install pycocotools pip install protobuf==3.20.0 Shell Step 4: Install the Vicuna Model Place the Vicuna model in the ./model directory at the root of the project: mkdir -p ./model git lfs install git clone https://huggingface.co/lmsys/vicuna-13b-v1.3 model/vicuna-13b-v1.3 Shell Wait for the model to download. Step 5: Download Ferret Weights Apple provides a file with the differences between Vicuna and Ferret’s weights. Download them: mkdir -p ./delta curl -o ./delta/ferret-13b-delta.zip https://docs-assets.developer.apple.com/ml-research/models/ferret/ferret-13b/ferret-13b-delta.zip unzip ./delta/ferret-13b-delta.zip -d ./delta Shell This step may take some time. Step 6: Transform Vicuna into Ferret To apply Ferret’s modifications to Vicuna: python -m ferret.model.apply_delta -- --base ./model/vicuna-13b-v1.3 -- --target ./model/ferret-13b-v1-3 -- --delta ./delta/ferret-13b-delta Shell Follow the logs to confirm the operation is proceeding well: /opt/homebrew/Caskroom/miniconda/base/envs/ferret/lib/python3.10/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable. warn(&quot;The installed version of bitsandbytes was compiled without GPU support. &quot; &#39;NoneType&#39; object has no attribute &#39;cadam32bit_grad_fp32&#39; Loading base model Loading checkpoint shards: 100%|██████████████████████████████████████████████████████| 3/3 [00:04&amp;lt;00:00, 1.57s/it] Loading delta Loading checkpoint shards: 100%|██████████████████████████████████████████████████████| 3/3 [00:08&amp;lt;00:00, 2.94s/it] Applying delta Applying delta: 100%|█████████████████████████████████████████████████████████████| 421/421 [00:16&amp;lt;00:00, 26.04it/s] Saving target model Log You have now installed Ferret on your Mac! Launching Ferret Demo The demo provided by Apple allows you to appreciate the capabilities of the new model through a web interface. This demonstrator includes a controller, a Gradio web server, and a model worker that loads the weights and performs inference. Launch the demo with these commands in three separate terminals: Step 7: First Terminal Start the controller: conda activate ferret python -m ferret.serve.controller --host 0.0.0.0 --port 10000 Shell Wait for the message indicating that the controller is operational: Uvicorn running on http://0.0.0.0:10000 (Press CTRL+C to quit) Starting the controller Step 8: Second Terminal Launch the web server: conda activate ferret python -m ferret.serve.gradio_web_server --controller http://localhost:10000 --model-list-mode reload --add_region_feature Shell Wait for the line Running on local URL: http://0.0.0.0:7860 to appear: Starting the Web Interface Step 9: Third Terminal Run the model worker: conda activate ferret python -m ferret.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path ./model/ferret-13b-v1-3 --add_region_feature Shell Monitor the logs to ensure everything is working correctly: Uvicorn running on http://0.0.0.0:40000 (Press CTRL+C to quit) Starting the Model Step 10: Access the Demo Click on the address http://localhost:7860/ to access the web interface of the demo. Ferret Demonstrator Interface Demo Testing Apple has included tests with images and pre-filled prompts to evaluate Ferret. Let’s test them! Test 1: The Dog and the Ferrets Select the first image (the white dog) in Examples. In Referring Input Type, choose Sketch. Draw on the dog and the ferret in Image &amp;amp; Sketch. Finally, click the Submit button next to the prompt What’s the relationship between object [region0] and object [region1]? Test of the dog and the ferrets The answers vary, but remain consistent with the image and probably depend on the pixels selected: Trial 1: The object [497, 303, 791, 658] is a dog, and it’s standing next to another dog [307, 507, 459, 690]. They seem to be enjoying their time together in the snow. Trial 2: The object [457, 283, 817, 701] is a dog, and the object [318, 498, 464, 707] is a ferret. The dog and the ferret appear to be sitting together in the snow, suggesting a friendly interaction or companionship. LimitationsFerret can consume a lot of memory, and it may be necessary to restart the model worker between tests. On my MacBook M1 Max with its 64 GB, 62 GB of RAM were used, 2 GB of files were cached, and 20 GB for the swap file. Test 2: The Jetty on the Lake Select the second image (the jetty). Without drawing, submit the prompt by clicking the Submit button next to the prompt What are the things I should be cautious about when I visit here? Tell me the coordinates in response. Test of the jetty on the lake Result: When you’re on the dock [218, 535, 810, 993], you should be mindful of a few things. Firstly, the water [0, 518, 998, 997] is a crucial aspect, and you should be careful not to go too close to the edge [731, 543, 998, 591]. Secondly, the surrounding trees [875, 366, 940, 510] [707, 385, 757, 535] [650, 393, 695, 529] [512, 420, 540, 526] [775, 375, 818, 522] could pose a hazard if you’re not cautious. Lastly, it’s important to be aware of the path [218, 535, 810,] to ensure you stay on the right track to avoid any unintentional falls or injuries. Test 3: The Living Room Choose the third image (the living room). Submit the prompt without drawing by clicking the Submit button next to the prompt Describe the image in detail. Ferret begins to respond… Test of the living room Result: Ferret writes its response slowly. It seems to be constrained by the 64 GB of RAM. It still manages to write this: The image depicts a cozy living room [12, 1, 982, 989] with various pieces of furniture [490, 588, 709, 883] and decorations [244, 27, 394, 110] [247, 28, 274, 109] [378, 43, 411, 108]. In the center, there is a colorful, multicolored rug [90, 689, 746, 994] on the floor. A flat-screen TV [1, 394, 93, 723] is mounted on the left wall, with remotes [5, 679, 60, 734] placed nearby. Two couches [513, 421, 938, 769] [501, 577, 993, 995] are arranged in the room, with one along the right wall and another on the far end of the room. A few chairs are also present, scattered around the room. A dining table [710, 392, 954, 511] [ Before crashing miserably with a NETWORK ERROR DUE TO HIGH TRAFFIC. PLEASE REGENERATE OR REFRESH THIS PAGE. (error_code: 1) In the logs, one can read: Caught Unknown Error MPS backend out of memory (MPS allocated: 26.50 GB, other allocations: 55.14 GB, max allowed: 81.60 GB). Tried to allocate 10.00 KB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure). No solution then for my MacBook Pro, the 80 GB occupied by Ferret is not enough… Test Summary After this series of tests, it’s clear that Ferret demonstrates an impressive capability to analyze and describe an image and to transcribe it into natural language, offering new possibilities. However, it also became apparent that Ferret can be subject to high memory consumption issues, particularly during prolonged processing, leading to noticeable slowdowns as memory begins to compress, and even crashes. Resources consumed by Ferret When Ferret operates normally, GPU usage peaks up to 90%, indicating that the neural network activity takes place in this part of the SoC (System on Chip). By contrast, CPU activity remains stable at around 20%. However, analyzing Ferret’s resource consumption tracking reveals that periods of slowdown in the model’s responses coincide with phases of RAM memory compression. GPU activity then drops to around 20% while CPU activity stays around 20%. The problem seems to reside in the memory, suggesting that the system is swapping or compressing/decompressing memory due to a lack of sufficient RAM available for the model and its processing. Optimizing the Ferret Model for Apple Devices Following the analysis of the installation and testing of the 13B model, it becomes evident that Apple must tackle the challenge of adapting its model to work optimally on its Macs and iPhones. To this end, Apple is considering various strategies, according to rumors and information available on the internet. Some of these strategies are already well-established, while others come directly from its research labs: Model Quantization Quantization reduces the precision of the model’s weights, thereby decreasing its size and resource consumption without significantly compromising prediction performance. While traditional models may use weights represented by 32-bit floating-point numbers (float32), quantization reduces this precision to more compact formats, such as 16 bits (float16) or even 8 bits (int8). This is particularly advantageous for iPhones, where storage space and computational capacity are more limited than on a Mac. The availability of a 7B version of Ferret illustrates this. Installing the 7B Version of FerretIf you have already followed the steps to install the 13B format of Ferret, installing the 7B version will be greatly simplified. The majority of the installation steps remain the same, with one exception: there is no need to recreate a virtual environment. To install the 7B Ferret, rerun the commands by replacing all the 13s with 7s. Model Sparsification and Pruning These are two related model compression techniques aimed at optimizing neural networks by reducing their complexity, for example, by decreasing the number of neurons or removing connections with weights close to zero without significantly compromising performance. Model Distillation This is a model optimization technique that involves transferring the knowledge from a large, complex model (the “teacher” model) to a smaller, simpler model (the “student” model). The goal is to teach the student model to replicate the performance of the teacher model while being lighter and faster to execute, preserving the quality of predictions. Split Deployment This method involves sharing the computational tasks of a model between local devices and the cloud. This approach leverages the computational capabilities of the cloud for heavy operations while performing lighter tasks locally. However, this strategy seems unlikely for Apple, which favors entirely local solutions or internal optimizations. Apple aims to maintain user data privacy and security by minimizing dependence on the cloud. Advanced Use of Flash Memory In a recently published article by researchers at Apple, LLM in a flash: Efficient Large Language Model Inference with Limited Memory, it is suggested that Apple considers using flash memory to store model parameters. These parameters are then dynamically transferred to DRAM during inference, thus reducing the volume of data exchanged and speeding up processing on devices with limited DRAM, like iPhones. This approach, combined with the use of innovative data management techniques such as windowing and row-column bundling, further optimizes the amount of data to be transferred and indirectly the speed of inference. Conclusion In summary, the integration of Ferret, Apple’s latest LLM model, on machines equipped with Apple Silicon processors, represents a significant advancement in the field of artificial intelligence. Despite some challenges inherent in adapting the original code, designed for NVIDIA GPUs, the efforts to port it to Apple’s Metal architecture have been straightforward. This advancement raises exciting questions about how Apple will execute its multimodal language model on devices with more limited resources like iPhones. There is no doubt that Apple has already found ways to run its Ferret on iPhones, utilizing advanced optimization techniques. Apple’s ability to effectively adapt cutting-edge technologies to their devices demonstrates their mastery of AI within their hardware and software ecosystem. It will be interesting to see how these developments will influence the user experience on our iPhones and Macs and what new uses Apple will introduce into our daily lives. Rumors are talking about a completely renewed user interface in iOS 18! We will surely know more at WWDC 2024 next June. The Pros Advantages of Ferret Description Multimodal Capabilities Combination of computer vision and natural language processing for enriched understanding and analysis of text and images. Enhanced Performance Ability to perform complex tasks with increased efficiency. Optimized User Interaction Improvement of interaction with users through better understanding of natural language, the external environment, and more accurate responses. Potential for Innovation Opens up new possibilities for innovative applications in various fields such as translation, voice assistance, augmented reality, and virtual reality. The Cons Disadvantages of Ferret Description Technical Complexity Implementation and optimization of the model can be complex for use on iPhones. Resource Needs Even when optimized, the model will still require significant resources in terms of processing and memory. Integration Limits Integrating with the existing ecosystem of iOS applications could pose real challenges. Energy Consumption Advanced AI use may lead to increased energy consumption, affecting battery life. Privacy Concerns Data management and privacy can be concerns, especially in applications sensitive to privacy. Uses Potential Functionality of Ferret in iOS and MacOS Description and Impact Instant Translation Significant improvement in real-time text translation through advanced deep learning capabilities. Can integrate with all apps from Safari to Pages. Optimized Voice Assistant Improvement of Siri (finally!) for better natural language understanding and more natural and effective interactions. Augmented and Virtual Reality Enrichment of augmented and virtual reality experiences through more sophisticated image and scene analysis in the Photos app and Camera app. Generative Text and Image Assistance Extraction of lyrics in Apple Music (with translation), support for text and image generation in Apple Pages and Keynote, or even coding assistance in Xcode."
    } ,
  
    {
      "title"    : "Mixtral 8x7B Locally: Train Your LLM with Your Own Data",
      "url"      : "/en/mixtral-ollama-llamaindex-llm/",
      "img"      : "PHBpY3R1cmU+CiAgPHNvdXJjZSBzcmNzZXQ9Ii9hc3NldHMvaW1nL21peHRyYWwtb2xsYW1hLWxsYW1haW5kZXgtbGxtLTMwMC00YTkyNGQ2YWUuYXZpZiAxLjB4LCAvYXNzZXRzL2ltZy9taXh0cmFsLW9sbGFtYS1sbGFtYWluZGV4LWxsbS02MDAtNGE5MjRkNmFlLmF2aWYgMi4weCIgdHlwZT0iaW1hZ2UvYXZpZiI+CiAgPHNvdXJjZSBzcmNzZXQ9Ii9hc3NldHMvaW1nL21peHRyYWwtb2xsYW1hLWxsYW1haW5kZXgtbGxtLTMwMC0yY2ZjZmEzYjMuanBnIDEuMHgsIC9hc3NldHMvaW1nL21peHRyYWwtb2xsYW1hLWxsYW1haW5kZXgtbGxtLTYwMC0yY2ZjZmEzYjMuanBnIDIuMHgiIHR5cGU9ImltYWdlL2pwZWciPgogIDxpbWcgc3JjPSIvYXNzZXRzL2ltZy9taXh0cmFsLW9sbGFtYS1sbGFtYWluZGV4LWxsbS0zMDAtMmNmY2ZhM2IzLmpwZyIgYWx0PSJNaXh0cmFsIDh4N0IgTG9jYWxseTogVHJhaW4gWW91ciBMTE0gd2l0aCBZb3VyIE93biBEYXRhIj4KPC9waWN0dXJlPgo=",
      "isencoded" : "y",
      "tags"     : "AI, LLM, Mixtral, Llama, Privacy",
      "content"  : "The newly established French company Mistral AI has managed to position itself as a leading player in the world of Artificial Intelligence. With its Large Language Model (LLM), Mixtral 8x7B, based on an innovative concept of Mixture of Experts (MoE), it competes with giants like Meta and its Llama 2 70B model, as well as OpenAI and its famous ChatGPT 3.5. Mistral AI’s adoption of the open-source Apache 2.0 license democratizes access to this cutting-edge technology, allowing a broad range of users and developers to exploit and customize this technology according to their specific needs. Let’s get hands-on with this model and see how to make the most of Mixtral 8x7B by customizing a LLM model with our own data, locally, to preserve its confidentiality. This approach finally offers unprecedented flexibility and reliability for developers and businesses seeking to integrate AI into their projects, all the while maintaining complete control over their data. Understanding AI Jargon The Concepts Behind Mixtral 8x7B Installing Mixtral 8x7B Step 1: Installing Ollama Step 2: Starting Mixtral 8x7B Testing the Intrinsic Capabilities of Mixtral 8x7B Optional Step: Create a Python Virtual Environment Step 3: Implementing the Test Application Step 4: Querying the Model Specializing Our Mixtral Model Step 5: Implementing the Model Specialization Step 6: Querying the New Model Conclusion Understanding AI Jargon Before diving further into our approach, it may be helpful to understand the terms we will use that are at the core of the currently popular AI models: LLM (Large Language Models): These are AI models designed to understand and generate natural language. They are based on vast datasets, with perhaps the most well-known being OpenAI’s ChatGPT. However, there are many others like Google’s BERT, Meta’s Llama, Hugging Face’s BLOOM, Technology Innovation Institute’s Falcon, and the one of our interest today, Mixtral by Mistral AI. RAG (Retrieval-Augmented Generation): This is a means of adding new information to an LLM or specializing it in a specific domain. It requires vectorial databases that allow the LLM to use these new pieces of information and provide more contextual responses. LangChain: This is a development framework dedicated to LLMs. It allows for the combination of a wide variety of language models including LLMs with external sources or user input components. It has become de facto the most used open source framework in applications utilizing LLMs. Token: This represents the basic unit in language processing by AI models. It can represent a word, character, or part of a word like a phoneme, for example. Thus, it is this abstraction that LLM models manipulate, and its size influences their capacity to analyze and generate language. Mixture-of-Experts (MoE): This is a technique where an AI model is divided into specialized ‘experts,’ each handling a different part of the information. Depending on the context of the request, the most relevant expert is solicited, which allows for a more precise and tailored response. This approach improves the quality of the information generated by leveraging the specific skills of each expert. The Concepts Behind Mixtral 8x7B Mixtral 8x7B is a Large Language Model (LLM) of the Mixture-of-Experts (MoE) type. It operates by directing each token to 2 out of 8 groups of experts that make up the model. The outputs from these experts are then combined to produce the final result, thus optimizing the processing and generation of the response. Each expert within the Mixtral 8x7B model has about 7 billion parameters, which explains the model’s name. When processing a request, Mixtral 8x7B uses only 12.6 billion parameters (approximately 2x7B), which speeds up its processing and reduces the necessary resources. The specialization of each expert allows Mixtral 8x7B to outperform larger models like Llama 2 70B (70 billion parameters), while being six times faster. Moreover, it equals or surpasses GPT3.5 on most standard benchmarks. Licensed under Apache 2.0, Mixtral 8x7B can be reused by developers, researchers, and companies, thus fostering innovation and collaboration in the field of AI. This open license allows for extensive adaptation and customization of the model, making the technology modifiable for a wide range of applications. Installing Mixtral 8x7B Step 1: Installing Ollama Previously, installing and operating an AI model on one’s computer was a very complex task. However, the introduction of Ollama, an open-source software, has significantly simplified this process. Indeed, Ollama allows users to easily run advanced models such as Mixtral 8x7B, directly on their own systems, paving the way for the democratization of these technologies. To install Ollama on your computer: Go to the GitHub project and follow the instructions: ollama/ollama&amp;nbsp;Public Or download the Ollama installation binary directly from https://ollama.ai/download and start the installation on your computer. Step 2: Starting Mixtral 8x7B To activate the Mixtral 8x7B neural network, run this command in your terminal: ollama run mixtral Shell During the first execution, Ollama will download the Mixtral 8x7B model, which is 26 GB in size. The download time will depend on your internet connection. It is necessary for your system to have at least 48 GB of RAM to efficiently run Mixtral 8x7B. In this scenario, choosing a Mac Apple Silicon with its unified memory presents a significant advantage, as it provides the GPU with access to a vast amount of memory, thereby enhancing its processing capabilities. Testing the Intrinsic Capabilities of Mixtral 8x7B In this first test, we will examine Mixtral’s ability to generate Java code using the Spring Boot 3.2 framework. This test will serve as a benchmark before specializing our LLM specifically for Spring Boot 3.2, thus providing a point of comparison to evaluate the improvements made by specialization. Optional Step: Create a Python Virtual Environment Depending on your preferences, you may create a virtual environment to isolate the test program and its dependencies. There are several tools for creating Python virtual environments, including: venv: Integrated into Python 3.3 and later, it allows the creation of lightweight virtual environments. virtualenv: An older, independent tool that offers additional features compared to venv. conda: Particularly useful for managing environments that require complex dependencies, including non-Python libraries. pipenv: Combines pip and virtualenv for an improved dependency management workflow. poetry: Manages dependencies and virtual environments, focusing on ease of use and reproducibility. With conda, to create a virtual environment named mixtral_ollama under Python 3.11, execute the commands: conda create --name mixtral_ollama python=3.11 conda activate mixtral_ollama Shell Step 3: Implementing the Test Application This step involves creating two files. The first, requirements.txt, contains the necessary dependencies for the application. The second, reference_test.py, is a Python script that sends a request to Mixtral 8x7B. We will use the LlamaIndex library to facilitate the use of LLMs and, subsequently, the integration of different data sources. requirements.txt : llama-index Python reference_test.py : from llama_index.llms import Ollama llm = Ollama(model=&quot;mixtral&quot;) prompt = ( &quot;Create a REST controller class in Java for a Spring Boot 3.2 application. &quot; &quot;This class should handle GET and POST requests, and include security and &quot; &quot;configuration annotations.&quot; ) response = llm.complete(prompt) print(response) Python Step 4: Querying the Model pip install -r requirements.txt python reference_test.py Bash We receive the following response from Mixtral: As Spring Boot 3.2 isn’t out yet, I’ll create a REST controller class for the latest stable version of Spring Boot (2.6.x) using Java 8. Here’s a simple example of a REST controller class to handle GET and POST requests with security and configuration annotations: import org.springframework.beans.factory.annotation.Autowired; import org.springframework.security.access.prepost.PreAuthorize; import org.springframework.web.bind.annotation.*; @RestController @RequestMapping(&quot;/api/example&quot;) @PreAuthorize(&quot;hasRole(&#39;ROLE_USER&#39;)&quot;) // Annotation de sécurité de base public class ExampleController { private final ExampleService exampleService; @Autowired public ExampleController(ExampleService exampleService) { this.exampleService = exampleService; } // Endpoint GET @GetMapping public Object getAll() { return exampleService.getAll(); } // Endpoint POST @PostMapping public Object create(@RequestBody ExampleDto exampleDto) { return exampleService.create(exampleDto); } } Java This example assumes an ExampleService handling business requests, returning Object or List. For Spring Boot 3.2 (once available), some dependencies or annotations might change, but the general approach remains the same: mapping HTTP requests to controller class methods, using annotations to configure controller behavior. Note: In this example, a class-level @PreAuthorize annotation requires authentication for all requests to this class’s endpoints, with the user needing “ROLE_USER” role. Modify as per your application’s needs. Mixtral 8x7B generated code based on Spring Boot 2.6.x as this is the version available in the data that trained the model. Since Spring Boot version 3.2 was only recently released, it was not included in these data. This explains why the model has no knowledge of Spring Boot in version 3.x. Specializing Our Mixtral Model Now, we will specialize our model. To do this, we will provide it with PDF documents containing specific information related to the new targeted context, in our example, the Spring Boot 3.2 reference documentation. Step 5: Implementing the Model Specialization We will place the PDF file of the Spring Boot reference documentation in a ./data subdirectory: mkdir ./data curl -o ./data/spring-boot-reference.pdf https://docs.spring.io/spring-boot/docs/3.2.1/reference/pdf/spring-boot-reference.pdf Bash Add new dependencies to our project in the requirements.txt file: llama-index qdrant_client pypdf transformers torch Python Implement the specialization script to inject the PDF into the MixTral 8x7B model, specialized_test.py. The vector database, Qdrant, acts as a RAG for our model: from pathlib import Path import qdrant_client from llama_index import VectorStoreIndex, ServiceContext, SimpleDirectoryReader from llama_index.llms import Ollama from llama_index.storage.storage_context import StorageContext from llama_index.vector_stores.qdrant import QdrantVectorStore # Loading the documents from the disk documents = SimpleDirectoryReader(&quot;./data&quot;).load_data() # Initializing the vector store with Qdrant client = qdrant_client.QdrantClient(path=&quot;./qdrant_data&quot;) vector_store = QdrantVectorStore(client=client, collection_name=&quot;springboot&quot;) storage_context = StorageContext.from_defaults(vector_store=vector_store) # Initializing the Large Language Model (LLM) with Ollama # The request_timeout may need to be adjusted depending on the system&#39;s performance capabilities llm = Ollama(model=&quot;mixtral&quot;, request_timeout=120.0) service_context = ServiceContext.from_defaults(llm=llm, embed_model=&quot;local&quot;) # Creating the index, which includes embedding the documents into the vector store index = VectorStoreIndex.from_documents(documents, service_context=service_context, storage_context=storage_context) # Querying the index with a specific question query_engine = index.as_query_engine() prompt = ( &quot;Create a REST controller class in Java for a Spring Boot 3.2 application. &quot; &quot;This class should handle GET and POST requests, and include security and &quot; &quot;configuration annotations.&quot; ) response = query_engine.query(prompt) print(response) Python Step 6: Querying the New Model pip install -r requirements.txt python specialized_test.py Bash We obtain a completely different response from Mixtral: Here’s an example of a Java-based REST controller class with the specified requirements for a Spring Boot 3.2 application: import java.time.Duration; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.context.annotation.Configuration; import org.springframework.http.HttpStatus; import org.springframework.http.ResponseEntity; import org.springframework.security.access.prepost.PreAuthorize; import org.springframework.web.bind.annotation.GetMapping; import org.springframework.web.bind.annotation.PostMapping; import org.springframework.web.bind.annotation.RequestBody; import org.springframework.web.bind.annotation.RestController; import org.springframework.web.client.RestTemplate; import org.springframework.web.client.RestTemplateBuilder; @Configuration(proxyBeanMethods = false) @RestController public class MyRestController { private final RestTemplate restTemplate; @Autowired public MyRestController(RestTemplateBuilder builder) { this.restTemplate = builder .setConnectTimeout(Duration.ofSeconds(5)) .setReadTimeout(Duration.ofSeconds(2)) .build(); } @GetMapping(&quot;/public-data&quot;) public ResponseEntity&amp;lt;String&amp;gt; getPublicData() { return new ResponseEntity&amp;lt;&amp;gt;(&quot;Public data&quot;, HttpStatus.OK); } @PostMapping(&quot;/private-data&quot;) @PreAuthorize(&quot;hasRole(&#39;ROLE_USER&#39;)&quot;) public ResponseEntity&amp;lt;String&amp;gt; postPrivateData(@RequestBody String request) { // Process the private data here return new ResponseEntity&amp;lt;&amp;gt;(&quot;Private data saved&quot;, HttpStatus.CREATED); } } Java This example includes: A REST controller (@RestController) with two endpoints: one for GET requests (public), and another for POST requests (private). Annotations for Spring Security to restrict access to the /private-data endpoint. A custom RestTemplateBuilder configuration at the class level with a constructor dependency injection. The controller uses constructor-based dependency injection instead of field-based to ensure proper initialization and testability. The specialized model now offers a more sophisticated REST controller implementation for Spring Boot 3.2. However, I haven’t verified this code or confirmed its specificity to Spring Boot 3. The aim was to test the model’s specialization capability rather than the exact accuracy of the generated code. Conclusion The combination of Mixtral 8x7B, Ollama, and LlamaIndex marks a significant advancement in customizing AI models and the development of tailor-made applications, by merging technical power with ease of use. This synergy not only enhances the protection of private data but also benefits from an open and free license, thereby encouraging collaboration and innovation. This makes artificial intelligence more accessible and adaptable to a variety of projects and users, democratizing its use in diverse contexts."
    } ,
  
    {
      "title"    : "Turn Your Nginx Server into a Fortress with Fail2ban and UFW",
      "url"      : "/en/ufw-fail2ban-nginx/",
      "img"      : "PHBpY3R1cmU+CiAgPHNvdXJjZSBzcmNzZXQ9Ii9hc3NldHMvaW1nL3Vmdy1mYWlsMmJhbi1uZ2lueC0zMDAtOTA1MTM0YzMzLmF2aWYgMS4weCwgL2Fzc2V0cy9pbWcvdWZ3LWZhaWwyYmFuLW5naW54LTYwMC05MDUxMzRjMzMuYXZpZiAyLjB4IiB0eXBlPSJpbWFnZS9hdmlmIj4KICA8c291cmNlIHNyY3NldD0iL2Fzc2V0cy9pbWcvdWZ3LWZhaWwyYmFuLW5naW54LTMwMC1lODFlZTJlNGEuanBnIDEuMHgsIC9hc3NldHMvaW1nL3Vmdy1mYWlsMmJhbi1uZ2lueC02MDAtZTgxZWUyZTRhLmpwZyAyLjB4IiB0eXBlPSJpbWFnZS9qcGVnIj4KICA8aW1nIHNyYz0iL2Fzc2V0cy9pbWcvdWZ3LWZhaWwyYmFuLW5naW54LTMwMC1lODFlZTJlNGEuanBnIiBhbHQ9IlR1cm4gWW91ciBOZ2lueCBTZXJ2ZXIgaW50byBhIEZvcnRyZXNzIHdpdGggRmFpbDJiYW4gYW5kIFVGVyI+CjwvcGljdHVyZT4K",
      "isencoded" : "y",
      "tags"     : "UFW, Fail2ban, Nginx, Security, Cybersecurity, DevSecOps",
      "content"  : "In today’s digital world, web server security is not just an option, but an absolute necessity. In an era where cyber attacks are multiplying and constantly evolving, effectively protecting one’s online infrastructure has become paramount. This article focuses on securing a Nginx web server, a platform widely used for its reliability and performance. We will address two essential tools in the arsenal of computer security: Fail2ban and UFW (Uncomplicated Firewall). Fail2ban is an intrusion prevention software that protects your server against unauthorized access attempts, often marked by multiple failed login attempts. On the other hand, UFW offers simplified firewall management, allowing easy control of incoming and outgoing traffic. In this article, we will detail how these tools can be used together to enhance the security of your Nginx server. We will explore the steps of installation and configuration, guiding you through the process of setting up effective security rules. Introduction to Fail2ban and UFW Fail2ban: The Digital Bodyguard UFW: A Simplified Firewall Approach Essential Complementarity Basic Installation and Configuration Step 1: Installing UFW Step 2: Installing Fail2ban Step 3: Understanding How Fail2ban Works Step 4: Configuring Fail2ban’s Basic Rules Advanced Configuration of UFW Step 5: Block Everything and Open Only What’s Necessary Step 6: Activating the Rules Step 7: Add Additional Rules Based on Your Services Advanced Configuration of Fail2ban Step 8: List Preconfigured Filters on Your Server Step 9: Add Jails to Your Configuration Step 10: Verification and Testing Visualization and Analysis of Bans Data Extraction Calculating Some Statistics Scans by Country Results Interpretation of Blocked Scans by Country Scans by Organization Results Interpretation of Blocked Scans by Organization Scans by City Results Interpretation of Blocked Scans Statistics by City Conclusion Introduction to Fail2ban and UFW Fail2ban: The Digital Bodyguard Fail2ban is an indispensable tool for server security. Its primary role is to monitor your server’s logs for signs of malicious activities, especially repeated unsuccessful login attempts. By detecting these attempts, often indicative of a brute-force attack, Fail2ban intervenes by temporarily banning the attacker’s IP address, thereby quarantining it to prevent future intrusions. This software operates through “filters” defined by the user, which specify the conditions under which an IP address should be banned. These filters are associated with “actions”, such as modifying firewall rules to block traffic from the banned address. Thus, using Fail2ban is a proactive method to protect your server against common attacks, while remaining flexible enough to adapt to various threat scenarios. UFW: A Simplified Firewall Approach UFW, standing for Uncomplicated Firewall, is an easy-to-use firewall for Linux-based operating systems. True to its name, UFW aims to simplify firewall management while offering robust protection. It serves as a user interface for iptables, the default firewall in Linux. With UFW, you can easily set up rules that determine which traffic is allowed to enter or leave your server. These rules can be defined based on ports, protocols, and IP addresses. UFW also allows for the configuration of more complex rules if necessary, while maintaining a simple interface for less experienced users. Essential Complementarity The combination of Fail2ban and UFW provides a solid layer of security for your Nginx server. While Fail2ban focuses on preventing brute-force attacks by banning suspicious IPs, UFW manages overall inbound and outbound traffic, thus offering a barrier against a wide range of threats. By using these two tools together, you create a server environment that is not only resistant to the most common attacks but also adaptable to emerging threats. Basic Installation and Configuration Quite an interesting illustration of a server setup by DALL·E While the following instructions are specific to Ubuntu Linux distribution, the principles and commands are largely transferable to other Linux systems. With minor adaptations, these steps can be applied to various distributions, making this guide useful for a wide range of Linux users. Step 1: Installing UFW 1. Install UFW: To install UFW, run: sudo apt-get update sudo apt-get install ufw Shell Step 2: Installing Fail2ban 1. Install Fail2ban: Execute the following commands to install Fail2ban on your Ubuntu server: sudo apt-get update sudo apt-get install fail2ban Shell 2. Activate the Fail2ban service: To start the service and enable it automatically at system startup, run the following commands: sudo systemctl start fail2ban sudo systemctl enable fail2ban Shell 3. Verify the installation: To ensure that the service has started and is functioning correctly, launch: sudo systemctl status fail2ban Shell You should see: ● fail2ban.service - Fail2Ban Service Loaded: loaded (/lib/systemd/system/fail2ban.service; enabled; vendor preset: enabled) Active: active (running) since Fri 2023-12-22 00:16:31 CET; 1 day 9h ago Docs: man:fail2ban(1) Main PID: 601234 (fail2ban-server) Tasks: 19 (limit: 11829) Memory: 39.8M CPU: 2h 43min 80ms CGroup: /system.slice/fail2ban.service └─601234 /usr/bin/python3 /usr/bin/fail2ban-server -xf start Dec 22 00:16:31 myserver systemd[1]: Started Fail2Ban Service. Dec 22 00:16:32 myserver fail2ban-server[601234]: Server ready Output Step 3: Understanding How Fail2ban Works Before delving into configuration, it is crucial to understand how Fail2ban operates. This software relies on three main components located in different directories: jails, filters, and actions. 1. Jails: These are sets of rules defining when and how an IP address should be banned. A jail is defined by log files to monitor, using filters (in the form of regex), and triggers actions when failures are detected (the filters return a result). Fail2ban includes predefined jail configurations in /etc/fail2ban/jail.conf. 2. Filters: They are used to analyze service logs via regex to detect suspicious behavior, such as intrusion attempts. They are found in /etc/fail2ban/filter.d/. 3. Actions: These can include banning an IP address, sending notifications, or executing custom scripts. The actions, defining commands to ban or unban an IP address, are located in /etc/fail2ban/action.d/. Step 4: Configuring Fail2ban’s Basic Rules WarningDo not directly modify the files /etc/fail2ban/fail2ban.conf and /etc/fail2ban/jail.conf: these files, containing fail2ban’s default configurations, can be reset during system updates. 1. Create a dedicated configuration file: To do this, open your favorite editor, here nano, by executing the command: sudo nano /etc/fail2ban/jail.d/custom.conf Shell 2. Override the base configurations: All default parameters and configurations are found in the file /etc/fail2ban/jail.conf. Here is a list of important parameters to override and adapt according to the behavior you desire: bantime: Defines the duration of an IP ban (default 10 minutes, recommended several hours or days). findtime: Period during which anomalies are searched for in the logs. ignoreip: List of IPs to ignore, including yours to avoid self-banning. maxretry: Number of failed attempts allowed before banning. Also define the use of UFW to take control of the banning (banaction and banaction_allports). Here is an example of a drastic configuration, banning any first intrusion attempt for 1 day. We also define the use of UFW, (note the local IP addresses that you may need to adjust according to your local network configuration): [DEFAULT] bantime = 1d findtime = 1d ignoreip = 127.0.0.1/8 192.168.0.0/16 maxretry = 1 banaction = ufw banaction_allports = ufw Conf 3. Restart the Fail2ban service: To apply your modifications, restart the Fail2ban service by launching the command: sudo systemctl restart fail2ban Shell 4. Verify the service status: sudo fail2ban-client status Shell Which should give you: $ sudo fail2ban-client status Status |- Number of jail: 1 `- Jail list: sshd Output Advanced Configuration of UFW Allegorical or even wacky illustration of UFW by DALL·E We will apply the principle of least privilege by defaulting to blocking all incoming and outgoing connections, and only allowing the necessary traffic for specific services. By following this principle, we minimize the risks associated with unauthorized access or exploitation of vulnerabilities, ensuring that only essential ports and services are accessible. Let’s see how to configure the firewall rules with UFW: Step 5: Block Everything and Open Only What’s Necessary Setting default rules: Block all incoming and outgoing connections by default: sudo ufw default deny incoming sudo ufw default deny outgoing Shell Authorizing necessary connections: Open HTTP and HTTPS ports in both directions, SSH, and outgoing DNS: Allow incoming connections for the web (HTTP and HTTPS): sudo ufw allow in 80/tcp sudo ufw allow in 443/tcp Shell Allow outgoing connections for the web: sudo ufw allow out 80/tcp sudo ufw allow out 443/tcp Shell Allow SSH connections (for remote management): sudo ufw allow in 22/tcp Shell Allow outgoing DNS connections (for domain name resolution): sudo ufw allow out 53/udp Shell Step 6: Activating the Rules Activating the UFW firewall: Enable UFW with: sudo ufw enable Shell Checking the configured rules: Recheck the status and rules of the firewall with: sudo ufw status verbose Shell Step 7: Add Additional Rules Based on Your Services Mail: Allow outgoing SMTP connections for sending emails: sudo ufw allow out 25/tcp Shell Additional Security (optional): Limit SSH connection attempts to enhance security: sudo ufw limit 22/tcp comment &#39;Allow 6 connections over 30 seconds&#39; Shell Restrict SSH access to certain IP addresses: sudo ufw delete allow in 22/tcp sudo ufw allow from 192.168.0.0/16 to any port 22 proto tcp Shell Advanced Configuration of Fail2ban Amphetamine-fueled illustration of Fail2ban by DALL·E Step 8: List Preconfigured Filters on Your Server Nginx Filters: List existing filters (may differ on your server): $ sudo ls -alt /etc/fail2ban/filter.d/nginx* -rw-r--r-- 1 root root 327 Nov 23 2020 /etc/fail2ban/filter.d/nginx-sslerror.conf -rw-r--r-- 1 root root 232 Nov 23 2020 /etc/fail2ban/filter.d/nginx-4xx.conf -rw-r--r-- 1 root root 564 Nov 23 2020 /etc/fail2ban/filter.d/nginx-forbidden.conf -rw-r--r-- 1 root root 681 Nov 23 2020 /etc/fail2ban/filter.d/nginx-botsearch.conf -rw-r--r-- 1 root root 485 Nov 23 2020 /etc/fail2ban/filter.d/nginx-http-auth.conf -rw-r--r-- 1 root root 1454 Nov 23 2020 /etc/fail2ban/filter.d/nginx-limit-req.conf Output Create Filters: Create filters that do not exist on your server. For this, enter the commands: For the nginx-sslerror.conf filter: This filter protects against SSL handshake failure attacks, where an attacker tries to negotiate an SSL/TLS connection with incorrect or malicious parameters. sudo bash -c &#39;cat &amp;gt; /etc/fail2ban/filter.d/nginx-sslerror.conf &amp;lt;&amp;lt;EOF [Definition] failregex = SSL_do_handshake--(--) failed .+ while SSL handshaking, client: &amp;lt;HOST&amp;gt;, server: .+ ignoreregex = datepattern = {^LN-BEG}%%ExY(?P&amp;lt;_sep&amp;gt;[-/.])%%m(?P=_sep)%%d[T ]%%H:%%M:%%S(?:[.,]%%f)?(?:--s*%%z)? ^[^--[]*--[({DATE}) {^LN-BEG} EOF&#39; Shell For the nginx-4xx.conf filter: This filter detects requests generating HTTP 4xx errors (like 404, 403, 400), often the result of attempts to access unauthorized or non-existent resources, indicating malicious probing. sudo bash -c &#39;cat &amp;gt; /etc/fail2ban/filter.d/nginx-4xx.conf &amp;lt;&amp;lt;EOF [Definition] failregex = ^&amp;lt;HOST&amp;gt;.*&quot;(GET|POST).*&quot; (404|444|403|400) .*$ ignoreregex = .*(robots.txt|favicon.ico|jpg|png) EOF&#39; Shell For the nginx-forbidden.conf filter: This filter targets attempts to access forbidden directories. It is useful for blocking directory scans attempting to discover hidden files or folders on the server. sudo bash -c &#39;cat &amp;gt; /etc/fail2ban/filter.d/nginx-forbidden.conf &amp;lt;&amp;lt;EOF [Definition] failregex = directory index of .+ is forbidden, client: &amp;lt;HOST&amp;gt;, server: .+ ignoreregex = EOF&#39; Shell For the nginx-botsearch.conf filter: This filter focuses on requests for URLs that do not exist (404 errors), often a sign of a bot or scanner trying to find vulnerabilities or hidden pages. sudo bash -c &#39;cat &amp;gt; /etc/fail2ban/filter.d/nginx-botsearch.conf &amp;lt;&amp;lt;EOF # Fail2Ban filter to match web requests for selected URLs that don&#39;t exist # [INCLUDES] # Load regexes for filtering before = botsearch-common.conf [Definition] failregex = ^&amp;lt;HOST&amp;gt; --- --S+ --[--] --&quot;(GET|POST|HEAD) --/&amp;lt;block&amp;gt; --S+--&quot; 404 .+$ ^ --[error--] --d+#--d+: --*--d+ (--S+ )?--&quot;--S+--&quot; (failed|is not found) --(2--: No such file or directory--), client--: &amp;lt;HOST&amp;gt;--, server--: --S*--, request: --&quot;(GET|POST|HEAD) --/&amp;lt;block&amp;gt; --S+--&quot;--, .*?$ ignoreregex = datepattern = {^LN-BEG}%%ExY(?P&amp;lt;_sep&amp;gt;[-/.])%%m(?P=_sep)%%d[T ]%%H:%%M:%%S(?:[.,]%%f)?(?:--s*%%z)? ^[^--[]*--[({DATE}) {^LN-BEG} # DEV Notes: # Based on apache-botsearch filter # # Author: Frantisek Sumsal EOF&#39; Shell For the nginx-http-auth.conf filter: This filter is used to detect and block repeated failed authentication attempts, indicating a possible brute-force attack on password-protected areas. sudo bash -c &#39;cat &amp;gt; /etc/fail2ban/filter.d/nginx-http-auth.conf &amp;lt;&amp;lt;EOF # fail2ban filter configuration for nginx [Definition] failregex = ^ --[error--] --d+#--d+: --*--d+ user &quot;(?:[^&quot;]+|.*?)&quot;:? (password mismatch|was not found in &quot;[^--&quot;]*&quot;), client: &amp;lt;HOST&amp;gt;, server: --S*, request: &quot;--S+ --S+ HTTP/--d+--.--d+&quot;, host: &quot;--S+&quot;(?:, referrer: &quot;--S+&quot;)?--s*$ ignoreregex = datepattern = {^LN-BEG} # DEV NOTES: # Based on samples in https://github.com/fail2ban/fail2ban/pull/43/files # Extensive search of all nginx auth failures not done yet. # # Author: Daniel Black EOF&#39; Shell For the nginx-limit-req.conf filter: This filter aims to block IP addresses that exceed the request limits defined in Nginx (limit_req), typical of a distributed denial of service (DDoS) attack or aggressive bot behavior. sudo bash -c &#39;cat &amp;gt; /etc/fail2ban/filter.d/nginx-limit-req.conf &amp;lt;&amp;lt;EOF # Fail2ban filter configuration for nginx :: limit_req # used to ban hosts, that were failed through nginx by limit request processing rate # # Author: Serg G. Brester (sebres) # # To use &#39;nginx-limit-req&#39; filter you should have `ngx_http_limit_req_module` # and define `limit_req` and `limit_req_zone` as described in nginx documentation # http://nginx.org/en/docs/http/ngx_http_limit_req_module.html # # Example: # # http { # ... # limit_req_zone $binary_remote_addr zone=lr_zone:10m rate=1r/s; # ... # # http, server, or location: # location ... { # limit_req zone=lr_zone burst=1 nodelay; # ... # } # ... # } # ... # [Definition] # Specify following expression to define exact zones, if you want to ban IPs limited # from specified zones only. # Example: # # ngx_limit_req_zones = lr_zone|lr_zone2 # ngx_limit_req_zones = [^&quot;]+ # Use following full expression if you should range limit request to specified # servers, requests, referrers etc. only : # # failregex = ^--s*--[[a-z]+--] --d+#--d+: --*--d+ limiting requests, excess: [--d--.]+ by zone &quot;(?:%(ngx_limit_req_zones)s)&quot;, client: &amp;lt;HOST&amp;gt;, server: --S*, request: &quot;--S+ --S+ HTTP/--d+--.--d+&quot;, host: &quot;--S+&quot;(, referrer: &quot;--S+&quot;)?--s*$ # Shortly, much faster and stable version of regexp: failregex = ^--s*--[[a-z]+--] --d+#--d+: --*--d+ limiting requests, excess: [--d--.]+ by zone &quot;(?:%(ngx_limit_req_zones)s)&quot;, client: &amp;lt;HOST&amp;gt;, ignoreregex = datepattern = {^LN-BEG} EOF&#39; Shell Step 9: Add Jails to Your Configuration To add these jails to the Fail2Ban configuration in the custom.conf file, follow these steps: Open the configuration file: Use the command to open the file in a text editor: sudo nano /etc/fail2ban/jail.d/custom.conf Shell Add jail configurations: Copy and paste the following configurations at the end of the file: [sshd] enabled = true [nginx-4xx] enabled = true port = http,https filter = nginx-4xx logpath = %(nginx_error_log)s [nginx-http-auth] enabled = true port = http,https filter = nginx-http-auth logpath = %(nginx_error_log)s [nginx-botsearch] enabled = true port = http,https filter = nginx-botsearch logpath = %(nginx_access_log)s [nginx-forbidden] enabled = true port = http,https filter = nginx-forbidden logpath = %(nginx_error_log)s [nginx-sslerror] enabled = true port = http,https filter = nginx-sslerror logpath = %(nginx_error_log)s [ufw] enabled = true filter = ufw logpath = /var/log/ufw.log Conf Save and close the file: After adding the configurations, save the file and close the text editor. Restart Fail2Ban: To apply the changes, restart Fail2Ban with: sudo systemctl restart fail2ban Shell This configuration will add and activate the specified jails for SSH, various Nginx configurations, and UFW. Step 10: Verification and Testing After configuring, it is essential to test and verify that everything is working correctly. Checking Fail2ban: Use this command to list the active jails and check that Fail2ban is functioning properly. sudo fail2ban-client status Shell Which returns: $ sudo fail2ban-client status Status |- Number of jail: 7 `- Jail list: nginx-4xx, nginx-botsearch, nginx-forbidden, nginx-http-auth, nginx-sslerror, sshd, ufw Output These steps form the basis for securing your Nginx server on Linux with Fail2ban and UFW. Remember that the configuration can be customized according to the specific needs of your server and network. You will find multiple filters for Fail2ban on the internet, tailored to the services running on your server. Visualization and Analysis of Bans Surprisingly wise illustration of &#39;visualization and analysis&#39; by DALL·E Data Extraction To effectively understand and analyze Fail2Ban’s security actions, it’s useful to have an overview of the banned IP addresses. The following script provides this visibility, categorized by jail: for jail in $(sudo fail2ban-client status | grep &#39;Jail list:&#39; | sed &#39;s/.*://;s/,//g&#39;); do echo &quot;Jail: $jail&quot;; sudo fail2ban-client status $jail | grep &#39;Banned IP&#39;; done Shell Here is an example of all the IPs that have been blocked on my server: Jail: nginx-4xx `- Banned IP list: Jail: nginx-botsearch `- Banned IP list: 199.229.240.163 Jail: nginx-forbidden `- Banned IP list: 104.199.31.214 146.190.242.134 152.32.211.69 159.203.88.161... Jail: nginx-http-auth `- Banned IP list: Jail: nginx-sslerror `- Banned IP list: 107.170.208.31 167.248.133.182 212.102.40.218 Jail: sshd `- Banned IP list: 101.34.23.155 101.43.39.167 103.144.3.14 103.39.209.130 103... Jail: ufw `- Banned IP list: 192.241.233.7 1.12.249.176 1.12.73.13 1.21.202.235 1.34.233... Output I truncated the IPs, there were more than 2600 for 24 hours of retention! This example shows that a very large number of IP addresses were blocked on my server, which does not contain highly sensitive data. The significant volume of banned IPs also highlights the extent and consistency of automated attacks against online servers. Even systems that seem uninteresting are frequently targeted by bots and cyber attackers seeking vulnerabilities, for malicious purposes such as spam or botnet creation. There are also many legitimate companies scanning the internet for compromised servers. Calculating Some Statistics The analysis of IP data can reveal significant information about the geographical distribution, organizational ownership, and location of these scans. The Bash script below allows for precise statistics to be calculated from the collected IP addresses. These statistics include the counting of the number of IP addresses by country, organization, and city. This process helps understand the trends of attacks and their geographic distribution. #!/bin/bash # Replace this with your personal API key to the free service https://ipinfo.io API_KEY=&quot;your_api_key_here&quot; # Name of the file containing IP addresses (one per line) FILE=&quot;ip_list.txt&quot; # Files for storing counts COUNTRY_FILE=&quot;country_count.txt&quot; ORG_FILE=&quot;org_count.txt&quot; CITY_FILE=&quot;city_count.txt&quot; # Initialize counting files if they do not exist &amp;gt; &quot;$COUNTRY_FILE&quot; &amp;gt; &quot;$ORG_FILE&quot; &amp;gt; &quot;$CITY_FILE&quot; # Function to obtain geolocation information of an IP address get_ip_info() { local ip=$1 curl -s &quot;https://ipinfo.io/$ip?token=$API_KEY&quot; } # Check if the file exists if [ ! -f &quot;$FILE&quot; ]; then echo &quot;File $FILE not found.&quot; exit 1 fi # Iterate over each line in the file while IFS= read -r ip do echo &quot;Processing ${ip}...&quot; ip_info=$(get_ip_info &quot;$ip&quot;) country=$(echo &quot;$ip_info&quot; | jq -r &#39;.country&#39;) org=$(echo &quot;$ip_info&quot; | jq -r &#39;.org&#39;) city=$(echo &quot;$ip_info&quot; | jq -r &#39;.city&#39;) # Update counting files echo &quot;$country&quot; &amp;gt;&amp;gt; &quot;$COUNTRY_FILE&quot; echo &quot;$org&quot; &amp;gt;&amp;gt; &quot;$ORG_FILE&quot; echo &quot;$city&quot; &amp;gt;&amp;gt; &quot;$CITY_FILE&quot; done &amp;lt; &quot;$FILE&quot; # Function to count occurrences count_occurrences() { sort -bfg | uniq -c } # Function to sort occurrences sort_occurrences() { sort -rn -k1,1 } # Display statistics echo &quot;Statistics by country code:&quot; cat &quot;$COUNTRY_FILE&quot; | count_occurrences | sort_occurrences echo &quot;Statistics by organization:&quot; cat &quot;$ORG_FILE&quot; | count_occurrences | sort_occurrences echo &quot;Statistics by city:&quot; cat &quot;$CITY_FILE&quot; | count_occurrences | sort_occurrences Bash Scans by Country Results 1055 US 361 CN 252 GB 135 NL 98 DE 71 BR 69 TW 57 KR 53 IN 45 BE 44 RU 43 FR 40 JP 25 VN 24 HK 21 SG 20 IT 19 CA 16 BG 13 TR 13 TH 13 SE 13 ID 13 AU 11 AR 9 PL 7 UA 7 PH 7 LT 7 IR 5 IL 4 MX 4 CL 3 VE 3 RO 3 NG 3 IQ 3 CZ 2 GR 2 ES 2 CO 2 CH 2 BD 2 AT 2 AE 1 ZA 1 SK 1 SA 1 PK 1 PE 1 PA 1 NO 1 MY 1 MG 1 MA 1 LU 1 KZ 1 GU 1 DK 1 CV 1 CR 1 BZ 1 BY 1 BO 1 AZ Output Interpretation of Blocked Scans by Country Top 20 Countries with the highest number of scans 1. High Scanning Activity in the United States (US) Number of occurrences: 1055 Implication: This high activity suggests a significant presence of scans, possibly automated, originating from the United States. This could indicate either a large number of active servers and computer systems or the presence of compromised networks. 2. Notable Presence of China (CN) and the United Kingdom (GB) China: 361 occurrences United Kingdom: 252 occurrences Implication: High frequency of scans from these regions, possibly indicating malicious activities or strong cyber defense. 3. Scans Originating from Europe Countries involved: Netherlands, Germany, Belgium, France, Russia Implication: Active scanning activities or networks often targeted by attacks, necessitating proactive defense. 4. Diversity of Asian Sources Countries involved: Taiwan, South Korea, India, Japan, Vietnam Implication: Regions active in scanning or targeted by attacks, reflecting the trends in cyber security in Asia. 5. Activity in Latin America and Africa Latin America: Brazil, Argentina, Mexico, Colombia Africa: Less prevalent but notable presence Implication: Trend towards the automation of attacks or the presence of compromised systems in these regions. 6. Countries with Few Occurrences Examples: African countries, European and Asian countries with a single or very few counts Implication: Sporadic scanning attempts or isolated actors. These statistics provide an insight into trends in cyber security and malicious activities on the Internet. They reveal the effectiveness of security measures like Fail2Ban and UFW in protecting a server from unauthorized or malicious access attempts. Scans by Organization Results 590 AS396982 Google LLC 384 AS14061 DigitalOcean, LLC 165 AS6939 Hurricane Electric LLC 108 AS4134 CHINANET-BACKBONE 98 AS398324 Censys, Inc. 71 AS37963 Hangzhou Alibaba Advertising Co.,Ltd. 59 AS45090 Shenzhen Tencent Computer Systems Company Limited 54 AS4837 CHINA UNICOM China169 Backbone 47 AS398705 Censys, Inc. 46 AS135377 UCLOUD INFORMATION TECHNOLOGY (HK) LIMITED 36 AS4766 Korea Telecom 35 AS132203 Tencent Building, Kejizhongyi Avenue 34 AS9829 National Internet Backbone 34 AS63949 Akamai Connected Cloud 27 AS16276 OVH SAS 24 AS51396 Pfcloud UG 24 AS211298 INTERNET MEASUREMENT 24 AS202425 IP Volume inc 24 AS198465 BtHoster LTD 21 AS394711 Limenet 17 AS20052 Arbor Networks, Inc. 17 AS10439 CariNet, Inc. 16 AS4713 NTT Communications Corporation 15 AS9009 M247 Europe SRL 15 AS211680 NSEC - Sistemas Informaticos, S.A. 13 AS58466 CHINANET Guangdong province network 12 AS51167 Contabo GmbH 12 AS3462 Data Communication Business Group 12 AS12876 SCALEWAY S.A.S. 10 AS9808 China Mobile Communications Group Co., Ltd. 10 AS36352 ColoCrossing 10 AS13213 UK-2 Limited 9 AS45102 Alibaba (US) Technology Co., Ltd. 8 AS7552 Viettel Group 8 AS50360 Tamatiya EOOD 8 AS36007 Kamatera, Inc. 8 AS34534 Harmony Hosting SARL 7 AS57523 Chang Way Technologies Co. Limited 7 AS21859 Zenlayer Inc 7 AS209605 UAB Host Baltic 7 AS198953 Proton66 OOO 6 AS174 Cogent Communications 6 AS140292 CHINATELECOM Jiangsu province Suzhou 5G network 5 AS49870 Alsycon B.V. 5 AS49581 Ferdinand Zink trading as Tube-Hosting 5 AS4808 China Unicom Beijing Province Network 5 AS3269 Telecom Italia S.p.A. 5 AS210644 AEZA INTERNATIONAL LTD 5 AS204428 SS-Net 5 AS149621 SB Secure Data centers India Private Limited 5 AS10617 SION S.A 4 AS9121 Turk Telekomunikasyon Anonim Sirketi 4 AS8151 UNINET 4 AS7018 AT&amp;amp;T Services, Inc. 4 AS701 Verizon Business 4 AS53667 FranTech Solutions 4 AS23969 TOT Public Company Limited 4 AS208843 Alpha Strike Labs GmbH 4 AS207812 DM AUTO EOOD 4 AS201814 MEVSPACE sp. z o.o. 4 AS20001 Charter Communications Inc 4 AS19318 Interserver, Inc 4 AS140803 HQDATA 3 AS9318 SK Broadband Co Ltd 3 AS8075 Microsoft Corporation 3 AS7303 Telecom Argentina S.A. 3 AS714 Apple Inc. 3 AS58461 CT-HangZhou-IDC 3 AS5607 Sky UK Limited 3 AS4812 China Telecom (Group) 3 AS45899 VNPT Corp 3 AS398101 GoDaddy.com, LLC 3 AS38365 Beijing Baidu Netcom Science and Technology Co., Ltd. 3 AS33491 Comcast Cable Communications, LLC 3 AS3215 Orange S.A. 3 AS2527 Sony Network Communications Inc. 3 AS22552 eSited Solutions 3 AS211607 Securitytrails, LLC 3 AS12389 PJSC Rostelecom 2 AS9506 Singtel Fibre Broadband 2 AS9319 HCN CHUNGBUK CABLE TV SYSTEMS 2 AS8551 Bezeq International Ltd. 2 AS8048 CANTV Servicios, Venezuela 2 AS7545 TPG Telecom Limited 2 AS7418 TELEFÓNICA CHILE S.A. 2 AS62904 Eonix Corporation 2 AS62160 WEB3 Leaders INC 2 AS58212 dataforest GmbH 2 AS4811 China Telecom (Group) 2 AS47154 HUSAM A. H. HIJAZI 2 AS44592 SkyLink Data Center BV 2 AS42237 w1n ltd 2 AS41436 Kamatera Inc 2 AS398791 GoDaddy.com, LLC 2 AS39501 NGSAS NedaGostarSaba 2 AS38283 CHINANET SiChuan Telecom Internet Data Center 2 AS35916 MULTACOM CORPORATION 2 AS33363 Charter Communications, Inc 2 AS33182 HostDime.com, Inc. 2 AS31898 Oracle Corporation 2 AS26599 TELEFÔNICA BRASIL S.A 2 AS2635 Automattic, Inc 2 AS25369 Hydra Communications Ltd 2 AS2514 NTT PC Communications, Inc. 2 AS22773 Cox Communications Inc. 2 AS22501 Cooperativa Telefonica Carlos Tejedor Ltda. 2 AS212815 Dyjix SAS 2 AS209828 Genc BT Bilisim Teknolojileri Limited Sirketi 2 AS209559 XHOST INTERNET SOLUTIONS LP 2 AS208091 XHOST INTERNET SOLUTIONS LP 2 AS20214 Comcast Cable Communications, LLC 2 AS18779 EGIHosting 2 AS17676 SoftBank Corp. 2 AS17511 OPTAGE Inc. 2 AS17421 Mobile Business Group 2 AS16509 Amazon.com, Inc. 2 AS16232 Telecom Italia S.p.A. 2 AS15895 &quot;Kyivstar&quot; PJSC 2 AS150706 Hong Kong Zhengxing Technology Co., Ltd. 2 AS14618 Amazon.com, Inc. 2 AS142002 Scloud Pte Ltd 2 AS135944 VinhNam Commercial informatics service corporation 2 AS135905 VIETNAM POSTS AND TELECOMMUNICATIONS GROUP 2 AS134238 CHINANET Jiangx province IDC network 2 AS12683 PJSC Rostelecom 2 AS1267 WIND TRE S.P.A. 2 AS1257 Tele2 Sverige AB 2 AS12400 Partner Communications Ltd. 2 AS11492 CABLE ONE, INC. 1 AS9976 Namincheon Brodcasting Co., Ltd. 1 AS9845 LG HelloVision Corp. 1 AS9824 JCOM Co., Ltd. 1 AS9697 LG HelloVision Corp. 1 AS9689 SK Broadband Co Ltd 1 AS9595 NTT-ME Corporation 1 AS9316 DACOM-PUBNETPLUS 1 AS9304 HGC Global Communications Limited 1 AS9299 Philippine Long Distance Telephone Company 1 AS9198 JSC Kazakhtelecom 1 AS9050 ORANGE ROMANIA COMMUNICATION S.A 1 AS8612 Tiscali Italia S.P.A. 1 AS8595 OOO WestCall Ltd. 1 AS852 TELUS Communications Inc. 1 AS8473 Bahnhof AB 1 AS8447 A1 Telekom Austria AG 1 AS8374 Polkomtel Sp. z o.o. 1 AS8369 Intersvyaz-2 JSC 1 AS812 Rogers Communications Canada Inc. 1 AS786 Jisc Services Limited 1 AS7713 PT Telekomunikasi Indonesia 1 AS7470 TRUE INTERNET Co.,Ltd. 1 AS7377 University of California, San Diego 1 AS6871 Plusnet 1 AS680 Verein zur Foerderung eines Deutschen Forschungsnetzes e.V. 1 AS6799 Ote SA (Hellenic Telecommunications Organisation) 1 AS64227 CONSOLIDATED TELEPHONE COMPANY 1 AS63961 Bangladesh Research and Education Network (BdREN) 1 AS6327 Shaw Communications Inc. 1 AS61857 SPEEDFAST TELECOM 1 AS6167 Verizon Business 1 AS6147 Telefonica del Peru S.A.A. 1 AS6130 American Internet Services, LLC. 1 AS6128 Cablevision Systems Corp. 1 AS60068 Datacamp Limited 1 AS59477 LIFEPC, s.r.o. 1 AS58541 Qingdao,266000 1 AS58519 Cloud Computing Corporation 1 AS58321 Oxylion S. A. 1 AS58224 Iran Telecommunication Company PJS 1 AS57678 Cat Technologies Co. Limited 1 AS57588 Hayat for Internet &amp;amp; communication LLC 1 AS57044 JSC &quot;ER-Telecom Holding&quot; 1 AS5650 Frontier Communications of America, Inc. 1 AS56478 Hyperoptic Ltd 1 AS56048 China Mobile Communicaitons Corporation 1 AS56047 China Mobile communications corporation 1 AS56046 China Mobile communications corporation 1 AS56042 China Mobile communications corporation 1 AS55720 Gigabit Hosting Sdn Bhd 1 AS55492 Dhaka Fiber Net Limited 1 AS5384 EMIRATES TELECOMMUNICATIONS GROUP COMPANY (ETISALAT GROUP) PJSC 1 AS53153 CINTE Telecom Comercio e Servicos Ltda. 1 AS53006 ALGAR TELECOM S/A 1 AS52936 ISOTELCO LTDA 1 AS52606 BRASILNETS COM. ATAC. DE EQ. INFORMATICA LTDA ME 1 AS52207 JSC &quot;ER-Telecom Holding&quot; 1 AS51852 Private Layer INC 1 AS51570 JSC &quot;ER-Telecom Holding&quot; 1 AS51115 HLL LLC 1 AS5089 Virgin Media Limited 1 AS49893 Bitrace telecom Ltd. 1 AS49202 Kisara LLC 1 AS49100 Pishgaman Toseeh Ertebatat Company (Private Joint Stock) 1 AS48854 team.blue Denmark A/S 1 AS48737 DoraTelekom 1 AS48715 Sefroyek Pardaz Engineering PJSC 1 AS4847 China Networks Inter-Exchange 1 AS48347 JSC Mediasoft ekspert 1 AS4816 China Telecom (Group) 1 AS48090 PPTECHNOLOGY LIMITED 1 AS47890 UNMANAGED LTD 1 AS4788 TM TECHNOLOGY SERVICES SDN. BHD. 1 AS47764 LLC VK 1 AS47583 Hostinger International Limited 1 AS46606 Unified Layer 1 AS4657 StarHub Ltd 1 AS45629 JasTel Network International Gateway 1 AS45458 SBN-ISP/AWN-ISP and SBN-NIX/AWN-NIX 1 AS44724 Octopusnet LTD 1 AS44634 LLC SibSvayzStroy 1 AS43260 DGN TEKNOLOJI A.S. 1 AS42668 Nevalink, LLC 1 AS400328 Intelligence Hosting LLC 1 AS398989 DeepIntent, Inc. 1 AS398722 Censys, Inc. 1 AS3920 ESTOXY OU 1 AS38478 SunnyVision Limited 1 AS38372 RJNET 1 AS38264 National WiMAX/IMS environment 1 AS38096 SK Broadband Co Ltd 1 AS3786 LG DACOM Corporation 1 AS37608 iRENALA 1 AS37517 CV Multimedia SA 1 AS36925 MEDITELECOM 1 AS36493 FIBERNETICS CORPORATION 1 AS36459 GitHub, Inc. 1 AS3605 Guam Cablevision, LLC. 1 AS35562 Kedr Ltd. 1 AS35125 PJSC Rostelecom 1 AS34984 Superonline Iletisim Hizmetleri A.S. 1 AS34622 Bredband i Kristianstad AB 1 AS33915 Vodafone Libertel B.V. 1 AS33668 Comcast Cable Communications, LLC 1 AS33659 Comcast Cable Communications, LLC 1 AS33588 Charter Communications 1 AS3329 VODAFONE-PANAFON HELLENIC TELECOMMUNICATIONS COMPANY SA 1 AS3303 Swisscom (Schweiz) AG 1 AS3301 Telia Company AB 1 AS328608 Africa on Cloud 1 AS3209 Vodafone GmbH 1 AS31213 PJSC MegaFon 1 AS31133 PJSC MegaFon 1 AS31034 Aruba S.p.A. 1 AS30722 Vodafone Italia S.p.A. 1 AS30036 Mediacom Communications Corp 1 AS29484 Ruhr-Universitaet Bochum 1 AS28573 Claro NXT Telecomunicacoes Ltda 1 AS2856 British Telecommunications PLC 1 AS28294 B S Costa Telecom 1 AS28283 Adylnet Telecom 1 AS28209 Under Servicos de Internet Ltda 1 AS27951 Media Commerce Partners S.A 1 AS27882 Telefónica Celular de Bolivia S.A. 1 AS270719 START NET TELECOM LTDA 1 AS269832 MDS TELECOM C.A. 1 AS269608 VELOSO NET SERV DE COMUNICACAO MULTIDIA EIRELI 1 AS267784 Flyservers S.A. 1 AS266608 Ola Fibra Telecomunicacoes LTDA 1 AS266181 GOLDEN LINK 1 AS263056 INDNET TELECOMUNICACOES LTDA 1 AS26277 ServerPoint.com 1 AS262663 METROFLEX TELECOMUNICACOES LTDA 1 AS262378 Compuservice Empreendimentos Ltda 1 AS262318 Horizons Telecomunicações e Tecnologia S.A. 1 AS2519 ARTERIA Networks Corporation 1 AS2518 BIGLOBE Inc. 1 AS2516 KDDI CORPORATION 1 AS25106 Mobile TeleSystems JLLC 1 AS25019 Saudi Telecom Company JSC 1 AS24961 myLoc managed IT AG 1 AS24700 WEB3 Leaders INC 1 AS24560 Bharti Airtel Ltd., Telemedia Services 1 AS24547 Hebei Mobile Communication Company Limited 1 AS24444 Shandong Mobile Communication Company Limited 1 AS24164 UNION BROADBAND NETWORK 1 AS23724 IDC, China Telecommunications Corporation 1 AS22408 West Ky Networks 1 AS216240 MortalSoft Ltd. 1 AS216167 Skoali SAS 1 AS215862 Taliene De Araujo Souza 1 AS213402 Rahat Telecom LLC 1 AS213149 Telelink Telecommunications Co for Internet services and Information Technology Ltd. 1 AS212913 FOP Hornostay Mykhaylo Ivanovych 1 AS211715 Partlix, Ltd. 1 AS2116 GLOBALCONNECT AS 1 AS211557 TAYNET TEKNOLOJI TICARET LIMITED SIRKETI 1 AS211235 AL-SAHIN AL-SHABALY Co. for Internet Services Ltd 1 AS211056 Amir Hosein Maaref 1 AS210218 Open Fiber S.P.A. 1 AS209711 MUV Bilisim ve Telekomunikasyon Hizmetleri Ltd. Sti. 1 AS209 CenturyLink Communications, LLC 1 AS208258 Access2.IT Group B.V. 1 AS207147 NETCOM GROUP SAS 1 AS206264 Amarutu Technology Ltd 1 AS206216 Advin Services LLC 1 AS206119 Veganet Teknolojileri ve Hizmetleri LTD STI 1 AS20473 The Constant Company, LLC 1 AS202520 SkyPass Solutions Sp. z.o.o. 1 AS202468 Noyan Abr Arvan Co. ( Private Joint Stock) 1 AS201776 Miranda-Media Ltd 1 AS20115 Charter Communications 1 AS19871 Network Solutions, LLC 1 AS197183 Occentus Network SL 1 AS197078 Yarnet Ltd 1 AS19037 AMX Argentina S.A. 1 AS18822 Manquehuenet 1 AS18809 Cable Onda 1 AS18403 FPT Telecom Company 1 AS18144 Energia Communications,Inc. 1 AS18081 Kintetsu Cable Network Co., Ltd. 1 AS18049 Taiwan Infrastructure Network Technologie 1 AS17858 LG POWERCOMM 1 AS17809 VEE TIME CORP. 1 AS17747 SITI NETWORKS LIMITED 1 AS17698 COMMUNITY NETWORK CENTER INCORPORATED. 1 AS17665 ONEOTT INTERTAINMENT LIMITED 1 AS17639 Converge ICT Solutions Inc. 1 AS17488 Hathway IP Over Cable Internet 1 AS17451 BIZNET NETWORKS 1 AS16863 Home Telephone Company, Inc. 1 AS16629 CTC. CORP S.A. (TELEFONICA EMPRESAS) 1 AS16116 Pelephone Communications Ltd. 1 AS15704 XTRA TELECOM S.A. 1 AS15493 &quot;Russian company&quot; LLC 1 AS15169 Google LLC 1 AS151487 Awesomecloud Limited 1 AS149570 Speech Tell Communication Private Limited 1 AS147176 NZ Network Enterprise Co., Ltd. 1 AS142111 Zhejiang Aiyun Network Technology Co Ltd 1 AS141679 China Telecom Beijing Tianjin Hebei Big Data Industry Park Branch 1 AS141480 Haash Media 1 AS141152 BATAAN SPACE CABLE NETWORK INC 1 AS140726 UNICOM AnHui province network 1 AS1403 EBOX 1 AS139752 Multinetwork Cable Television, Inc 1 AS139281 Equinix Korea LLC 1 AS138968 rainbow network limited 1 AS138152 YISU CLOUD LTD 1 AS138025 RBC Cable Master System 1 AS137941 Mabuhay Cable TV Inc 1 AS137718 Beijing Volcano Engine Technology Co., Ltd. 1 AS137443 Anchnet Asia Limited 1 AS136052 PT Cloud Hosting Indonesia 1 AS135161 GMO-Z com NetDesign Holdings Co., Ltd. 1 AS13490 Buckeye Cablevision, Inc. 1 AS13489 EPM Telecomunicaciones S.A. E.S.P. 1 AS134810 China Mobile Group JiLin communications corporation 1 AS134765 CHINANET Yunnan province IDC1 network 1 AS134762 CHINANET Liaoning province Dalian MAN network 1 AS134756 CHINANET Nanjing Jishan IDC network 1 AS134420 Chongqing Telecom 1 AS134143 Professional Data Kinetics Pty Ltd 1 AS133676 Precious netcom pvt ltd 1 AS133159 Mammoth Media Pty Ltd 1 AS132335 LeapSwitch Networks Pvt Ltd 1 AS13188 CONTENT DELIVERY NETWORK LTD 1 AS131414 Long Van Soft Solution JSC 1 AS131353 NhanHoa Software company 1 AS131090 CAT TELECOM Public Company Ltd,CAT 1 AS12897 ENTEGA Medianet GmbH 1 AS12874 Fastweb SpA 1 AS12849 Hot-Net internet services Ltd. 1 AS12735 TurkNet Iletisim Hizmetleri A.S. 1 AS12730 PJSC Rostelecom 1 AS12494 OOO &quot;Post ltd&quot; 1 AS12322 Free SAS 1 AS1221 Telstra Corporation Ltd 1 AS11830 Instituto Costarricense de Electricidad y Telecom. 1 AS11351 Charter Communications Inc 1 AS10796 Charter Communications Inc 1 AS10269 Belize Telemedia Limited Output Interpretation of Blocked Scans by Organization Top 20 Organizations with the highest number of scans 1. Google LLC (AS396982) Number of occurrences: 590 Implication: Significant activity from IP addresses associated with Google, indicating potential use of Google services for network scanning or misuse of their PaaS GCP platform. 2. DigitalOcean, LLC (AS14061) Number of occurrences: 384 Implication: High scanning activity from this cloud organization, suggesting either legitimate services or potentially compromised cloud instances. 3. Hurricane Electric LLC (AS6939) Number of occurrences: 165 Implication: Notable presence of scans from this Internet service provider, possibly indicating abuse of its services for scanning activities. 4. Chinese Organizations Number of occurrences: CHINANET-BACKBONE (AS4134): 108 Hangzhou Alibaba (AS37963): 71 Tencent (AS45090): 59 CHINA UNICOM (AS4837): 54 Other Chinese organizations with significant occurrences Implication: Scanning activities emanating from major Chinese corporations, reflecting either legitimate operations or abuse of their networks. 5. Censys, Inc. (AS398324, AS398705) Number of occurrences: Total of 145 Implication: Censys, a data security-focused company, appears engaged in scanning activity, likely in a security research context. 6. OVH SAS (AS16276) and Other Hosting Providers Implication: Scans from OVH and other hosting providers may indicate either legitimate services or abuse of hosting resources. 7. Diversity of Global Sources Implication: The presence of organizations from many countries (e.g., KR, IN, RU, JP, VN) shows a diversity in the geographical origin of scans, which may reflect a wide range of intentions and practices. 8. Scanning Activities by ISPs and Telecommunications Companies Examples: Korea Telecom (AS4766), NTT Communications (AS4713) Implication: These ISPs and telecommunications companies may be involved in scans for maintenance, security, or other legitimate activities. These statistics underscore the importance of ongoing monitoring and protection against potentially malicious scanning activities. The diversity of sources, ranging from cloud service providers to major telecommunications companies, illustrates the complexity of the online security ecosystem. Scans by City Results 314 San Francisco 218 London 210 North Charleston 107 Chicago 103 Amsterdam 77 Pleasanton 75 Frankfurt am Main 62 Kingsburg 57 Shenzhen 55 São Paulo 50 Shanghai 48 Taichung 45 Brussels 42 Beijing 38 Council Bluffs 33 Nanjing 32 Hangzhou 25 Hopel 24 Thetford 22 Paris 22 Hong Kong 21 Singapore 21 Santa Clara 20 San Diego 18 Morris Plains 18 Los Angeles 17 Moscow 17 Ann Arbor 16 Tokyo 16 Sofia 16 North Bergen 15 Seoul 14 Shenyang 14 Palo Alto 12 Chengdu 11 Wuhan 11 Taiyuan 10 Taipei 10 Sydney 10 Stockholm 10 San Mateo 10 Ho Chi Minh City 10 Clifton 10 Bangkok 9 Qingdao 9 Milan 9 Kunming 9 Jakarta 9 Fremont 9 Chongqing 8 Saint Petersburg 8 Roubaix 8 Changchun 7 Phoenix 7 New York City 7 Lille 7 Istanbul 7 Beauharnois 6 Zhengzhou 6 Zhangjiakou 6 Warsaw 6 Toronto 6 Dallas 6 Buffalo 6 Ashburn 6 Aachen 5 Vilnius 5 Mumbai 5 Mangalagiri 5 Harbin 5 Hanoi 5 Haarlem 5 Changsha 5 Bengaluru 4 Thiruvananthapuram 4 Tehran 4 Santiago 4 Richardson 4 Las Vegas 4 Incheon 4 Düsseldorf 4 Berlin 4 Atlanta 3 Thoothukudi 3 Shanxi 3 Seattle 3 San Jose 3 Reston 3 Prague 3 Nürnberg 3 Montréal 3 Lagos Island 3 Kollam 3 Hefei 3 Gwangju 3 Guiyang 3 Guangzhou 3 General Alvear 3 Chennai 3 Caracas 3 Busan 3 Banqiao 3 Baghdad 2 Zürich 2 Xining 2 Vinh 2 Vienna 2 Utsunomiya 2 Timişoara 2 Tianjin 2 The Dalles 2 Suwon 2 Shijiazhuang 2 Seongnam-si 2 Rio de Janeiro 2 Pyatigorsk 2 Portsmouth 2 Philadelphia 2 Palermo 2 Olathe 2 Nanchang 2 Mito 2 Miryang 2 Mar del Plata 2 Kolkata 2 Kaunas 2 Jalandhar 2 Hunan 2 Hoàn Kiếm 2 Hazāribāgh 2 Gruzino 2 Göteborg 2 Fuzhou 2 Fengshan 2 Dubai 2 Doddaballapura 2 Dhaka 2 Contai 2 Coimbatore 2 Chinch&#39;ŏn 2 Cheongju-si 2 Central 2 Buenos Aires 2 Ankara 1 Đồng Hới 1 Ōtsu 1 Şişli 1 Ōbu 1 Zolotonosha 1 Zhongxing New Village 1 Zhenjiang 1 Zhanjiang 1 Zapolyarnyy 1 Yongsan-dong 1 Yogyakarta 1 Yingkou 1 Yilan 1 Yigo Village 1 Yesan 1 Yelets 1 Yekaterinburg 1 Yecheon 1 Yaroslavl 1 Yangsan 1 Yangquan 1 Xi’an 1 Wrocław 1 Winter Park 1 Whittier 1 Waterloo 1 Waterford 1 Wakefield 1 Waegwan 1 Volgograd 1 Vladivostok 1 Vigia 1 Veranópolis 1 Vancouver 1 Valencia 1 Una 1 Ulsan 1 Uberlândia 1 T’aebaek 1 Tyumen 1 Tula 1 Tuguegarao 1 Trelew 1 Tilburg 1 Thessaloníki 1 The Acreage 1 Thái Nguyên 1 Tempe 1 Tefé 1 Tân An 1 Takasaki 1 Tainan 1 Sylvania 1 Surat Thani 1 Stratford-upon-Avon 1 Strasbourg 1 Southend-on-Sea 1 South Riding 1 Smolensk 1 Simferopol 1 Shulin 1 Shaoxing 1 Sevastopol 1 Serpong 1 Sergiyev Posad 1 Sejong 1 Secaucus 1 Satellite Beach 1 Sardinal 1 Sapporo 1 Santa Monica 1 Santa Fe 1 San Pedro 1 Samut Songkhram 1 Sainte-Geneviève-des-Bois 1 Saint-Quentin-en-Yvelines 1 Ryazan’ 1 Rondonópolis 1 Rome 1 Riverside 1 Rishon LeTsiyyon 1 Raebareli 1 Quận Bốn 1 Puerto Madryn 1 Provo 1 Praia 1 Poznań 1 Pohang 1 Petaling Jaya 1 Pathanāmthitta 1 Passos 1 Pasig City 1 Paripark 1 Pardīs 1 Panvel 1 Panama City 1 Palm Beach Gardens 1 Padova 1 Oslo 1 Osaka 1 Okayama 1 Novosibirsk 1 Nova Iguaçu 1 Ningbo 1 Niagara Falls 1 New Delhi 1 Neietsu 1 Neapoli 1 Naples 1 Nanning 1 Nagoya 1 Murray 1 Munich 1 Motoyoyogichō 1 Morro 1 Monterrey 1 Mokotów 1 Minsk 1 Minatitlán 1 Michelstadt 1 Miaoli 1 Melbourne 1 Massy 1 Masan 1 Masaguisi 1 Marinilla 1 Manila 1 Magnitogorsk 1 Mafra 1 Mâcon 1 Machida 1 Macapá 1 Luxembourg 1 Lutsk 1 Ludhiāna 1 Liverpool 1 Linfen 1 Largo 1 La Paz 1 Kōriyama 1 Kyiv 1 Kurashiki 1 Köln 1 Koesan 1 Kobe 1 Kislovodsk 1 Kimhae 1 Kharkiv 1 Kalispell 1 Jinrongjie 1 Jinan 1 Jiaxing 1 Jerusalem 1 Jeju City 1 Isparta 1 Isfahan 1 Indianapolis 1 Imperatriz 1 Iksan 1 Ikoma 1 Ichikawa-minami 1 Hwawŏn 1 Huzhou 1 Huizhou 1 Hrochoť 1 Honolulu 1 Hohhot 1 Hitachi-Naka 1 Hisar 1 Hicksville 1 Heyuan 1 Hatsudai 1 Harston 1 Hammarslund 1 Hamburg 1 Halesowen 1 Haikou 1 Haifa 1 Hadano 1 Gumi 1 Guanajuato 1 Guadalajara 1 Groningen 1 Greenford 1 Goose Creek 1 Goochland 1 Gold Coast 1 Goiânia 1 Gifu-shi 1 Giddalūr 1 Ghāziābād 1 Genoa 1 Gangseo-gu 1 Fukuyama 1 Fontana 1 Florence 1 Feltham 1 Faisalabad 1 Essen 1 Englewood Cliffs 1 El Pedregal 1 Ekibastuz 1 Eilat 1 Ecoporanga 1 Douliu 1 Doral 1 Derval 1 Dammam 1 Daejeon 1 Da Nang 1 Curitiba 1 Cormano 1 Çorlu 1 Córdoba 1 Corcuera 1 Columbus 1 Ciudad López Mateos 1 Cirebon 1 Cincinnati 1 Cibinong 1 Chư Ty 1 Chiang Mai 1 Chernihiv 1 Changde 1 Chandīgarh 1 Chandler 1 Ceres 1 Castiglione delle Stiviere 1 Casablanca 1 Cambridge 1 Cabanatuan City 1 Bursa 1 Burnaby 1 Bucharest 1 Bryansk 1 Brampton 1 Boydton 1 Bologna 1 Bogotá 1 Bochum 1 Boardman 1 Bhopāl 1 Belford Roxo 1 Balanga 1 Baku 1 Atlantic City 1 Ashdod 1 Antananarivo 1 Anseong 1 Andong 1 Anderson 1 Amritsar 1 Akashi 1 Aitkin 1 Ahvaz 1 Ahmedabad 1 Adelaide 1 Aabenraa Output Interpretation of Blocked Scans Statistics by City Top 20 Cities with the highest number of scans 1. San Francisco (314 Occurrences) Implication: Significant activity from this technologically advanced city, likely reflecting the use of services based in Silicon Valley. 2. London (218 Occurrences) Implication: Strong activity from this global metropolis, suggesting scanning operations from companies based in London or using services hosted there. 3. North Charleston (210 Occurrences) Implication: A surprising number of scans from this city, possibly indicating the presence of cloud service infrastructures or other technology companies. 4. Chinese Cities (Shenzhen, Shanghai, Beijing, etc.) Total Occurrences: Over 150 Implication: Significant scans emanating from several major Chinese cities, illustrating the breadth of technological and digital presence in China. 5. Amsterdam (103 Occurrences) Implication: With its cloud service infrastructure and data centers, Amsterdam is a notable source of scans. 6. Frankfurt am Main (75 Occurrences) Implication: High activity in this German city, known for its data centers and key role in European networks. 7. Diversity of Global Sources Implication: The variety of cities (such as Seoul, Tokyo, Paris, Moscow) indicates a global distribution of scan sources, reflecting the global nature of online activity and potential threats. 8. Specific Activities of Certain Cities Examples: São Paulo, Singapore, Santa Clara Implication: These cities, known for their concentration of technology companies, show scanning activity that may be attributed to legitimate research or network abuses. These statistics highlight the geographical diversity of scanning activities and underscore the importance of cybersecurity measures. The presence of scans from major technological and financial cities indicates that these regions are hotspots for online activities, necessitating increased vigilance for network protection. Conclusion Securing your Nginx server with Fail2ban and UFW is a crucial step in protecting your data and infrastructure against increasingly sophisticated cyber attacks. In this article, we have seen how Fail2ban and UFW can be used together to form a robust barrier against attacks, by combining the detection and banning of malicious IP addresses with rigorous management of network traffic. Implementing these tools is just the beginning of an ongoing security process. It is essential to keep these systems updated, regularly adjust configurations in response to new threats, and continuously monitor logs for intrusion attempts. Data analysis clearly demonstrates the importance of these tools in detecting and blocking a multitude of unauthorized access attempts from around the world. Finally, keep in mind that security is an evolving practice. Threats are constantly changing, and our defense methods must adapt accordingly. Do not hesitate to explore new tools, share your knowledge with the community, and stay informed of the latest trends in cybersecurity. Stay safe, stay secure!"
    } ,
  
    {
      "title"    : "Everything You Need to Know About Hexagonal Architecture: Kernel, Ports, Adapters",
      "url"      : "/en/hexagonal-architecture/",
      "img"      : "PHBpY3R1cmU+CiAgPHNvdXJjZSBzcmNzZXQ9Ii9hc3NldHMvaW1nL2hleGFnb25hbC1hcmNoaXRlY3R1cmUtMzAwLTU3ZmU2MmYwMS5hdmlmIDEuMHgsIC9hc3NldHMvaW1nL2hleGFnb25hbC1hcmNoaXRlY3R1cmUtNjAwLTU3ZmU2MmYwMS5hdmlmIDIuMHgiIHR5cGU9ImltYWdlL2F2aWYiPgogIDxzb3VyY2Ugc3Jjc2V0PSIvYXNzZXRzL2ltZy9oZXhhZ29uYWwtYXJjaGl0ZWN0dXJlLTMwMC05M2M0YWQ1MTQuanBnIDEuMHgsIC9hc3NldHMvaW1nL2hleGFnb25hbC1hcmNoaXRlY3R1cmUtNjAwLTkzYzRhZDUxNC5qcGcgMi4weCIgdHlwZT0iaW1hZ2UvanBlZyI+CiAgPGltZyBzcmM9Ii9hc3NldHMvaW1nL2hleGFnb25hbC1hcmNoaXRlY3R1cmUtMzAwLTkzYzRhZDUxNC5qcGciIGFsdD0iRXZlcnl0aGluZyBZb3UgTmVlZCB0byBLbm93IEFib3V0IEhleGFnb25hbCBBcmNoaXRlY3R1cmU6IEtlcm5lbCwgUG9ydHMsIEFkYXB0ZXJzIj4KPC9waWN0dXJlPgo=",
      "isencoded" : "y",
      "tags"     : "Hexagonal-Architecture, Clean-Architecture, DDD, SOLID, Craftsmanship",
      "content"  : "In the vast world of software development, architecture serves as the cornerstone upon which the construction of any computer system relies. The choice of architecture is a crucial decision that influences how an application is designed, evolves, and is maintained. A critical aspect to consider is that once a technical choice is made for an application, it becomes challenging to change course as the project progresses. The risks of regressions increase, and the point of no return is quickly reached. This is where hexagonal architecture comes into play to address these challenges. 1. Definition and History 2. Principles of Hexagonal Architecture 3. Comparison with Other Architectural Approaches Comparison with Traditional MVC Architecture Connection with Domain-Driven Design (DDD) Perspective of Hexagonal Architecture Compared to Clean Architecture 4. Advantages of Hexagonal Architecture Reduction of Functional Regression Risks during Technical Changes Ease of Adding New Features and Modifying Existing Ones Simplicity of Business Logic Testing Promotion of BDD and DDD Approaches 5. Limitations and Drawbacks Increased Number of Packages and Project Structure Complexity Potential Inefficiency in Certain Contexts 6. Implementation of Hexagonal Architecture Steps to Implement Hexagonal Architecture Example of Implementation in a Concrete Context (Banking Application) 7. Best Practices and Tips Pre-Adoption Questions Recommendations for Successful Implementation Managing Change and Business Model Evolution 8. Additional Resources Links to Supplementary Articles and Resources 9. Conclusion 1. Definition and History Hexagonal architecture, also known as the Ports and Adapters architecture, was formulated by Alistair Cockburn in 2005. It is built on the fundamental idea that applications should be driven by both users and programs, batch scripts, or automated tests. Furthermore, it advocates for developing and testing these applications in isolation, free from dependencies on databases and execution systems. At the core of hexagonal architecture lies an essential principle: isolating the application’s business logic. This concept strongly resembles Domain-Driven Design (DDD), which emphasizes the importance of the business domain over technology. It’s important to note that hexagonal architecture and DDD are distinct concepts, although they can complement each other. 2. Principles of Hexagonal Architecture Concepts of Kernel, Port, and Adapter The fundamental principles of hexagonal architecture can be summarized in three key points: Isolation of Business Logic: The application’s business logic is separated from its technical implementation. This separation is achieved by breaking the application into three distinct parts: the Business (or Core) part, the Interface part (Drivers, Primary Actors, those who invoke the application), and the Infrastructure part (Driven, Secondary Actors, those called by the application). The Business part is isolated within a fictitious structure often represented by an hexagon, hence the name. Independence of the Business Part: According to hexagonal architecture, the Business part is independent of everything else. It is the Interface and Infrastructure parts that depend on the Business part. In other words, dependencies are established from the outside to the inside of the hexagon. Ports and Adapters: To enable communication between the Business part and the external world (Drivers and Driven), hexagonal architecture relies on the concept of ports and adapters. Adapters act as translators between the business domain and the external technical part. Ports define the interfaces with which adapters interact with the Core. Both are essential to manage changes in the external world without affecting the business logic. This organization ensures that any technical change has no impact on the business logic, reducing the risk of regressions and simplifying tests of the business part. 3. Comparison with Other Architectural Approaches Hexagonal architecture is a powerful architectural approach, but it is essential to compare it to other approaches to understand its advantages and specifics. Comparison with Traditional MVC Architecture Hexagonal architecture distinguishes itself from the traditional Model-View-Controller (MVC) architecture, which has long been used to develop applications. MVC architecture is based on separating components into three distinct layers: the model (which manages data and business logic), the view (which handles the user interface), and the controller (which acts as an intermediary between the model and the view). MVC is widely used in web and desktop applications. Hexagonal architecture, on the other hand, focuses on separating business logic from the rest of the application. The hexagon, at the core of this architecture, represents the business model, while ports and adapters facilitate communication with the outside world. This approach places a stronger emphasis on business logic and protects it from technical dependencies. Unlike MVC, which can sometimes lead to code where business logic is intertwined with presentation logic, hexagonal architecture promotes a clearer isolation. Connection with Domain-Driven Design (DDD) Hexagonal architecture and Domain-Driven Design (DDD) share a common philosophy: the importance of business logic in software development. However, it’s crucial to note that they are distinct concepts. DDD is a software design approach that emphasizes a deep understanding of the business domain and modeling that domain in code. It encourages collaboration between domain experts and developers to create a representation of the domain that best reflects reality. Hexagonal architecture is an architectural structure that implements the separation of business logic, infrastructure, and presentation. It draws inspiration from DDD by placing the business domain at the center of attention, but it focuses more on how components communicate, using ports and adapters. In summary, hexagonal architecture can be implemented in a DDD context to ensure that business logic is isolated from technical details, thus facilitating better application of DDD principles. Perspective of Hexagonal Architecture Compared to Clean Architecture Hexagonal architecture and Clean Architecture share similarities in their software development philosophies, but they differ in their implementation. Clean Architecture, popularized by Uncle Bob (Robert C. Martin), emphasizes separation of concerns and inverted dependencies. It is based on concentric circles, with each circle representing a software layer, and the business domain at the center. Outer circles contain the user interface, technical details, and databases. Hexagonal architecture, on the other hand, emphasizes business logic using the hexagon analogy. It provides stricter isolation of business logic by placing it at the heart of the application and protecting it from external dependencies. Ports and adapters are there to interact with the external world. In comparison, Clean Architecture offers a more structured approach with well-defined layers, while hexagonal architecture is more flexible in how it allows interaction with the business domain. It’s important to note that these two approaches can be complementary. Clean Architecture can be used to structure an entire application, while hexagonal architecture can be used to specifically isolate business logic. The choice between the two depends on the project’s needs and priorities. In conclusion, hexagonal architecture, while sharing fundamental principles with other approaches, stands out for its focus on business logic and its approach to separation of concerns. It can be a valuable solution for projects aiming to isolate and protect business logic while enabling increased scalability and adaptability. 4. Advantages of Hexagonal Architecture Hexagonal architecture offers several significant advantages that make it a popular approach in software development. Reduction of Functional Regression Risks during Technical Changes One of the main advantages of hexagonal architecture is its ability to reduce the risks of functional regressions during technical changes. Unlike some other architectures where business logic can be tightly coupled with technical details, hexagonal architecture isolates business logic within the hexagon. This means that when it’s necessary to modify elements such as the database, storage system, or other technical components, the business logic remains largely unchanged. This isolation minimizes potential functional regressions, ensuring the stability of the application even during technical alterations. Ease of Adding New Features and Modifying Existing Ones Hexagonal architecture lends itself well to adding new features and modifying existing ones. By isolating business logic, it becomes simpler to introduce new features without disrupting what already exists. This approach also promotes flexibility, as business knowledge is robustly protected. Developers can thus focus on expanding the application without fearing adverse effects on other parts of the system. Simplicity of Business Logic Testing Hexagonal architecture significantly simplifies testing the business logic of the application. Given that business logic is clearly isolated and has no technical dependencies, tests can concentrate on this crucial part of the application without handling complex dependencies. Automated tests become easier to write and execute, fostering practices like Test-Driven Development (TDD) and Behavior-Driven Development (BDD). This testing simplicity contributes to code quality improvement and ensures that the business logic functions as intended. Promotion of BDD and DDD Approaches Hexagonal architecture actively promotes Behavior-Driven Development (BDD) and Domain-Driven Design (DDD) approaches. BDD emphasizes the expected behavior of an application, while DDD focuses on modeling a complex business domain. By isolating business logic and using automated tests, hexagonal architecture facilitates the implementation of these approaches. It allows for a better understanding of expected behaviors and more precise modeling of the business domain, enhancing the overall quality of the application. In conclusion, hexagonal architecture offers major advantages, making it an attractive choice for software projects where business logic needs to be at the core of the application while allowing for maximum testability, scalability, and adaptability. 5. Limitations and Drawbacks Despite its many advantages, hexagonal architecture is not without limitations and drawbacks. It is essential to consider these aspects to determine if this approach is suitable for a specific project. Increased Number of Packages and Project Structure Complexity One notable drawback of hexagonal architecture is the increased number of packages and the complexity of the project structure. Compared to some other architectures, such as monolithic or 3-tier architecture, hexagonal architecture typically requires more extensive file organization. The strict separation of business logic, interface, and infrastructure results in a proliferation of packages. For some developers, this may seem excessive and lead to more complex project structure management. However, this complexity can be mitigated with proper code conventions and organization. Potential Inefficiency in Certain Contexts The efficiency of hexagonal architecture depends on the application context. This approach shines, particularly when business logic is complex and stable, and frequent technical changes are expected. However, in simpler contexts or when business stability is not a concern, hexagonal architecture may seem excessive. Small-scale projects or those where business logic is not the focus may not fully benefit from hexagonal architecture. In such cases, lighter architectures may be preferable. It is essential to carefully evaluate the specific project needs and determine if hexagonal architecture is the best solution. 6. Implementation of Hexagonal Architecture To implement hexagonal architecture in a project, it is crucial to follow several key steps. Additionally, a concrete example of implementation, such as a banking application, can help illustrate how this architecture works in practice. Steps to Implement Hexagonal Architecture Understanding the Business Domain: Before getting started, it is essential to have a solid understanding of the application’s business domain. Identify the actors, use cases, and business rules that will be at the core of your application. This step is crucial for properly isolating business logic. Creating the Business Model: Design a business model that is independent of technologies. This model should represent key concepts of the business domain, such as entities, values, and rules. Ensure that the model is stable and does not depend on technical details. Defining Ports and Adapters: Identify the points of interaction between the business model and the rest of the application. Create interfaces (ports) that define the contracts for these interactions. Adapters are responsible for implementing these interfaces. Implementing API and SPI Interfaces: Split the interfaces into two distinct parts: API interfaces (Application Provider Interface) for components that need to call the business model (Drivers) and SPI interfaces (Service Provider Interface) for retrieving data from the infrastructure (Driven). These interfaces should have clear business meanings. Developing Business Logic: Implement the business logic within the business model. This part of the code should be independent of technical details and data sources. Using inversion of control helps keep the model isolated. Creating Adapters: Develop adapters for the Interface and Infrastructure parts. These adapters are responsible for the interaction between the business model and the outside world. They enable adaptation to specific technologies while preserving the stability of the business logic. Business Logic Testing: As the business logic is well-isolated, you can test this part independently of data sources or presentation. Mocks can be used to simulate interactions with the adapters. Continuous Evolution: Domain knowledge may evolve over time. It is important to consider that the business model is not static and may require adjustments in the case of changes or developments. Example of Implementation in a Concrete Context (Banking Application) Consider a banking management application that allows handling banking actions like deposits and withdrawals. To implement hexagonal architecture in this context, here’s how it could unfold: Business Model: Create a business model representing concepts like bank accounts, transactions, and associated business rules. API Interfaces: Define API interfaces for banking actions, such as “Deposit money into a bank account” and “Withdraw money from a bank account.” These interfaces define contracts for these actions. SPI Interfaces: Create SPI interfaces for retrieving bank accounts and recording transactions. These interfaces are intended to interact with the infrastructure. Business Logic: Implement the business logic to manage banking actions in line with the defined business rules. Adapters: Develop adapters for the Presentation part (e.g., web controllers) and Persistence (e.g., data access layer using JPA). These adapters translate calls from API and SPI interfaces into concrete actions. With this implementation, the business logic remains isolated, allowing independent testing without worrying about technical details like databases or the user interface. Additionally, if technical changes occur, the business logic remains stable, reducing the risks of functional regressions. 7. Best Practices and Tips Adopting hexagonal architecture can bring many advantages, but it requires careful planning and implementation. Here are some best practices and tips to make the most of this architectural approach. Pre-Adoption Questions Nature of Your Application: Before opting for hexagonal architecture, consider the nature of your application. Is your application complex and has significant business logic? If yes, hexagonal architecture can be a good choice. If your application is simple, it may not fully benefit from this approach. Stability of Business Rules: Are the business rules of your application stable, or do they change frequently? Hexagonal architecture is particularly suitable for stable business rules because it minimizes the impact of technical changes. Need for Isolation: Do you need to isolate the business logic of the application from technical details and data sources? If business logic stability is crucial, hexagonal architecture can be an excellent choice. Recommendations for Successful Implementation Deep Understanding of the Business Domain: Before starting implementation, ensure a deep understanding of the business domain. Identify essential actors, use cases, and business rules. Poor domain understanding can lead to costly errors. Clear Business Model: Create a clear business model that is independent of technologies. Ensure that the model faithfully represents business domain concepts. Avoid introducing technical details into the model. Business Logic Testing: Invest time in testing the business logic. As it is isolated, tests can be conducted independently, ensuring that the business logic functions correctly. Adequate Documentation: Make sure to document your hexagonal architecture well. This includes descriptions of interfaces, adapters, and business logic. Clear documentation facilitates understanding and maintenance. Managing Change and Business Model Evolution Flexibility of the Business Model: Understand that the business model may evolve over time. Be prepared to make changes to the model when business needs change. Hexagonal architecture facilitates these evolutions while preserving stability. Version Management: If your application evolves, and new versions are deployed, ensure proper management of API and SPI interface versions. This ensures compatibility between different versions of the application. Follow Best Practices: Stay updated on best software development practices, such as using SOLID principles, managing automated tests, and implementing behavior-driven development (BDD) and domain-driven development (DDD) practices. 8. Additional Resources Links to Supplementary Articles and Resources If you want to deepen your understanding of hexagonal architecture, here are some additional resources that may be helpful: Alistair Cockburn’s Original Article: For an in-depth read on hexagonal architecture by its creator. A Detailed Article on the Same Subject: For an alternative perspective on the topic. How to start with Domain-Driven Design: DDD is often related to hexagonal architecture, and these resources provide an excellent resource for understanding domain-driven design. 8 resources to learn about SOLID design principles: SOLID principles are crucial for implementing hexagonal architecture. 9. Conclusion Advantages: Reducing the risks of functional regressions during technical changes. Ease of adding new features and modifying existing ones. Simplicity of business logic testing. Promotion of BDD and DDD approaches. Disadvantages: Increased number of packages and complexity of the project structure. Potential inefficiency in certain contexts. Hexagonal architecture, also known as ports and adapters architecture, is an architectural approach that isolates the business logic of an application from technical details and data sources. Here are the key takeaways: Hexagonal architecture emphasizes the separation of business logic (the Hexagon) from technical parts (the adapters). Dependencies go from the outside to the Hexagon. It facilitates reducing the risks of functional regressions during technical changes, allows the addition of new features, and simplifies testing of business logic. Hexagonal architecture can be associated with Domain-Driven Design (DDD), but it remains distinct. It also promotes SOLID principles and BDD and DDD approaches. Implementing hexagonal architecture involves creating a technology-independent business model, clear interfaces, and adapters to communicate with technical parts. The advantages of hexagonal architecture mainly manifest in contexts where business logic is stable and complex. In conclusion, hexagonal architecture offers a robust approach to developing applications with complex business logic. It can help you create scalable and easily maintainable applications."
    } ,
  
    {
      "title"    : "SDKMAN: Install Multiple JDK Versions and Much More!",
      "url"      : "/en/installer-java-sdkman/",
      "img"      : "PHBpY3R1cmU+CiAgPHNvdXJjZSBzcmNzZXQ9Ii9hc3NldHMvaW1nL3Nka21hbi1wb3N0LTMwMC05ZWRiNGQ3NzUuYXZpZiAxLjB4LCAvYXNzZXRzL2ltZy9zZGttYW4tcG9zdC02MDAtOWVkYjRkNzc1LmF2aWYgMi4weCIgdHlwZT0iaW1hZ2UvYXZpZiI+CiAgPHNvdXJjZSBzcmNzZXQ9Ii9hc3NldHMvaW1nL3Nka21hbi1wb3N0LTMwMC1jOTQzNDg5ODkuanBnIDEuMHgsIC9hc3NldHMvaW1nL3Nka21hbi1wb3N0LTYwMC1jOTQzNDg5ODkuanBnIDIuMHgiIHR5cGU9ImltYWdlL2pwZWciPgogIDxpbWcgc3JjPSIvYXNzZXRzL2ltZy9zZGttYW4tcG9zdC0zMDAtYzk0MzQ4OTg5LmpwZyIgYWx0PSJTREtNQU46IEluc3RhbGwgTXVsdGlwbGUgSkRLIFZlcnNpb25zIGFuZCBNdWNoIE1vcmUhIj4KPC9waWN0dXJlPgo=",
      "isencoded" : "y",
      "tags"     : "SDKMAN, Java, JDK, GraalVM, macOS, Tool",
      "content"  : "When a new stable version of macOS becomes available, there are times when I decide to perform a clean reinstall of my computer. After this process, it becomes necessary to reinstall all the essential tools for my work, which can be quite tedious. It is in this context that SDKMAN comes into play as an extremely practical utility that goes far beyond the mere management of JDKs on your computer. An Essential Tool: SDKMAN SDKMAN Installation Installation on macOS and Linux: Installation on Windows: JDK Installation Candidate vs Version Candidate Version Actual installation of the JDK JDK Selection Displaying the Current Version Displaying Installed Versions Switching Versions Taking It Further An Essential Tool: SDKMAN SDKMAN, short for “Software Development Kit Manager,” is an invaluable tool for Java developers who wish to effectively manage JDK (Java Development Kit) versions. It greatly simplifies the management of different JDK versions, providing straightforward control over your Java development environment. The advantages of SDKMAN for JDK version management include: Easy Installation: SDKMAN streamlines the JDK installation process by automating it. No more searching for downloads or dealing with tedious manual configurations and installations. With just a few simple commands, SDKMAN takes care of everything. Version Management: With SDKMAN, you can install multiple JDK versions simultaneously on your system. This allows you to easily switch between different versions based on your project requirements. Flexibility and Adaptability: SDKMAN offers a wide range of JDK versions, including both stable releases and the latest development versions. You can choose the version that best suits your project, considering specific features or compatibility requirements. Simplified Updates: SDKMAN makes updating your JDK a breeze. The tool notifies you about new available versions and enables quick and hassle-free installations. Cross-Platform Compatibility: Whether you’re working on macOS, Windows, or Linux, SDKMAN adapts to your environment. It ensures a consistent and seamless experience, regardless of the operating system you’re using. In summary, SDKMAN is a powerful and essential tool for Java developers. It simplifies JDK version management, keeping you up-to-date with the latest features and preventing compatibility issues. Whether you’re a seasoned developer or just starting out, SDKMAN gives you full control over your Java development environment, allowing you to work efficiently and without hassle. SDKMAN Installation To take advantage of the benefits offered by SDKMAN, it is essential to install it correctly on your system. Here are the detailed instructions for installing SDKMAN on macOS, Windows, and Linux: Installation on macOS and Linux: Step 1: Open your terminal. Step 2: Execute the following command to download the SDKMAN installation script: curl -s &quot;https://get.sdkman.io&quot; | bash Shell Step 3: Wait for the script to be downloaded and installed. Step 4: After installation: Load a new terminal session Or execute the following command to load SDKMAN into your current session: source &quot;$HOME/.sdkman/bin/sdkman-init.sh&quot; Shell Step 5: To verify if SDKMAN has been successfully installed, type the following command: sdk version Shell You should see: SDKMAN! script: 5.18.2 native: 0.4.2 Output Installation on Windows: On Windows, it’s necessary to have a Bash terminal. The easiest way is to install Windows Subsystem for Linux 2 (WSL2). To do this, follow the instructions provided on the Microsoft website https://learn.microsoft.com/en-us/windows/wsl/install. Once installed, proceed as if you were on macOS or Linux. Congratulations! You have now installed SDKMAN on your system. You are ready to enjoy its powerful features for JDK version management. NoteAll SDK installations will now be done under your HOME directory that is in the ~/.sdkman on linux-friendly OS. JDK Installation Once you have successfully installed SDKMAN, you can proceed with the installation of a specific JDK using this tool. Take a moment to think about it, and let’s explore some SDKMAN commands. Candidate vs Version As the name suggests, SDKMAN allows you to install SDKs, and Java is just one of the potential candidate’s. So, first, you need to choose the SDK (candidate) to install. Candidate To see the list of available SDK/candidates, use the following command: sdk list Shell NoteType q to exit the list. You can see that there are many things you can install. Just to name a few: Gradle Groovy Java Maven Micronaut SBT Scala Spring Boot Tomcat VisualVM Version Now, let’s focus on the candidate we’re interested in: Java. Let’s see the versions that SDKMAN offers us. To do that, let’s query SDKMAN: sdk list java Shell Here is the list I obtained: ================================================================================ Vendor | Use | Version | Dist | Status | Identifier -------------------------------------------------------------------------------- Corretto | | 21 | amzn | | 21-amzn | | 20.0.2 | amzn | | 20.0.2-amzn | | 20.0.1 | amzn | | 20.0.1-amzn | | 17.0.8 | amzn | | 17.0.8-amzn | | 17.0.7 | amzn | | 17.0.7-amzn | | 11.0.20 | amzn | | 11.0.20-amzn | | 11.0.19 | amzn | | 11.0.19-amzn | | 8.0.382 | amzn | | 8.0.382-amzn | | 8.0.372 | amzn | | 8.0.372-amzn Gluon | | 22.1.0.1.r17 | gln | | 22.1.0.1.r17-gln | | 22.1.0.1.r11 | gln | | 22.1.0.1.r11-gln GraalVM CE | | 21 | graalce | | 21-graalce | | 20.0.2 | graalce | | 20.0.2-graalce | | 20.0.1 | graalce | | 20.0.1-graalce | | 17.0.8 | graalce | | 17.0.8-graalce | | 17.0.7 | graalce | | 17.0.7-graalce GraalVM Oracle| | 21 | graal | | 21-graal | | 20.0.2 | graal | | 20.0.2-graal | | 20.0.1 | graal | | 20.0.1-graal | | 17.0.8 | graal | | 17.0.8-graal | | 17.0.7 | graal | | 17.0.7-graal Java.net | | 22.ea.18 | open | | 22.ea.18-open | | 22.ea.17 | open | | 22.ea.17-open | | 22.ea.16 | open | | 22.ea.16-open | | 22.ea.15 | open | | 22.ea.15-open | | 21 | open | | 21-open | | 21.ea.35 | open | | 21.ea.35-open | | 20.0.2 | open | | 20.0.2-open JetBrains | | 17.0.8 | jbr | | 17.0.8-jbr | | 17.0.8.1 | jbr | | 17.0.8.1-jbr | | 17.0.7 | jbr | | 17.0.7-jbr | | 11.0.14.1 | jbr | | 11.0.14.1-jbr Liberica | | 21.fx | librca | | 21.fx-librca | | 21 | librca | | 21-librca | | 20.0.2.fx | librca | | 20.0.2.fx-librca | | 20.0.2 | librca | | 20.0.2-librca | | 20.0.1.fx | librca | | 20.0.1.fx-librca | | 20.0.1 | librca | | 20.0.1-librca | | 17.0.8.fx | librca | | 17.0.8.fx-librca | | 17.0.8.1.fx | librca | | 17.0.8.1.fx-librca | | 17.0.8.1 | librca | | 17.0.8.1-librca | | 17.0.8 | librca | | 17.0.8-librca | | 17.0.7.fx | librca | | 17.0.7.fx-librca | | 17.0.7 | librca | | 17.0.7-librca | | 11.0.20.fx | librca | | 11.0.20.fx-librca | | 11.0.20.1.fx | librca | | 11.0.20.1.fx-librca | | 11.0.20.1 | librca | | 11.0.20.1-librca | | 11.0.20 | librca | | 11.0.20-librca | | 11.0.19.fx | librca | | 11.0.19.fx-librca | | 11.0.19 | librca | | 11.0.19-librca | | 8.0.382.fx | librca | | 8.0.382.fx-librca | | 8.0.382 | librca | | 8.0.382-librca | | 8.0.372.fx | librca | | 8.0.372.fx-librca | | 8.0.372 | librca | | 8.0.372-librca Liberica NIK | | 23.r20 | nik | | 23.r20-nik | | 23.r17 | nik | | 23.r17-nik | | 23.1.r21 | nik | | 23.1.r21-nik | | 23.0.1.r20 | nik | | 23.0.1.r20-nik | | 23.0.1.r17 | nik | | 23.0.1.r17-nik | | 22.3.3.r17 | nik | | 22.3.3.r17-nik | | 22.3.3.r11 | nik | | 22.3.3.r11-nik | | 22.3.2.r17 | nik | | 22.3.2.r17-nik | | 22.3.2.r11 | nik | | 22.3.2.r11-nik Microsoft | | 21 | ms | | 21-ms | | 17.0.8.1 | ms | | 17.0.8.1-ms | | 17.0.8 | ms | | 17.0.8-ms | | 17.0.7 | ms | | 17.0.7-ms | | 11.0.20.1 | ms | | 11.0.20.1-ms | | 11.0.20 | ms | | 11.0.20-ms | | 11.0.19 | ms | | 11.0.19-ms Oracle | | 21 | oracle | | 21-oracle | | 20.0.2 | oracle | | 20.0.2-oracle | | 20.0.1 | oracle | | 20.0.1-oracle | | 17.0.8 | oracle | | 17.0.8-oracle | | 17.0.7 | oracle | | 17.0.7-oracle SapMachine | | 21 | sapmchn | | 21-sapmchn | | 20.0.2 | sapmchn | | 20.0.2-sapmchn | | 20.0.1 | sapmchn | | 20.0.1-sapmchn | | 17.0.8 | sapmchn | | 17.0.8-sapmchn | | 17.0.8.1 | sapmchn | | 17.0.8.1-sapmchn | | 17.0.7 | sapmchn | | 17.0.7-sapmchn | | 11.0.20 | sapmchn | | 11.0.20-sapmchn | | 11.0.20.1 | sapmchn | | 11.0.20.1-sapmchn | | 11.0.19 | sapmchn | | 11.0.19-sapmchn Semeru | | 20.0.2 | sem | | 20.0.2-sem | | 20.0.1 | sem | | 20.0.1-sem | | 17.0.8 | sem | | 17.0.8-sem | | 17.0.8.1 | sem | | 17.0.8.1-sem | | 17.0.7 | sem | | 17.0.7-sem | | 11.0.20 | sem | | 11.0.20-sem | | 11.0.20.1 | sem | | 11.0.20.1-sem | | 11.0.19 | sem | | 11.0.19-sem Temurin | | 20.0.2 | tem | | 20.0.2-tem | | 20.0.1 | tem | | 20.0.1-tem | | 17.0.8 | tem | | 17.0.8-tem | | 17.0.8.1 | tem | | 17.0.8.1-tem | | 17.0.7 | tem | | 17.0.7-tem | | 11.0.20 | tem | | 11.0.20-tem | | 11.0.20.1 | tem | | 11.0.20.1-tem | | 11.0.19 | tem | | 11.0.19-tem Tencent | | 17.0.8 | kona | | 17.0.8-kona | | 17.0.7 | kona | | 17.0.7-kona | | 11.0.20 | kona | | 11.0.20-kona | | 11.0.19 | kona | | 11.0.19-kona | | 8.0.382 | kona | | 8.0.382-kona | | 8.0.372 | kona | | 8.0.372-kona Zulu | | 21 | zulu | | 21-zulu | | 21.fx | zulu | | 21.fx-zulu | | 20.0.2 | zulu | | 20.0.2-zulu | | 20.0.2.fx | zulu | | 20.0.2.fx-zulu | | 20.0.1 | zulu | | 20.0.1-zulu | | 20.0.1.fx | zulu | | 20.0.1.fx-zulu | | 17.0.8 | zulu | | 17.0.8-zulu | | 17.0.8.fx | zulu | | 17.0.8.fx-zulu | | 17.0.8.1 | zulu | | 17.0.8.1-zulu | | 17.0.8.1.fx | zulu | | 17.0.8.1.fx-zulu | | 17.0.7 | zulu | | 17.0.7-zulu | | 17.0.7.fx | zulu | | 17.0.7.fx-zulu | | 11.0.20 | zulu | | 11.0.20-zulu | | 11.0.20.fx | zulu | | 11.0.20.fx-zulu | | 11.0.20.1 | zulu | | 11.0.20.1-zulu | | 11.0.20.1.fx | zulu | | 11.0.20.1.fx-zulu | | 11.0.19 | zulu | | 11.0.19-zulu | | 11.0.19.fx | zulu | | 11.0.19.fx-zulu | | 8.0.382 | zulu | | 8.0.382-zulu | | 8.0.382.fx | zulu | | 8.0.382.fx-zulu | | 8.0.372 | zulu | | 8.0.372-zulu | | 8.0.372.fx | zulu | | 8.0.372.fx-zulu ================================================================================ Omit Identifier to install default version 17.0.8.1-tem: $ sdk install java Use TAB completion to discover available versions $ sdk install java [TAB] Or install a specific version by Identifier: $ sdk install java 17.0.8.1-tem Hit Q to exit this list view ================================================================================ Output Actual installation of the JDK Place your bets … me, it’s done! As I am working on the build of Java code in native code at the moment, I choose GraalVM CE in version 20.0.2 and I select its identifier 20.0.2-graalce. It’s your turn. To install it, I run the command: sdk install java 20.0.2-graalce Shell Which gives me the output, the installation process Downloading: java 20.0.2-graalce In progress... ################################## 100.0% Repackaging Java 20.0.2-graalce... Done repackaging... Cleaning up residual files... Installing: java 20.0.2-graalce Done installing! Setting java 20.0.2-graalce as default. Output Done! No, not yet … I need more JDKs for comparison. Moreover, this is why we installed this tool. For my part, I install two others: sdk install java 21.fx-librca Shell sdk install java 23.r20-nik Shell JDK Selection Let’s now see how to select a specific version of Java. Displaying the Current Version Let’s see what the sdk command tells us: % sdk current java Using java version 20.0.2-graalce Output And let’s see what java tells us: % java --version openjdk 20.0.2 2023-07-18 OpenJDK Runtime Environment GraalVM CE 20.0.2+9.1 (build 20.0.2+9-jvmci-23.0-b15) OpenJDK 64-Bit Server VM GraalVM CE 20.0.2+9.1 (build 20.0.2+9-jvmci-23.0-b15, mixed mode, sharing) Output Displaying Installed Versions sdk list java Shell Switching Versions % sdk use java 21.fx-librca Using java version 21.fx-librca in this shell. Output Let’s verify with Java: % java --version openjdk 21 2023-09-19 LTS OpenJDK Runtime Environment (build 21+37-LTS) OpenJDK 64-Bit Server VM (build 21+37-LTS, mixed mode, sharing) Output And there you have it… Taking It Further You can find additional useful commands at this address: https://sdkman.io/usage, including the env command: https://sdkman.io/usage#env. Now it’s your turn to explore further. Cheers…"
    } ,
  
    {
      "title"    : "The Future of DevOps: How AI Will Transform the Software Industry",
      "url"      : "/en/future-devops-with-ai/",
      "img"      : "PHBpY3R1cmU+CiAgPHNvdXJjZSBzcmNzZXQ9Ii9hc3NldHMvaW1nL2Z1dHVyZS1kZXZvcHMtd2l0aC1haS0zMDAtMmM1YjdkNDYwLmF2aWYgMS4weCwgL2Fzc2V0cy9pbWcvZnV0dXJlLWRldm9wcy13aXRoLWFpLTYwMC0yYzViN2Q0NjAuYXZpZiAyLjB4IiB0eXBlPSJpbWFnZS9hdmlmIj4KICA8c291cmNlIHNyY3NldD0iL2Fzc2V0cy9pbWcvZnV0dXJlLWRldm9wcy13aXRoLWFpLTMwMC02NTFlYTdmMzMuanBnIDEuMHgsIC9hc3NldHMvaW1nL2Z1dHVyZS1kZXZvcHMtd2l0aC1haS02MDAtNjUxZWE3ZjMzLmpwZyAyLjB4IiB0eXBlPSJpbWFnZS9qcGVnIj4KICA8aW1nIHNyYz0iL2Fzc2V0cy9pbWcvZnV0dXJlLWRldm9wcy13aXRoLWFpLTMwMC02NTFlYTdmMzMuanBnIiBhbHQ9IlRoZSBGdXR1cmUgb2YgRGV2T3BzOiBIb3cgQUkgV2lsbCBUcmFuc2Zvcm0gdGhlIFNvZnR3YXJlIEluZHVzdHJ5Ij4KPC9waWN0dXJlPgo=",
      "isencoded" : "y",
      "tags"     : "DevOps, AI, Transformation, Trend",
      "content"  : "The integration of Artificial Intelligence (AI) into the software industry represents a major pivot towards the future of this industry. In this ever-evolving digital era, it is essential to seize the opportunities presented by AI advancements and understand how they will shape all phases and lifecycles of applications. The main objective of this post is to take stock of this changing landscape, helping you grasp how AI will reshape the entire process of development, deployment, and application management, and why this is crucial for companies looking to remain competitive in the constantly evolving digital world. The Foundations of AI in DevOps Definition of AI and DevOps The Importance of AI in DevOps Enhanced Automation Concrete Examples Prediction and Early Problem Detection Use Cases Enhanced Security and Compliance Examples of Scenarios Optimizing Incident Management Use Cases Impacts on the Application Lifecycle Considerations for Development and Operations Teams Challenges and Obstacles to Overcome Identification of Potential Challenges Strategies to Overcome These Obstacles Case Studies and Lessons Learned Case Studies Lessons Learned Conclusion The Foundations of AI in DevOps To better understand the implications of Artificial Intelligence (AI) in DevOps, it’s essential to lay the groundwork by defining these two key domains. Definition of AI and DevOps Artificial Intelligence (AI) refers to the creation of computer systems capable of performing tasks that would typically require human intelligence, such as solving complex problems, making decisions, and learning from data. It encompasses a wide range of technologies, including machine learning, deep learning, and natural language processing. DevOps, on the other hand, is a software development methodology aimed at breaking down silos between development (Dev) and operations (Ops) teams, fostering close collaboration for continuous, fast, and reliable software delivery. It relies on principles such as automation, configuration management, continuous deployment, and monitoring. The Importance of AI in DevOps The integration of AI into DevOps is a natural evolution as companies seek to enhance the efficiency, quality, and security of their software development. This union offers several key benefits: Enhanced Automation: AI enables the automation of complex tasks, reducing human errors and speeding up application deployment and management processes. Prediction and Early Problem Detection: By using AI to analyze real-time data, DevOps teams can predict potential issues before they become critical, minimizing downtime. Improved Security: AI strengthens security by quickly identifying potential threats and assisting in the implementation of proactive defenses. Resource Optimization: With AI, IT resources can be used more efficiently, reducing operational costs. In summary, the integration of AI into DevOps is based on a solid understanding of AI and DevOps principles and their synergy to promote smarter and more efficient development and deployment practices. Let’s delve into these four key trends in detail. Enhanced Automation One of the major trends of Artificial Intelligence (AI) in DevOps is enhanced automation. This evolution is based on the idea of using AI to automate more tasks within the DevOps pipeline, increasing efficiency and reducing manual workload. Traditionally, DevOps has already emphasized automation, but AI adds an extra dimension by enabling smarter automation. AI can be used to make complex decisions, such as resource optimization, workload management, and automatic error correction. This capacity for intelligent automation frees DevOps teams from repetitive and tedious tasks, allowing them to focus on more strategic and creative aspects of their work. Concrete Examples To illustrate this trend, here are some concrete examples of enhanced automation through AI: Optimizing Resource Allocation: AI can analyze real-time resource needs of applications and automatically allocate resources optimally based on the current workload. Intelligent Continuous Deployment: AI systems can make real-time decisions about deploying a new version based on application performance and user feedback. Automatic Issue Resolution: AI can identify and automatically resolve common issues, such as configuration errors, without human intervention. Enhanced Automated Testing: AI-powered automated tests can generate more complex and realistic test scenarios, improving test coverage. Enhanced automation through AI represents a significant shift in how DevOps operations are conducted. It promises smoother processes, more reliable deployments, and more efficient resource utilization, contributing to strengthening a company’s competitiveness in an ever-evolving digital environment. Prediction and Early Problem Detection Another major trend of Artificial Intelligence (AI) in DevOps concerns AI’s ability to predict potential problems and enable early detection. This development is crucial for ensuring system reliability and operational continuity. AI brings a predictive dimension to DevOps operations by using machine learning models to analyze real-time data. It can anticipate potential issues by identifying trends and spotting anomalies in the data. This capability allows for preventive measures before a problem becomes critical, minimizing downtime and disruptions for end users. Use Cases To illustrate this trend, here are some use cases of AI for anticipating problems in DevOps: Prediction of Hardware Failures: AI can analyze hardware performance data and identify early signs of impending failures, enabling preventive replacement. Anticipating Security Issues: By monitoring network traffic patterns and analyzing security logs, AI can spot suspicious activities and detect potential attacks before they cause damage. Optimizing Cloud Resources: AI can predict cloud resource needs based on past usage trends, allowing for more accurate resource allocation and cost reduction. Early Detection of Code Bugs: AI-powered static analysis tools can identify potential code issues even before they are deployed, reducing regressions. AI can play a vital role by seamlessly integrating into monitoring and incident management processes, contributing to a proactive approach to application reliability and operational continuity. Enhanced Security and Compliance Artificial Intelligence (AI) in the field of DevOps plays a crucial role in improving the security and compliance of DevOps pipelines. This trend highlights how AI strengthens these essential aspects of software operations management. AI contributes in several ways to enhance security and compliance in DevOps pipelines. Firstly, it can automatically detect anomalies and suspicious activities, helping identify potential threats at an early stage. Furthermore, it can automate security patch management and ensure that systems are always up-to-date in terms of security. Lastly, AI can analyze configurations and compliance policies, reporting deviations and assisting in ensuring that regulations and standards are adhered to. Examples of Scenarios To illustrate this trend, here are some concrete examples of improved security scenarios through AI: Enhanced Intrusion Detection: AI systems can continuously monitor network traffic and security logs to identify unusual activities, such as intrusion attempts, and trigger real-time alerts. Automated Patch Management: AI can analyze known vulnerabilities in software and systems, then automate the patch deployment process to minimize the risk of security breaches. Continuous Compliance Configuration Analysis: AI can continuously compare system configurations with compliance policies and report any discrepancies, thus helping maintain a compliant environment. Phishing Attack Prevention: AI solutions can analyze emails and messages to detect phishing attempts, helping protect users against attacks. The integration of AI into DevOps enhances system robustness and builds trust among users and stakeholders in deployed applications. Optimizing Incident Management One of the prominent trends in integrating Artificial Intelligence (AI) into DevOps is optimizing incident management. This trend highlights the crucial role of AI in efficiently managing and resolving operational problems and incidents. AI has become a valuable ally for DevOps teams when it comes to incident management. It offers the ability to monitor systems and applications in real-time, quickly identify anomalies, and report potential incidents. Furthermore, it can assist in diagnosing the underlying cause of issues, speeding up their resolution. Use Cases AI accelerates incident management in several ways: Early Detection: By monitoring real-time metrics and logs, AI can detect warning signs before a major incident occurs, enabling early intervention. Instant Notification: When an anomaly or incident is detected, AI can automatically generate real-time notifications to inform DevOps teams, ensuring a rapid response. Advanced Diagnostics: AI can analyze performance data to understand the root cause of an incident, allowing for more precise problem resolution. Automated Resolution: In some cases, AI can suggest corrective actions or even automatically resolve minor incidents, reducing manual workload. Optimizing incident management through AI enhances the ability of DevOps teams to effectively handle issues and minimize downtime. It also contributes to continuous process improvement and a better user experience by ensuring the availability of applications and services. Impacts on the Application Lifecycle Artificial Intelligence (AI) in DevOps has a significant impact on the design, development, deployment, and management of applications. This trend redefines how teams approach every aspect of the application lifecycle: Design: The integration of AI influences the design of applications by enabling the addition of intelligent features such as content recommendation, real-time personalization, and user needs detection. Applications are now designed to leverage machine learning models and AI algorithms, making them smarter and more responsive to user needs. Development: AI is also used to accelerate application development. We are starting to see AI tools that automatically generate code based on specifications, significantly reducing development time. Deployment: AI plays a key role in application deployment by automating many tasks, from managing cloud resources to selecting optimal deployment environments. Decisions regarding production deployment are often made based on real-time performance and usage data analysis. Operation: AI is revolutionizing application operation as well. It offers advanced monitoring capabilities, enabling early problem detection and proactive resolution. Moreover, it can optimize patch management, ensuring that applications are always secure and up-to-date. Management: Lastly, application management is transformed by AI. AI systems can provide advanced insights into performance, usage, and future needs. This helps management teams make informed decisions regarding investment, resource optimization, and strategic planning. Considerations for Development and Operations Teams Development and operations teams need to take several considerations into account when integrating AI into their practices: Training and Skills: Team members must be trained in the use of AI and associated tools. This requires an investment in developing AI skills within the organization. Collaboration: DevOps teams must collaborate closely with AI experts to design applications that fully leverage AI capabilities. Effective communication between teams is essential. Data Governance: AI relies on high-quality data. Teams must establish rigorous data governance to ensure that AI models are powered by reliable data compliant with regulations. Security: AI can introduce new security challenges. Teams must implement robust security measures to protect AI models and sensitive data. In conclusion, the integration of AI into DevOps fundamentally transforms how teams design, deploy, and manage applications. It offers opportunities for smarter and more responsive applications but also requires adjustments in skills, collaboration, and data governance within DevOps teams. Challenges and Obstacles to Overcome Artificial Intelligence in DevOps offers numerous advantages, but it is not without challenges. It is essential to identify these potential obstacles and implement strategies to overcome them. Identification of Potential Challenges Increased Complexity: AI introduces a new layer of complexity to DevOps operations due to the need to manage machine learning models, large datasets, and AI-specific infrastructures. Need for Quality Data: AI models rely on high-quality data. Acquiring, preparing, and managing this data can be a challenge, especially in environments where data is fragmented or unstructured. AI Skills: Successful AI integration requires specialized AI and machine learning skills. The lack of qualified personnel can be a major hurdle. Security and Privacy: AI can raise concerns about security and privacy, particularly regarding the protection of AI models and the management of sensitive data. Strategies to Overcome These Obstacles Interdisciplinary Collaboration: Encourage close collaboration between DevOps teams, AI experts, and data scientists. This collaboration fosters mutual understanding and efficient problem-solving. Data Governance: Establish robust data governance to ensure the quality, security, and compliance of data used in AI projects. Training and Education: Invest in the training and education of your DevOps team to acquire AI skills. This may include online training programs, workshops, and certifications. Accessible AI Solutions: Look for AI solutions that simplify the integration and management of AI in DevOps pipelines. Many platforms offer user-friendly tools. Enhanced Security: Pay special attention to the security of AI models and data. Use encryption, monitoring, and access control methods to protect these assets. By overcoming these challenges, DevOps teams can fully harness the benefits of AI while minimizing risks. It is important to recognize that AI integration is an evolving process that requires perseverance and adaptability on the part of DevOps teams. Case Studies and Lessons Learned The successful integration of Artificial Intelligence (AI) into DevOps and software practices is illustrated by several case studies of companies that have successfully embraced the challenge. These experiences provide valuable insights and best practices to remember: Case Studies Netflix: Netflix uses AI to enhance content personalization for its subscribers. The company has integrated AI models into its DevOps pipeline to recommend movies and series based on each user’s viewing behavior. This has significantly increased subscriber retention and customer satisfaction. Spotify: Spotify uses AI to analyze user music preferences and generate personalized playlists. AI can be used for creating tailored content, thus improving user engagement. Microsoft: Microsoft uses AI to enhance the security of its products by detecting threats and strengthening data protection. AI can enhance security and compliance in the software industry. Google: Google uses AI to optimize data center management. The company has developed AI systems that predict resource demand and adjust server allocation automatically to save energy and reduce operational costs. IBM: IBM has integrated AI into its DevOps process to automatically detect security vulnerabilities in source code. This has significantly reduced security risks and ensured safer applications. Lessons Learned Start Small: Successful companies often began with pilot projects before deploying AI at scale. Starting small allows testing concepts and adjusting practices before generalizing them. Interdisciplinary Collaboration: Close collaboration between DevOps teams, data scientists, and AI experts is essential. Effective communication fosters mutual understanding of needs and requirements. Measurement and Evaluation: Implement tracking measures to assess the impact of AI on DevOps performance. This allows for strategy adjustments based on real results. Security and Privacy: Pay special attention to the security of AI models and data used. Ensure that security mechanisms are integrated from the beginning. Continuous Training: Invest in continuous training for your staff to develop the skills needed for AI integration. Technologies evolve rapidly, and a well-trained team is a valuable asset. Conclusion The integration of Artificial Intelligence (AI) into the DevOps domain opens up new perspectives and significant transformations. Through our exploration of future trends in this integration, we can summarize the key points that define the future of DevOps. We have seen how enhanced automation through AI, prediction and early issue detection, improved security and compliance, as well as incident management optimization, redefine how we design, deploy, and manage applications. These trends have a direct impact on continuous delivery, making it more agile, reliable, and secure. As in many fields, DevOps seems inseparable from AI, and it is essential for companies to adapt to these new realities. We therefore encourage exploration and gradual adoption of these trends to remain competitive in the ever-evolving world of technology. Collaboration between DevOps teams, AI experts, and data scientists will become increasingly valuable in creating intelligent and responsive DevOps pipelines. In summary, a new era dawns for DevOps, with AI as a catalyst for innovation and continuous improvement. By embracing these trends, companies can not only maintain their competitiveness but also pave the way for new possibilities in the development, deployment, and management of applications."
    } ,
  
    {
      "title"    : "DevOps Defense: How to Counter a Zombie Attack",
      "url"      : "/en/devops-vs-zombies/",
      "img"      : "PHBpY3R1cmU+CiAgPHNvdXJjZSBzcmNzZXQ9Ii9hc3NldHMvaW1nL2Rldm9wcy12cy16b21iaWVzLTMwMC05NTMxZGNiOGUuYXZpZiAxLjB4LCAvYXNzZXRzL2ltZy9kZXZvcHMtdnMtem9tYmllcy02MDAtOTUzMWRjYjhlLmF2aWYgMi4weCIgdHlwZT0iaW1hZ2UvYXZpZiI+CiAgPHNvdXJjZSBzcmNzZXQ9Ii9hc3NldHMvaW1nL2Rldm9wcy12cy16b21iaWVzLTMwMC1jZWI0M2Q5M2YuanBnIDEuMHgsIC9hc3NldHMvaW1nL2Rldm9wcy12cy16b21iaWVzLTYwMC1jZWI0M2Q5M2YuanBnIDIuMHgiIHR5cGU9ImltYWdlL2pwZWciPgogIDxpbWcgc3JjPSIvYXNzZXRzL2ltZy9kZXZvcHMtdnMtem9tYmllcy0zMDAtY2ViNDNkOTNmLmpwZyIgYWx0PSJEZXZPcHMgRGVmZW5zZTogSG93IHRvIENvdW50ZXIgYSBab21iaWUgQXR0YWNrIj4KPC9waWN0dXJlPgo=",
      "isencoded" : "y",
      "tags"     : "DevOps, SRE, ZeroTrust, Cybersecurity, Zombie",
      "content"  : "Let’s embark on a thought experiment that envisions a zombie attack on an infrastructure while contemplating how DevOps principles can be implemented to ensure effective protection against this threat and maintain the operational continuity of our system. Throughout this thought experiment, we will explore how DevOps principles, along with those of Site Reliability Engineering (SRE), can be effectively applied to secure our infrastructure and keep its functions operational. We will imagine what the different stages of automated defense should be, from early detection of zombies to the implementation of physical or virtual barriers, all the way to automated responses to attacks. We will also discuss the concept of resilience and the importance of team training to handle unexpected situations. While this scenario may not likely feature among the future hit series on your favorite video-on-demand platform, it offers an intriguing perspective on the impact of automation and site reliability engineering in the realm of DevOps and, more broadly, in managing the operational continuity of our technological infrastructures. Zombies Attack! The Zombie Threat DevOps Defense Automating Defense Early Detection of Zombies Implementing Automated Barriers Automated Response to Attacks Resilience and Redundancy DevOps Team Training Conclusion Zombies Attack! The Zombie Threat Our technological infrastructure is suddenly faced with an unexpected menace: a horde of hungry zombies. This threat poses a unique challenge, characterized by unpredictable attacks and a lack of conventional logic. In this context, zombies symbolize a latent danger to the continuous operation of our infrastructure. DevOps Defense DevOps principles can be mobilized for defense. Indeed, DevOps, traditionally focused on operational efficiency and collaboration, provides a solid framework for responding to unforeseen situations. Automation, early detection, and rapid response to attacks will be key elements of our defense strategy. Now, let’s delve into more specific solutions for automating the defense of our infrastructure against unexpected assailants. Automating Defense One initial solution is to implement automated mechanisms, as advocated by DevOps principles. Early Detection of Zombies The first crucial step in defending our infrastructure against zombies is to detect their presence as early as possible. In the realm of DevOps, this equates to establishing automated monitoring systems capable of identifying early signs of the attack. These early detection systems are essential for enabling a rapid and coordinated response: Real-time Monitoring: Automation of real-time monitoring is essential for swiftly detecting unusual activities or signs of zombies. Utilize tools for monitoring logs, metrics, and performance to continually monitor the state of your infrastructure. Automated Alerts: Configure automated alerts that trigger as soon as suspicious activity is detected. These alerts can be sent via email, through instant messaging systems, or dedicated communication channels within your teams. Behavior Pattern Analysis: Employ behavior pattern analysis to identify anomalies. Machine learning tools can be trained to recognize normal behavior patterns in your infrastructure and flag any deviations. Automated Event Correlation: Implement automated event correlation systems to link monitoring data and identify potential indicators of compromise. These systems can identify patterns that would be challenging to spot manually. Integration with Security Tools: Integrate your automated monitoring systems with security tools such as firewalls, intrusion detection systems (IDS), and intrusion prevention systems (IPS). This enables a swifter response to detected threats. Automated Penetration Testing: Implement automated penetration tests to identify potential vulnerabilities in your infrastructure. These tests can be run regularly to detect weaknesses before they can be exploited by assailants. Automated Monitoring Dashboards: Create automated monitoring dashboards that provide a real-time overview of your infrastructure’s status. These dashboards enable your teams to quickly track key indicators. Implementing Automated Barriers Once the presence of zombies is detected, the rapid implementation of physical or virtual barriers is necessary to halt them. DevOps can assist in automating this process using scripts and rules to deploy barriers in a coordinated manner. Isolation Scripts: Develop automated scripts capable of isolating parts of the infrastructure affected by zombies. These scripts should be able to disable access to compromised areas and prevent the spread of infection. Deployment of Dynamic Firewalls: Utilize dynamic firewalls that can be configured automatically to block traffic from suspicious or unauthorized sources. These firewall rules can be deployed in response to security alerts. Automated Resource Scheduling: Employ automated resource scheduling to distribute the load in order to avoid a concentration of zombies in a given area. Automation can automatically distribute traffic to healthy servers. Automated Traffic Redirection: Implement mechanisms for automated traffic redirection to divert zombies to quarantine zones. This can be achieved using redirection rules at the network switch or server level. Automated Patch Deployment: If vulnerabilities are exploited by zombies, automatically deploy patches to plug them. Automation can enable a swift response by applying security patches. Credential and Access Management: Automate credential and access management to ensure that only authorized individuals have access to certain parts of the infrastructure. Unused or compromised accounts should be automatically disabled. Security Certificate Management: Utilize automation for security certificate management. Expired or invalid certificates should be renewed automatically to prevent security breaches. Automated Response to Attacks When a zombie attack is underway, automated response is imperative to minimize potential damage and ensure operational continuity. Automation can be used to manage the response to attacks in a consistent and rapid manner, while minimizing disruptions to the rest of the infrastructure. Real-time Data Replication: Employ real-time data replication to automatically back up critical data. This minimizes data loss during an attack and facilitates post-incident recovery. Automatic Service Recovery: Implement mechanisms for automatic service recovery. In the event of service unavailability, automation can automatically restart the service or switch to a backup instance. Automated Security Reassessment: Automate the reassessment of the security of the entire infrastructure following an attack. This ensures that no residual vulnerabilities are left behind by the attack. Resilience and Redundancy The resilience of the infrastructure and the implementation of redundancy mechanisms play a crucial role in preventing collapse in the event of a zombie attack. Let’s explore the significance of these measures in ensuring operational continuity. Business Continuity Plan: A well-defined business continuity plan is essential to anticipate and manage the consequences of a zombie attack. This plan should include detailed procedures for post-incident recovery, resource reallocation, and crisis management. System Redundancy: System redundancy involves having backup components or servers ready to take over in case of failure. By automating failure detection and switchover to backup systems, you can minimize service interruptions. Automated Failover: Automation can facilitate seamless failover between primary and backup systems when needed. Automated failover systems can be configured to respond quickly to failures and ensure smooth operational continuity. Automated Backup and Restoration: Automate the process of backing up critical data and restoring it in case of loss. Regular automated backups ensure data protection and swift recovery. Automated Resilience Testing: Schedule automated resilience tests to periodically assess your infrastructure’s ability to withstand attacks. These tests can simulate zombie attack scenarios and help identify areas in need of improvement. Ongoing DevOps Team Training: Ensure that your DevOps team is trained in resilience and redundancy management. Automation of training and simulation exercises can contribute to strengthening the team’s skills. DevOps Team Training Team training holds crucial importance in dealing with unforeseen situations, even as improbable as a zombie apocalypse. The Need for Training: Preparedness is key in managing crisis scenarios, even the most unusual ones. Team training ensures that each member understands their role in the event of a zombie attack and knows how to act quickly and effectively. Zombie Attack Simulation Exercises: Organize simulation exercises to train your team. These fun exercises simulate an attack and allow members to practice their skills in a real-world scenario. Emergency Scenarios and Response Protocols: Develop specific emergency scenarios and tailored response protocols. Ensure that every team member understands these protocols and knows how to follow them when needed. Security Automation Training: Security automation training is essential to ensure that your team can respond rapidly and cohesively. Train team members in using automated tools and scripts to enhance security. Continuous Skill Updates: The field of security and crisis management is constantly evolving. Ensure that your team regularly attends training and courses to stay updated with the latest trends and best practices in security. Interdepartmental Collaboration: Foster interdepartmental collaboration within your teams. Ensure that each member understands the roles of others and can work effectively as a team to address challenges. Conclusion While the idea of a zombie apocalypse may seem like a highly improbable risk in the real world, this exploration highlights the importance of preparation, training, and automation, which are fundamental elements of DevOps. They can be used to mitigate an attack in any scenario. Ultimately, they ensure operational continuity, security, and infrastructure resilience, whether the threat is fictional or real."
    } ,
  
    {
      "title"    : "Zero Trust: The New Norm for Securing Your Kubernetes Cluster",
      "url"      : "/en/zero-trust-kubernetes/",
      "img"      : "PHBpY3R1cmU+CiAgPHNvdXJjZSBzcmNzZXQ9Ii9hc3NldHMvaW1nL3plcm8tdHJ1c3Qta3ViZXJuZXRlcy0zMDAtYjdlZDhiZThkLmF2aWYgMS4weCwgL2Fzc2V0cy9pbWcvemVyby10cnVzdC1rdWJlcm5ldGVzLTYwMC1iN2VkOGJlOGQuYXZpZiAyLjB4IiB0eXBlPSJpbWFnZS9hdmlmIj4KICA8c291cmNlIHNyY3NldD0iL2Fzc2V0cy9pbWcvemVyby10cnVzdC1rdWJlcm5ldGVzLTMwMC1hNTBjMTJlZmMuanBnIDEuMHgsIC9hc3NldHMvaW1nL3plcm8tdHJ1c3Qta3ViZXJuZXRlcy02MDAtYTUwYzEyZWZjLmpwZyAyLjB4IiB0eXBlPSJpbWFnZS9qcGVnIj4KICA8aW1nIHNyYz0iL2Fzc2V0cy9pbWcvemVyby10cnVzdC1rdWJlcm5ldGVzLTMwMC1hNTBjMTJlZmMuanBnIiBhbHQ9Ilplcm8gVHJ1c3Q6IFRoZSBOZXcgTm9ybSBmb3IgU2VjdXJpbmcgWW91ciBLdWJlcm5ldGVzIENsdXN0ZXIiPgo8L3BpY3R1cmU+Cg==",
      "isencoded" : "y",
      "tags"     : "Kubernetes, ZeroTrust, Cybersecurity, AI",
      "content"  : "Cybersecurity is a major concern in today’s ever-evolving technological landscape. With the proliferation of digital threats, traditional trust-based approaches are no longer sufficient to protect our systems. This is where the concept of Zero Trust comes into play. Zero Trust is a security model that assumes any attempt to access the network or resources must be verified, regardless of the user’s or device’s location. In other words, no entity can be presumed safe without explicit verification. In this context, adopting a Zero Trust approach is crucial in Kubernetes environments, which have become the cornerstones of modern IT infrastructures. Kubernetes, as a container management system, facilitates the deployment and management of applications in distributed and highly dynamic environments. However, the open and complex nature of Kubernetes also makes the cluster vulnerable to various potential attacks. The purpose of this article is to provide readers with a comprehensive guide on implementing a Zero Trust architecture in their Kubernetes cluster. By following the presented steps, administrators and security teams can significantly enhance the security of their cluster, thereby reducing the risks of intrusions, unauthorized access, and data compromises. Understanding the Basic Principles of Zero Trust Defining the Concept of Zero Trust and Its Core Principles Why Implicit Trust Should No Longer Be Granted in a Modern IT Environment The Benefits of Adopting a Zero Trust Architecture Assessing the Security of Your Existing Kubernetes Cluster Key Security Aspects to Consider in a Kubernetes Cluster Potential Vulnerabilities and Risks of an Insecure Cluster Tools and Methods for Assessing the Current Security of Your Kubernetes Cluster Designing a Zero Trust Architecture Key Components of a Zero Trust Architecture Segmenting the Cluster into Trust Zones and Implementing Segmentation Policies Authentication and Authorization Mechanisms to Enhance Security Best Practices Examples Strengthening Communication Security Using Encryption to Protect Communications Between Nodes Solutions for Data Encryption in Transit and at Rest Benefits of Using Certificates and Regular Key Rotation Monitoring and Detecting Suspicious Behaviors Tools for Monitoring and Detecting Anomalous Behaviors Using Logs and Metrics to Detect Intrusion Attempts and Malicious Activities Best Practices for Establishing Proactive Monitoring Conclusion Understanding the Basic Principles of Zero Trust The concept of Zero Trust is a revolutionary approach to cybersecurity that stands apart from traditional methods based on implicit trust. Unlike conventional security models that typically grant extensive privileges to internal users and devices, Zero Trust adopts a more rigorous and cautious approach. Defining the Concept of Zero Trust and Its Core Principles At the heart of Zero Trust lies the essential idea that any interaction between users, devices, and resources must be explicitly and continuously verified, regardless of the user’s or device’s location. In other words, no entity is presumed to be safe by default. To access resources, the user or device must be authenticated, authorized, and regularly reevaluated throughout the session. Zero Trust is based on the following core principles: Micro-segmentation: Networks and systems are divided into smaller, distinct segments. Each segment is treated as an isolated security perimeter, thus limiting the attack surface in case of a breach. Principle of Least Privilege: Users and devices are granted only the access privileges necessary to perform their specific tasks. Any additional access is restricted to minimize risks. Multi-Factor Authentication (MFA): MFA requires users to provide multiple identification elements, such as a password, token, or biometric fingerprint, to prove their identity. Continuous Monitoring: Activities of users, devices, and resources are continuously monitored in real-time to detect any suspicious or abnormal behavior. Why Implicit Trust Should No Longer Be Granted in a Modern IT Environment In the past, traditional security approaches often operated on an “implicit trust” model. This meant that once a user or device was authenticated and given network access, they were generally free to browse and interact with various resources without constant reevaluation. However, with the rapid evolution of cyber threats and the rise of sophisticated cyber attacks, this blind trust has proven its limitations. Attacks such as massive data breaches, ransomware, and phishing attacks have demonstrated that cybercriminals can bypass traditional defenses by exploiting weaknesses in implicit trust. Once an attacker has infiltrated the network, they can move laterally and access sensitive resources, potentially causing significant harm and compromising data confidentiality. The Benefits of Adopting a Zero Trust Architecture Adopting such an architecture offers several significant security advantages: Reduced Compromise Risks: By challenging implicit trust and requiring continuous authentication, Zero Trust limits the attack surface and reduces the risks of compromising the Kubernetes cluster. Protection of Sensitive Resources: By micro-segmenting the network and applying the principle of least privilege, critical resources in the cluster are better protected against unauthorized access. Early Threat Detection: Continuous monitoring and detection of suspicious activities enable a swift response to potential threats, minimizing the impact of incidents. Improved Compliance: By strengthening Kubernetes security through a Zero Trust architecture, organizations can better meet regulatory compliance requirements and avoid costly penalties. Assessing the Security of Your Existing Kubernetes Cluster Before implementing a Zero Trust architecture, it is essential to assess the security of your existing Kubernetes cluster. This step helps identify potential vulnerabilities and risks associated with an insecure cluster, providing a starting point to enhance the overall security of your environment. Key Security Aspects to Consider in a Kubernetes Cluster In a Kubernetes cluster, several security aspects must be considered to ensure the protection of resources and sensitive data. Here are the key points to evaluate: Access Controls: Verify authorization policies and roles granted to users, services, and applications to ensure they have only the necessary permissions for their tasks. Authentication: Examine authentication mechanisms in place to ensure that only legitimate users and services can access the cluster. Container Security: Ensure that container images used in the cluster come from trusted sources and are free from known vulnerabilities. Node Isolation: Evaluate network and node isolation policies to minimize unauthorized interactions between pods and nodes. Encryption: Check for the implementation of data encryption in transit and at rest to protect sensitive information. Secrets Management: Review methods used to manage secrets, such as API keys and credentials, to avoid accidental exposures. Monitoring and Logging: Ensure appropriate monitoring and logging mechanisms are in place to detect suspicious activities and facilitate investigation in case of incidents. Potential Vulnerabilities and Risks of an Insecure Cluster An insecure Kubernetes cluster can pose serious vulnerabilities and risks to the entire system. Here are some examples of possible consequences: Unauthorized Access: Poorly configured access policies may allow unauthorized users or services to interact with sensitive resources, exposing data to unauthorized access. Privilege Escalation: Vulnerabilities in role and permission management could allow an attacker to escalate to a higher privilege level, compromising the cluster’s integrity. Container Vulnerabilities: The use of insecure or outdated container images may expose the cluster to known exploits, resulting in security flaws and potential attacks. Denial-of-Service (DDoS) Attacks: A misconfigured cluster may be vulnerable to DDoS attacks, leading to resource overload and disruption of application functionality. Information Leakage: Lack of encryption or poor secrets management can lead to sensitive information leakage, compromising data confidentiality. Tools and Methods for Assessing the Current Security of Your Kubernetes Cluster To evaluate the security of your Kubernetes cluster, consider the following tools and methods: kube-bench: An open-source tool that checks the security configurations of your Kubernetes cluster against best security practices (https://github.com/aquasecurity/kube-bench). kube-hunter: Another open-source tool that performs penetration testing on your cluster to identify potential vulnerabilities (https://github.com/aquasecurity/kube-hunter). Manual Inspection: Conduct manual audits of access policies, roles, permissions, and network configurations to identify potential weaknesses. Log and Metric Analysis: Monitor cluster logs and metrics to detect suspicious activities or abnormal behavior. Designing a Zero Trust Architecture Once you have assessed the security of your existing cluster, it’s time to design a Zero Trust architecture to enhance the security of your environment. A Zero Trust architecture for Kubernetes is based on several concepts, such as segmenting the cluster into trust zones, advanced authentication and authorization mechanisms, and rigorous security policies. Key Components of a Zero Trust Architecture Micro-segmentation: Micro-segmentation involves dividing the cluster into smaller, isolated segments called “trust zones.” Each zone represents a distinct security perimeter where access and interactions are strictly controlled. Multi-Factor Authentication (MFA): MFA requires users and services to provide multiple identification elements to prove their identity. This typically includes combinations of passwords, tokens, biometric fingerprints, or other authentication factors. Role-Based Access Control (RBAC): RBAC defines specific roles for users and services, granting appropriate access privileges based on their responsibilities. This restricts resource access only to authorized users (The Definitive Guide to Role-Based Access Control). Data Encryption: Encryption is used to protect data in transit and at rest. It ensures that even in case of a security breach, data remains unintelligible to unauthorized individuals. Continuous Monitoring: Real-time monitoring of cluster activities helps detect suspicious behaviors and intrusion attempts. Segmenting the Cluster into Trust Zones and Implementing Segmentation Policies Segmenting the Kubernetes cluster into trust zones is a crucial step in strengthening security. Trust zones can be defined based on the sensitivity of resources and applications. For example, you can have one zone for highly sensitive resources, another for public applications, and another for internal applications. For each trust zone, define strict segmentation policies using Network Policies. These policies determine which entities are allowed to communicate with each other and with what level of access. You can restrict communications between certain zones and specify specific rules to allow or deny connections. Service Mesh solutions like Istio and Linkerd provide functionality that’s nearly akin to Network Policies, coupled with other features such as encrypting traffic between Pods, load balancing, rate limiting, and more. CyclonusCyclonus is a fuzz testing tool that explores hundreds of Network Policy configuration scenarios and assesses the compliance of your Container Network Interface (CNI) provider. Authentication and Authorization Mechanisms to Enhance Security Certificate-Based Authentication: Instead of relying solely on passwords, certificate-based authentication uses X.509 certificates to verify the identity of users and services. This enhances security by eliminating the need to store passwords and making identity impersonation more difficult. Two-Factor Authentication (2FA): By adding an additional layer of security to authentication, such as a token or code generated by an application, 2FA significantly reduces the risk of unauthorized access. Integration with Identity Providers (IdP): Integrating Kubernetes with external identity providers, such as Active Directory or LDAP, allows centralized management of identities and access. Role-Based Access Control (RBAC): Use RBAC to assign specific roles to each user and service based on their responsibilities. This establishes granular control over access. Best Practices Examples Limit Default Access: Do not grant default permissions to all resources. Users and services should require explicit authentication and authorization to access resources. Regular Key and Certificate Rotation: Ensure regular rotation of keys and certificates to minimize risks in case of compromise. Follow the Principle of Least Privilege: Grant only the necessary privileges to each entity, reducing the risks of unauthorized access or misuse. Establish Strong Security Policies: Define strict security policies and apply them to all trust zones to ensure uniform cluster protection. Monitor Logs and Metrics: Set up continuous monitoring of cluster activities to detect suspicious behaviors and respond promptly to incidents. Strengthening Communication Security Communication security within a Kubernetes cluster is essential to protect sensitive data and exchanges between nodes. Encryption plays a fundamental role in enhancing security by safeguarding data in transit and at rest. In this section, we will explore how to use encryption to secure communications between cluster nodes, present solutions for data encryption in transit and at rest, and discuss the benefits of using certificates and regular key rotation. Using Encryption to Protect Communications Between Nodes Encrypting communications between cluster nodes ensures that all exchanged data becomes unintelligible to unauthorized individuals. This means that even if an attacker manages to intercept network traffic, they will not be able to access sensitive information without the appropriate decryption key. Encryption is achieved using cryptographic protocols such as TLS (Transport Layer Security), which secures network connections. When two nodes communicate with each other, the data traveling through the network is automatically encrypted and decrypted at connection points, ensuring the confidentiality and integrity of exchanges. Solutions for Data Encryption in Transit and at Rest Encryption in Transit: TLS is widely used to encrypt data in transit. It secures communications over the network and is crucial in a Kubernetes environment, where nodes and services interact constantly. TLS uses digital certificates to authenticate nodes and establish secure connections through encrypted channels, preventing attackers from intercepting and reading data in transit. Encryption of Data at Rest: Data at rest encryption, including the protection of data stored in persistent volumes, databases, and other storage mediums within the cluster, is vital. Kubernetes offers features for encrypting data at rest by utilizing the Kubernetes Secrets functionality and integrating with encrypted storage solutions, such as HashiCorp Vault. Benefits of Using Certificates and Regular Key Rotation Using Certificates: Certificates play a crucial role in encrypting communications. They enable the authentication of nodes and services, ensuring that only legitimate entities can communicate within the cluster. Digital certificates are based on a Public Key Infrastructure (PKI) and ensure the integrity of communications by verifying the identity of the involved parties (mutualTLS or mTLS). Regular Key Rotation: Regular key rotation is a recommended security practice to minimize risks in case of key compromise. By regularly changing the encryption keys used to secure communications, the exposure time in case of key loss or theft is reduced. This also helps prevent attacks based on outdated keys. Enhancing Protection of Sensitive Data: By combining the use of certificates for authentication and regular key rotation for confidentiality, communications within the Kubernetes cluster benefit from an additional layer of protection. Sensitive data remains secure and inaccessible to malicious actors, ensuring the confidentiality and integrity of exchanged information. Regulatory Compliance: By implementing robust security practices such as using certificates and regular key rotation, businesses can better meet regulatory compliance requirements. These practices demonstrate a commitment to protecting sensitive data and ensuring communication security. Monitoring and Detecting Suspicious Behaviors Monitoring and detecting suspicious behaviors within a Kubernetes cluster are key elements to ensure the security and availability of applications. By monitoring real-time cluster activities, it is possible to detect intrusion attempts and malicious activities before they cause significant damage. Tools for Monitoring and Detecting Anomalous Behaviors Prometheus: Prometheus is an open-source monitoring and metrics collection system specifically designed for Kubernetes. It allows you to collect, store, and process metrics related to the cluster, nodes, and applications’ performance. Prometheus also offers a simple graphical interface to visualize metrics and facilitate real-time cluster performance analysis (https://prometheus.io/). Grafana: Grafana is an open-source data visualization and metrics analysis platform. By using Grafana in conjunction with Prometheus, you can create customized dashboards to monitor and analyze critical metrics of your Kubernetes cluster (https://grafana.com/). Falco: Falco is an open-source intrusion detection tool specifically designed for Kubernetes. It monitors real-time system activities and detects anomalous behaviors based on user-defined rules. Falco can be configured to send alerts upon detecting malicious activities (https://falco.org/). Sysdig: Sysdig is a monitoring and security solution for Kubernetes environments. It provides in-depth visibility into the cluster, including metrics, events, and system activities. Sysdig also offers intrusion detection and performance analysis capabilities (https://sysdig.com/). Using Logs and Metrics to Detect Intrusion Attempts and Malicious Activities Logs: Kubernetes cluster logs record events and activities occurring in the system. By monitoring logs, it is possible to detect suspicious behaviors, such as unauthorized access attempts, connection errors, or abnormal activities. For example, repeated failed login attempts could indicate a brute-force attack. Metrics: Metrics provide information about the performance and status of the Kubernetes cluster. By monitoring metrics, you can identify bottlenecks, overloads, or spikes in activity that could be related to attacks or malicious behaviors. For example, a sudden and unusual increase in network traffic could indicate an ongoing DDoS attack. Best Practices for Establishing Proactive Monitoring Set Alert Thresholds: Configure alert thresholds for logs and metrics to be notified as soon as abnormal activities are detected. This enables a rapid response in case of an incident. Use Event Correlation: Utilize event correlation tools to holistically analyze logs and metrics, enabling the detection of patterns and behaviors that may not be apparent through individual data analysis (an article describing some of these tools https://geekflare.com/best-aiops-platforms/) Schedule Regular Audits: Conduct regular audits of monitoring and logs to identify potential trends and anomalies. This helps to spot new threats or weaknesses in the system. Involve Security Teams: Involve security teams in the proactive monitoring of the Kubernetes cluster. Collaboration between DevOps and security teams is essential for a swift and coordinated response to security incidents. Establish Incident Response Procedures: Define clear incident response procedures to react promptly in case of detecting malicious activities. Well-designed incident response plans help minimize damage and quickly restore system integrity. Conclusion In this article, we explored the concept of Zero Trust in cybersecurity and its application in the Kubernetes environment. We emphasized the importance of adopting a Zero Trust approach to strengthen the security of a Kubernetes cluster and protect sensitive applications and data from cyberattacks. By applying the principles of Zero Trust, you can reduce the risk of compromise, safeguard sensitive data, and ensure regulatory compliance. Security becomes an ongoing process rather than a one-time measure, providing a stronger defense against ever-evolving cyber threats. Finally, let’s not forget the importance of staying informed about the latest developments in cybersecurity. Stay up-to-date with new threats, best practices, and emerging security tools to ensure optimal protection of your Cloud and Kubernetes environments. Security is an ongoing process, and by remaining vigilant, you will be better prepared to anticipate and counter future cybersecurity threats."
    } ,
  
    {
      "title"    : "AWS CloudFormation: Infrastructure-as-Code by Amazon",
      "url"      : "/en/aws-cloudformation-infrastructure-as-code/",
      "img"      : "PHBpY3R1cmU+CiAgPHNvdXJjZSBzcmNzZXQ9Ii9hc3NldHMvaW1nL2F3cy1jbG91ZGZvcm1hdGlvbi1pbmZyYXN0cnVjdHVyZS1hcy1jb2RlLWRvY3VtZW50YXRpb24tMzAwLTk2MThhZjc1NC5hdmlmIDEuMHgsIC9hc3NldHMvaW1nL2F3cy1jbG91ZGZvcm1hdGlvbi1pbmZyYXN0cnVjdHVyZS1hcy1jb2RlLWRvY3VtZW50YXRpb24tNjAwLTk2MThhZjc1NC5hdmlmIDIuMHgiIHR5cGU9ImltYWdlL2F2aWYiPgogIDxzb3VyY2Ugc3Jjc2V0PSIvYXNzZXRzL2ltZy9hd3MtY2xvdWRmb3JtYXRpb24taW5mcmFzdHJ1Y3R1cmUtYXMtY29kZS1kb2N1bWVudGF0aW9uLTMwMC1mOGY5MWVkNDUuanBnIDEuMHgsIC9hc3NldHMvaW1nL2F3cy1jbG91ZGZvcm1hdGlvbi1pbmZyYXN0cnVjdHVyZS1hcy1jb2RlLWRvY3VtZW50YXRpb24tNjAwLWY4ZjkxZWQ0NS5qcGcgMi4weCIgdHlwZT0iaW1hZ2UvanBlZyI+CiAgPGltZyBzcmM9Ii9hc3NldHMvaW1nL2F3cy1jbG91ZGZvcm1hdGlvbi1pbmZyYXN0cnVjdHVyZS1hcy1jb2RlLWRvY3VtZW50YXRpb24tMzAwLWY4ZjkxZWQ0NS5qcGciIGFsdD0iQVdTIENsb3VkRm9ybWF0aW9uOiBJbmZyYXN0cnVjdHVyZS1hcy1Db2RlIGJ5IEFtYXpvbiI+CjwvcGljdHVyZT4K",
      "isencoded" : "y",
      "tags"     : "AWS, CloudFormation, Infrastructure-as-Code, Stack, StackSet, Resource, Parameter, Mapping, Output, Condition, Rollback, Drift, Template, Automation, Cost",
      "content"  : "CloudFormation is a powerful Infrastructure-as-Code (IaC) tool provided by AWS. It allows you to describe and manage your AWS infrastructure as code, offering an automated and reproducible approach to creating and managing your cloud resources. In this article, our goal is to introduce you to the fundamental principles and key concepts of AWS CloudFormation. You will learn how to use CloudFormation to deploy and manage your AWS resources effectively and consistently. Benefits of CloudFormation 1. Simplified Management with Infrastructure-as-Code 2. Cost Control 3. Increased Productivity 4. Efficient Code Management CloudFormation Operation 1. Using Templates 2. Managing Updates and Deletions 3. CloudFormation Designer Template Syntax Resources Parameters Mappings Outputs Conditions Using Intrinsic Functions Fn::Ref Fn::GetAtt Fn::FindInMap Fn::Join Rollback Management Using Stacks Stack Definition ChangeSets Nested Stacks Using StackSets Introduction to StackSets Creating, Updating, and Deleting StackSets Advantages of StackSets Drift Management with CloudFormation Drift Detection and its Benefits Drift Detection Process for a Stack Cost Estimation CloudFormation Pricing Model Using the AWS Pricing Calculator Conclusion Benefits of CloudFormation The Infrastructure-as-Code (IaC) approach offered by CloudFormation provides several significant benefits. Here are the key advantages you can gain from using CloudFormation to deploy and manage your AWS infrastructure: 1. Simplified Management with Infrastructure-as-Code By adopting the IaC approach with CloudFormation, you can describe your infrastructure using a programming language, typically in JSON or YAML format. This allows you to manage your infrastructure consistently, reproducibly, and version-controlled. You can store your configuration files in a version control system like Git, track changes to your infrastructure, and validate them during code reviews. Infrastructure-as-Code helps you maintain control over your infrastructure and follow configuration management best practices. 2. Cost Control CloudFormation offers built-in features that allow you to estimate the costs of your infrastructure before even deploying it. By using CloudFormation configuration files, you can evaluate the costs of the required AWS resources for your infrastructure. This feature helps you plan and optimize your expenses by ensuring you have the necessary resources while avoiding unnecessary costs. Furthermore, CloudFormation facilitates cost management by allowing you to delete and recreate your infrastructure as needed. For example, you can schedule the deletion of your environment during periods of inactivity to save costs and automatically recreate it when needed. 3. Increased Productivity CloudFormation’s declarative approach allows you to describe your infrastructure without worrying about the details of resource orchestration. CloudFormation takes care of ordering and creating all the resources in the correct order in your infrastructure. This saves you time and enhances productivity by avoiding tedious manual tasks. Additionally, CloudFormation provides a clear overview of the current state of your infrastructure at any given time. You can easily visualize and understand your entire infrastructure using CloudFormation configuration files. This simplifies collaboration between development teams, operations, and compliance officers. 4. Efficient Code Management With CloudFormation, you can structure your infrastructure code using the principle of Separation of Concerns by using stacks. Stacks allow you to isolate and reuse specific parts of your infrastructure code. This facilitates code management, resource reuse, and adoption of software development best practices such as modularity and maintainability. CloudFormation Operation CloudFormation operates using templates that describe your AWS infrastructure. A template is a file in either JSON or YAML format that contains all the necessary resources, parameters, mappings, conditions, and outputs to create and manage your infrastructure. 1. Using Templates CloudFormation templates serve as a blueprint for your infrastructure. They describe the resources you want to create, such as EC2 instances, databases, queues, IAM roles, and more. You can specify the properties of each resource, such as the instance type, security policies, permissions, and more. Templates allow you to define the order in which resources should be created, ensuring that dependencies between resources are properly managed. You can also use intrinsic functions to perform advanced operations, such as retrieving values from other resources or parameters. 2. Managing Updates and Deletions CloudFormation simplifies the management of updates and deletions of resources. When you make changes to your template, CloudFormation analyzes the differences between the current configuration and the desired configuration. It then makes the necessary modifications to update your infrastructure accordingly. This allows you to keep your infrastructure up to date while minimizing service interruptions. Furthermore, CloudFormation also handles the management of resource deletions. When you delete a resource or a stack, CloudFormation ensures that all associated resources are also deleted. This ensures clean and complete management of your infrastructure. 3. CloudFormation Designer CloudFormation Designer is a graphical interface that allows you to design and visualize your CloudFormation templates. With CloudFormation Designer, you can visually create and edit your templates using a graphical representation of resources. This makes it easier to design your infrastructure and understand its structure. CloudFormation Designer provides drag-and-drop functionality to add resources, visual connections to represent dependencies, as well as validation features to check the validity of your template. You can then export your template for use with CloudFormation. Elastic Beanstalk and CodeStar make use of CloudFormation in the background. Template Syntax A CloudFormation template consists of different sections that define the resources, parameters, mappings, outputs, and conditions of your infrastructure. Each section has a specific role in creating and configuring your AWS environment. ReferenceTemplate files comprise multiple parts. You can find the complete syntax references at the following link: AWS Template Reference. Here’s an overview of the main sections of a CloudFormation template: Resources The Resources section contains the list of resources you want to create in your infrastructure. Each resource is defined as an object with its specific properties. Each resource must have a Type attribute that specifies the type of AWS resource to create. For example, you can define EC2 instances, security groups, databases, and more. Here’s an example syntax for creating an EC2 instance and its security group: Resources: MyEC2Instance: Type: AWS::EC2::Instance Properties: ImageId: ami-12345678 InstanceType: t2.micro SecurityGroupIds: - sg-abcdefgh Yaml Syntax by `Resources` TypeDepending on the specific resource, there are different syntax forms. You can find detailed information for each Resources type at the following link: AWS resource and property types reference. Parameters The Parameters section allows you to define customizable parameters for your template. Parameters enable you to provide values when deploying your infrastructure, allowing you to customize configurations based on your needs. You can specify the expected data type for each parameter, as well as optional constraints. Here’s an example syntax for defining two parameters, InstanceType and KeyName: Parameters: InstanceType: Type: String Default: t2.micro Description: EC2 instance type KeyName: Type: AWS::EC2::KeyPair::KeyName Description: EC2 key pair name ConstraintDescription: Must be the name of an existing EC2 key pair. Yaml Mappings The Mappings section allows you to define mappings between keys and values. Mappings can be used to simplify the configuration of your template and enable customization based on different criteria, such as region or environment. You can use the Fn::FindInMap intrinsic function to retrieve the corresponding values. Here’s an example syntax for defining a mapping between EC2 instance types and their architectures: Mappings: InstanceTypeToArch: t2.micro: Arch: HVM64 t2.small: Arch: HVM64 Yaml Outputs The Outputs section allows you to define values that you want to make available after the creation of your infrastructure. You can specify outputs to capture important information, such as resource identifiers, IP addresses, and more. These outputs can be used by other resources or displayed at the end of the deployment. Here’s an example syntax for defining two outputs, InstanceId and PublicIP: Outputs: InstanceId: Description: ID of the created EC2 instance Value: !Ref MyEC2Instance PublicIP: Description: Public IP address of the created EC2 instance Value: !GetAtt MyEC2Instance.PublicIp Yaml Conditions The Conditions section allows you to define logical conditions to control the creation of certain resources or the configuration of their properties. You can use intrinsic functions such as Fn::And, Fn::Equals, Fn::If, Fn::Not, Fn::Or to evaluate conditions and make decisions based on their results. Here’s an example syntax for defining a condition based on the EC2 instance type: Conditions: IsMicroInstance: !Equals [!Ref InstanceType, t2.micro] Yaml These sections form the building blocks for creating CloudFormation templates. You can combine and customize them based on your specific needs. By using these sections, you can describe and organize your AWS infrastructure in a declarative and reproducible manner. Note that the provided examples use YAML syntax, but you can also use JSON to define your CloudFormation templates. Using Intrinsic Functions CloudFormation offers a variety of intrinsic functions that allow you to perform advanced operations in your templates. These functions help you assign dynamic values to resource properties, reference other resources, or perform string manipulations. Here are some commonly used intrinsic functions: Fn::Ref The Fn::Ref intrinsic function allows you to reference parameters or resources in your template. When you use Fn::Ref with a parameter, it returns the value specified during deployment. When used with a resource, it typically returns a value that you can use to refer to that resource. Here’s an example of using Fn::Ref to retrieve the value of a parameter named InstanceType: InstanceType: !Ref InstanceType Yaml Fn::GetAtt The Fn::GetAtt intrinsic function allows you to retrieve the value of a specific attribute of a resource. You can use Fn::GetAtt to obtain information about a created resource, such as its ID, IP address, and more. Here’s an example of using Fn::GetAtt to get the public IP address of an EC2 instance named MyEC2Instance: PublicIP: !GetAtt MyEC2Instance.PublicIp Yaml Fn::FindInMap The Fn::FindInMap intrinsic function allows you to retrieve a value corresponding to keys in a mapping structure. You can use this function to simplify the retrieval of values based on specific criteria, such as region or instance type. Here’s an example of using Fn::FindInMap to get the architecture corresponding to an EC2 instance type: Arch: !FindInMap [InstanceTypeToArch, !Ref InstanceType, Arch] Yaml Fn::Join The Fn::Join intrinsic function allows you to concatenate a list of values into a single value, separated by a specified delimiter. This is useful when you want to combine multiple values into a single string. Here’s an example of using Fn::Join to concatenate two values, separated by a comma: CombinedValue: !Join [&quot;,&quot;, [Value1, Value2]] Yaml These intrinsic functions are just a few examples among many others available in CloudFormation. They allow you to perform advanced operations and add flexibility to your templates. It’s important to consult the Intrinsic Function Reference for a complete list of intrinsic functions and to understand their specific functionalities. Rollback Management When performing updates to CloudFormation templates, a Rollback mechanism is available to ensure the integrity of your infrastructure in case of errors during the update process. The main idea behind Rollback is to restore the previous state of your infrastructure by undoing the changes made during the erroneous update. This minimizes the impact of errors and maintains the functionality of your environment. When you initiate a template update, CloudFormation goes through the following steps: Change Analysis: CloudFormation analyzes the differences between your current state and the new template and determines the actions needed to reach the desired state. Change Execution: CloudFormation applies the necessary changes to update your infrastructure based on the new template. This may involve creating, modifying, or deleting resources. Stability Verification: After the changes are applied, CloudFormation verifies the stability of your environment by running tests. If errors are detected during this phase, the Rollback mechanism is triggered. If an error is detected, Rollback is triggered, and CloudFormation performs the following actions: Deletion of Newly Created Resources: Any resources created during the erroneous update are deleted to revert to the previous state. Restoration of Modified Resources: Resources modified during the erroneous update are restored to their previous state. Reversal of Deletions: If any resources were deleted during the update, Rollback reverses those deletions to restore them. The Rollback mechanism provides an additional layer of safety when managing your CloudFormation templates. In case of issues, you can be assured that your infrastructure will be restored to a previous functional state. It is important to note that Rollback is not enabled by default; you need to specify this option when initiating the update of your template if you want to benefit from this functionality. Using Stacks In AWS CloudFormation, Stacks play a crucial role in managing your AWS resources. A Stack is a collection of AWS resources that can be managed as a single unit. It allows you to create, update, or delete a set of resources in a consistent and reproducible manner. Here are some key points about using Stacks: Stack Definition A Stack is created from a CloudFormation template, which describes the set of resources and their configuration. Using Stacks provides several advantages: Simplified Resource Management: By grouping related resources into a Stack, you can manage and process them as a logical unit. This makes it easier to create, update, and delete your resources in a single process. Reproducibility and Consistency: The CloudFormation template ensures that your infrastructure is created consistently with each Stack deployment. This avoids manual errors and ensures reproducibility of your environment. Simplified Tracking and Management: With CloudFormation, you have a clear and comprehensive view of the state of your infrastructure. You can easily visualize the resources, their dependencies, and the changes made over time. ChangeSets When making changes to an existing Stack, it is important to understand the impact of those changes before applying them. ChangeSets are a mechanism provided by AWS CloudFormation to preview the proposed modifications before implementing them. A ChangeSet represents the changes that will be applied to the Stack. It allows you to review the planned actions, identify resource additions, updates, or deletions, and validate the changes before applying them. ChangeSets provide an additional layer of safety and enable you to make informed decisions about managing your resources. Nested Stacks Nested Stacks are Stacks created within the context of another parent Stack. They allow you to reuse common components or break down your infrastructure into smaller, self-contained logical modules. By using Nested Stacks, you can simplify the management of your templates by organizing them in a hierarchical manner. Each Nested Stack can have its own template and manage its specific resources while being integrated into the parent Stack. This facilitates the management and maintenance of your resources by dividing them into smaller, reusable components. You can create dedicated templates for each component and reference them in your parent Stack. Nested Stacks also promote modularity, flexibility, and reusability of your infrastructures across different contexts. Using StackSets In AWS CloudFormation, StackSets provide a powerful solution for deploying Stacks across multiple AWS accounts and regions. A StackSet allows you to create, update, and delete Stacks in a set of target accounts using a single CloudFormation template. Here are the key points to know about using StackSets: Introduction to StackSets A StackSet allows you to centrally manage the deployment of a set of Stacks across different AWS accounts and regions. You specify a CloudFormation template along with the parameters and capabilities required by that template. When creating a StackSet, you have the flexibility to deploy Stacks to one or multiple target AWS accounts and choose the regions where you want to deploy them. This offers you the flexibility to deploy and manage your infrastructure consistently across different contexts. StackSets are particularly useful in multi-account environments , where you can deploy a standardized architecture in each account automatically and under control. Creating, Updating, and Deleting StackSets Creating a StackSet involves specifying the CloudFormation template you want to use to create the Stacks, along with the target accounts where you want to deploy those Stacks. You can also specify the AWS regions where you want to deploy them. Once you have created a StackSet, you can update it in various ways: Modifying Parameters: You can modify existing parameters in the template or add new resources to perform specific changes to your infrastructure. Replacing the Template: You have the option to replace the existing template with a different template to make more substantial changes to your StackSet. Adding Accounts and Regions: You can expand your StackSet by adding additional target accounts or deploying Stacks to new AWS regions. Deleting a StackSet will result in the deletion of the associated Stacks in the specified target accounts. You can choose to delete only specific Stacks or delete all the Stacks within the StackSet. It is important to note that you can only delete a StackSet once all Stack instances have been deleted. Advantages of StackSets StackSets offer numerous benefits in deploying and managing infrastructure at scale: Consistent Deployment: StackSets ensure consistent deployment of the same Stack resources in each specified target account and region. This ensures the consistency and compliance of your infrastructure. Centralized Management: By using a StackSet, you can centrally manage Stacks across multiple accounts and regions, making it easier to manage and coordinate your large-scale infrastructure. Ease of Updating: StackSets make it easy to update Stacks across your entire environment by applying changes to the CloudFormation template. This allows you to keep your infrastructure up to date efficiently and securely. StackSets provide a flexible and scalable approach for deploying and managing your Stacks in distributed environments. They allow you to maintain consistency and control while simplifying the management and updates of your infrastructure. Drift Management with CloudFormation Drift detection is a critical feature of AWS CloudFormation that allows you to check if a Stack has deviated from its initial configuration defined in the CloudFormation template. Drift occurs when unauthorized changes are made to the Stack’s resources outside of CloudFormation, leading to variances from the intended state. Here are the key points to understand about drift management with CloudFormation: Drift Detection and its Benefits Drift detection helps you maintain the integrity and compliance of your infrastructure by identifying discrepancies between the actual state of resources and the expected state defined in the CloudFormation template. It helps you detect unauthorized modifications, incorrect configurations, or deleted resources. Drift detection is useful in the following scenarios: Compliance Maintenance: You can verify if the resources in your Stack comply with security rules, best practices, or internal policies of your organization. Drift detection helps you identify discrepancies and take appropriate actions to correct them. Tracking Unauthorized Changes: You can detect changes made to resources outside of CloudFormation, which helps prevent human errors or unauthorized actions that could result in undesirable configurations. Drift Detection Process for a Stack To detect drift in a Stack within AWS CloudFormation, you can follow these steps: Go to the AWS CloudFormation console at https://console.aws.amazon.com/cloudformation . In the list of Stacks, select the Stack for which you want to perform drift detection. In the Stack detail panel, choose Stack Actions and then select Detect drift. AWS CloudFormation will then compare the current state of the Stack’s resources with the state defined in the CloudFormation template. You will receive a detailed report indicating the resources that have drifted, i.e., the resources that have been modified, deleted, or whose configuration differs from what is specified in the template. By analyzing the drift report, you can take appropriate actions to restore the desired state of your infrastructure. This may involve updating the CloudFormation template, recreating certain resources, or other corrective actions. Drift detection with CloudFormation is a valuable way to maintain the integrity of your infrastructure and ensure compliance with the expected configurations. By regularly following the drift detection process, you can ensure that your infrastructure remains aligned with your requirements and prevent undesirable configurations. Cost Estimation When using AWS CloudFormation, it is important to understand the associated pricing model and estimate the costs related to your Stacks. Here are the key points to consider regarding cost estimation in CloudFormation: CloudFormation Pricing Model AWS CloudFormation does not charge any additional fees for using its service. You are only billed for the AWS resources you deploy using CloudFormation. This means that the costs associated with using CloudFormation are the same as if you had created and managed those resources manually. The fees for resources deployed by CloudFormation are calculated based on the standard AWS rates for each specific service. For example, if you deploy Amazon EC2 instances, you will be charged according to the usual EC2 rates. Using the AWS Pricing Calculator To estimate the costs associated with your CloudFormation Stacks, AWS provides a convenient tool called the AWS Pricing Calculator. This tool allows you to obtain a detailed cost estimation based on your specific configuration. Here’s how to estimate the costs of a CloudFormation Stack using the AWS Pricing Calculator: On the Stack creation wizard review page, in the Template section, click the Estimate costs link. The link will redirect you to the AWS Pricing Calculator in a new browser tab. In the Pricing Calculator, you can review and adjust the settings for each resource deployed by your Stack, as well as data transfer settings, AWS regions, etc., to obtain an accurate estimate. Once you have configured all the necessary parameters, the AWS Pricing Calculator will provide you with a detailed monthly cost estimation associated with your CloudFormation Stack. It is important to note that the AWS Pricing Calculator allows you to account for different factors and options specific to your infrastructure. This helps you evaluate the potential costs before deploying your CloudFormation Stacks and plan accordingly. By using the AWS Pricing Calculator, you can gain a clear understanding of the costs associated with your CloudFormation Stacks and make informed decisions regarding budgeting and cost optimization. Conclusion In this article, we explored the fundamental principles and key concepts of AWS CloudFormation as an Infrastructure-as-Code tool. Here is a summary of the key points we covered: CloudFormation allows you to describe your infrastructure as code and automatically create it based on your configuration. The benefits of CloudFormation stem from the Infrastructure-as-Code approach, which provides efficient infrastructure management. You can estimate the cost of your infrastructure from CloudFormation code, facilitating budget planning. CloudFormation’s declarative code and resource management within Stacks enable improved productivity and maintaining a clear state of your infrastructure. CloudFormation’s templates are in JSON or YAML format and include multiple sections such as Resources, Parameters, Mappings, Outputs, and Conditions. You can use intrinsic functions like Ref, GetAtt, FindInMap, Join, etc., to manipulate values and attributes in the templates. CloudFormation supports the Rollback mechanism during template updates, allowing you to revert to a previous state in case of errors. Stacks are units for managing resources in CloudFormation, enabling the creation, updating, and deletion of resource sets consistently. ChangeSets allow you to preview proposed changes before applying them to your existing Stacks. Nested Stacks allow you to reuse common components by declaring them in dedicated templates. StackSets enable you to deploy Stacks across multiple AWS accounts and regions from a single CloudFormation template. Drift detection helps you identify discrepancies between the current state of resources and the expected state defined in the CloudFormation template. You can estimate the costs associated with your CloudFormation Stacks using the AWS Pricing Calculator. We highly encourage the use of AWS CloudFormation for efficient infrastructure management. By embracing the Infrastructure-as-Code approach and leveraging the powerful capabilities of CloudFormation, you can automate the creation, updating, and deletion of your infrastructure, improving efficiency, consistency, and compliance in your environments. Remember to consult the official AWS CloudFormation documentation to deepen your knowledge and make the most of this powerful tool. With CloudFormation, you can build and manage your infrastructures more effectively, allowing you to focus on innovation and achieving your business objectives."
    } ,
  
    {
      "title"    : "Git Branch Management: Exploring the Realm of Possibilities",
      "url"      : "/en/git-workflows/",
      "img"      : "PHBpY3R1cmU+CiAgPHNvdXJjZSBzcmNzZXQ9Ii9hc3NldHMvaW1nL2dpdC13b3JrZmxvd3MtMzAwLWQ1NzY2MGI5MS5hdmlmIDEuMHgsIC9hc3NldHMvaW1nL2dpdC13b3JrZmxvd3MtNjAwLWQ1NzY2MGI5MS5hdmlmIDIuMHgiIHR5cGU9ImltYWdlL2F2aWYiPgogIDxzb3VyY2Ugc3Jjc2V0PSIvYXNzZXRzL2ltZy9naXQtd29ya2Zsb3dzLTMwMC1mMzgyYTk5MWMuanBnIDEuMHgsIC9hc3NldHMvaW1nL2dpdC13b3JrZmxvd3MtNjAwLWYzODJhOTkxYy5qcGcgMi4weCIgdHlwZT0iaW1hZ2UvanBlZyI+CiAgPGltZyBzcmM9Ii9hc3NldHMvaW1nL2dpdC13b3JrZmxvd3MtMzAwLWYzODJhOTkxYy5qcGciIGFsdD0iR2l0IEJyYW5jaCBNYW5hZ2VtZW50OiBFeHBsb3JpbmcgdGhlIFJlYWxtIG9mIFBvc3NpYmlsaXRpZXMiPgo8L3BpY3R1cmU+Cg==",
      "isencoded" : "y",
      "tags"     : "Git, Git-Flow, GitHub-Flow, Trunk-Based-Flow, Forking-Flow, Workflow, DevOps",
      "content"  : "When it comes to collaborative software development, effective Git branch management is crucial for your team’s efficiency and productivity. In this article, we will explore various Git branch management models to help you optimize your workflow. Git branch management is a practice that involves using separate branches to develop features, fix bugs, and manage versions in your project. It enables multiple developers to work simultaneously on features or fixes without interfering with each other. By using branches, you can isolate changes, test and validate features before merging them into the main branch. Understanding the different Git branch management models is essential in choosing the method that best suits your team and project. Each model has its own advantages, with approaches tailored to different team sizes, project complexities, and deployment goals. Let’s explore how to choose the appropriate branch management model for your project among the existing options. What is Git Branch Management? Basic Models Basic Workflow Centralized Workflow Feature-Oriented Models Feature Branch Workflow Git Flow Platform-Oriented Models GitHub Flow GitLab Flow Models for External Contributions Forking Workflow Gated Branch Workflow Models for Specific Strategies Trunk Based Flow Release Branch Workflow Environment Branch Workflow Models for Specific Approaches Component-based Workflow Maintenance Branch Workflow Task Branch Workflow Conclusion What is Git Branch Management? Git branch management is an essential practice in collaborative software development. It involves using separate branches in the Git version control system to organize and manage changes in a project. When multiple developers work simultaneously on a project, it is crucial to be able to work in isolation on features or fixes without affecting the work of others. This is where Git branches come into play. A Git branch is essentially an independent line of development that allows developers to work on specific changes without disrupting the main branch. The primary role of Git branch management is to facilitate seamless collaboration and control over changes. Each developer can create their own branch to work on a specific task, whether it’s developing a new feature, fixing a bug, or making enhancements. Branches help separate the work in progress from stable and operational code, which typically resides in the main branch. Once developers have completed their modifications on their respective branches, they can merge them into the main branch. This merging can be done after code review and appropriate testing to ensure the quality and stability of the changes. Git branch management offers several benefits in collaborative software development. It enables better isolation of changes, facilitates testing and validation, simplifies tracking of modifications, and streamlines the resolution of potential conflicts. Moreover, it allows developers to work in parallel on distinct features, accelerating development and enhancing team efficiency. Basic Models Git branch management offers several fundamental models that are used in various software development contexts. In this section, we will examine two basic Git branch management models: the Basic Workflow and the Centralized Workflow. Basic Workflow Basic Workflow Key Features: In this model, changes are made directly on the master or main branch. There are no separate branches for features or bug fixes. This model is straightforward to understand and implement, requiring no complex workflows or additional specific branches. Objectives: It is simple and generally suitable for small projects or when a single person is working on the project. It simplifies the branch management process by avoiding the proliferation of specific branches for each feature or task. Limitations: This model is not ideal for collaborative projects involving multiple developers, as direct changes to the main branch can lead to frequent conflicts and make it challenging to track changes. This model can become less efficient when multiple features are developed in parallel or when frequent merge conflicts occur. It may not be suitable for projects requiring finer version control granularity or isolation of changes. Centralized Workflow Centralized Workflow Key Features: In this traditional model, developers collaborate directly on the main branch, such as master or main. They may use branches, such as branch, to isolate features or bug fixes, but the collaboration primarily happens on the main branch. Changes are then integrated into the main branch through the merging process. This model is straightforward to understand and implement, requiring no complex workflows or additional specific branches. Objectives: The Centralized Workflow is often used in projects where simplicity and direct collaboration are favored. It facilitates collaboration by allowing developers to work directly on the main branch. It simplifies the branch management process by avoiding the proliferation of specific branches for each feature or task. Limitations: This model can become challenging to manage in projects involving multiple developers working simultaneously on different features or fixes. Merge conflicts may occur more frequently, and it can be more challenging to track specific changes made by each developer. It may not be suitable for projects requiring finer version control granularity or isolation of changes. It is important to note that these basic Git branch management models are simple and may be suitable for certain projects, but they do not address the needs of more complex collaborations or large-scale projects commonly found. Feature-Oriented Models In this chapter, we will explore two Git branch management models that are specifically designed to organize and integrate features into your project. Feature Branch Workflow Feature Branch Workflow Key Features: The Feature Branch Workflow is a Git branch management model where features are developed on separate branches before being merged into the main branch. Each feature is developed in its own branch, which facilitates isolation, testing, and reviews. Objectives: Enable parallel development of independent features. Encourage feature isolation for easier testing and validation. Facilitate collaboration by allowing developers to work on specific branches without affecting the main branch. Limitations: Managing numerous feature branches can become complex and require effective coordination. Merge conflicts may occur when merging feature branches into the main branch. Git Flow Git Flow (simplified) Key Features: Git Flow is a comprehensive Git branch management model that offers specific branches for features, stable releases, and continuous integration. It utilizes multiple branches, including: The master or main branch, which holds the current production-ready release. The develop branch, which contains a copy of the master branch along with any changes made since the last release. The feature branches, which are created from develop and used to implement specific features. The release branch, which contains all changes to be included in a future release and undergoes extensive testing in dedicated environments. The hotfix branch, which is created from master or main for urgent bug fixes. Objectives: Facilitate parallel development of features on separate branches. Provide a clear and structured process for creating stable releases and managing emergency fixes. Encourage smooth continuous integration and quality testing before release. Limitations: This model may seem complex for small projects or smaller teams. Managing the different branches may require a solid understanding of the model and effective coordination. Using the Feature Branch Workflow, developers can work on separate branches to develop features without disrupting the main branch. This facilitates collaboration, testing, and reviews before the final merge. On the other hand, Git Flow offers a more structured and comprehensive approach to branch management by providing specific branches for each phase of a project’s lifecycle. It provides a clear framework for development, validation, stable releases, and emergency fixes. Platform-Oriented Models In this section, we will explore two Git branch management models that are specifically designed for collaborative development platforms: GitHub Flow and GitLab Flow. These models leverage the features of their respective platforms to facilitate collaboration, code reviews, and continuous integration. GitHub Flow GitHub Flow Key Features: GitHub Flow is a simple Git branch management model based on the use of pull requests and continuous integration. Development takes place on separate branches, feature, before being merged into the main branch, master or main. Objectives: Foster collaboration among team members through the use of pull requests to share and review changes. Promote continuous integration by running automated tests on feature branches before merging. Simplify the branch management process by focusing on key steps: branch creation, development, pull request, and merging. Limitations: This model may lack structure for projects requiring more formal version management or deeper validation controls. Handling merge issues and conflicts can become more complex when multiple pull requests are pending. GitLab Flow GitLab Flow Key Features: GitLab Flow is a Git branch management model offered by GitLab, which utilizes merge requests and incorporates additional testing steps into the workflow. It provides specific features such as deployment environments (e.g., production deployment branch) and merge approvals. Objectives: Facilitate collaboration and review of changes through the use of merge requests to discuss and merge feature branches. Incorporate additional testing steps, such as integration or performance testing, before merging changes. Enable more advanced deployment management with the ability to create specific environments for testing and validation. Limitations: Introducing additional testing steps can extend the development cycle and require adequate testing infrastructure. Initial configuration and environment setup may require additional effort. These two models, GitHub Flow and GitLab Flow, leverage the features of their respective platforms to facilitate collaboration, code reviews, and continuous integration. The terms “pull request” and “merge request” are often used interchangeably and refer to similar mechanisms in version control platforms like GitHub and GitLab. Functionally, pull requests and merge requests offer similar features, including the ability to review changes, provide feedback, engage in discussions, and perform testing before merging changes into the main branch. Models for External Contributions In this section, we will explore two Git branch management models specifically tailored for external contributions: the Forking Workflow and the Gated Branch Workflow. These models are commonly used in open-source projects to facilitate external developer contributions and ensure code quality before merging. Forking Workflow Forking Workflow Key Features: The Forking Workflow is a widely used Git branch management model in open-source projects. It involves creating forks (independent copies) of the main repository, where external contributors make their changes. The changes are then submitted as pull requests to be merged into the main repository. Objectives: Foster open collaboration and external contribution by allowing contributors to work on their own forks independently from the main repository. Facilitate the review of changes through the use of pull requests, enabling project maintainers to discuss and assess contributions before integration. Limitations: This model can result in an increase in forks and branches, requiring effective management and coordination. Setting up and coordinating pull requests can take time and require additional effort from project maintainers. Gated Branch Workflow Key Features: The Gated Branch Workflow is a Git model that incorporates control branches for performing validations before merging changes. These control branches, also known as validation branches, serve as checkpoints where changes are tested and validated before being merged into the main branch. Objectives: Ensure a high level of quality and stability by performing validations and tests before merging changes into the main branch. Allow development teams to work in parallel on feature branches while maintaining a structured and controlled workflow. Limitations: Adding control branches can introduce complexity to the branch management process, requiring proper coordination and configuration. Potential delays due to validations and tests can impact the speed of feature delivery. These two models, the Forking Workflow and the Gated Branch Workflow, offer specific approaches to managing external contributions or ensuring code quality before merging. Models for Specific Strategies In this section, we will explore three Git branch management models tailored to specific strategies: Trunk Based Flow, Release Branch Workflow, and Environment Branch Workflow. These models offer unique approaches to organizing workflow and addressing specific development and deployment needs. Trunk Based Flow Trunk Based Flow Key Features: Trunk Based Flow is a Git branch management model focused on a stable main branch and short-lived feature branches. Developers work directly on the main branch, and new features are developed on separate branches before being quickly merged into the main branch. Objectives: Promote continuous integration by regularly merging features into the main branch. Reduce complexity by limiting the number of branches and fostering a linear and direct workflow. Limitations: This model may not be suitable for projects requiring stricter feature isolation or more granular version control. Merge conflicts can occur if multiple developers modify the same parts of the code simultaneously. Release Branch Workflow Key Features: Release Branch Workflow is a Git branch management model that uses version branches for long-term bug fixes. Developers work on separate feature branches and then merge completed features into a dedicated version branch to prepare them for stable release. Objectives: Facilitate management of stable versions by isolating fixes and changes related to a specific version. Enable thorough testing and bug fixes on the version branch before release. Limitations: This model may require careful coordination and management of different version branches, especially for projects with multiple versions under maintenance. Updates or urgent fixes may require additional operations to apply them to all relevant version branches. Environment Branch Workflow Key Features: Environment Branch Workflow is a Git branch management model that uses specific branches for each deployment environment. Developers work on separate feature branches and merge them into dedicated environment branches for testing, validation, and deployment to specific environments. Objectives: Facilitate deployment and management of different environment configurations. Enable environment-specific testing before deployment. Limitations: This model can result in multiple branches specific to each environment, requiring rigorous coordination and management. It may be more complex to set up and maintain for projects with numerous deployment environments. These three models, Trunk Based Flow, Release Branch Workflow, and Environment Branch Workflow, offer specific approaches to address specific development and deployment strategies. Models for Specific Approaches In this section, we will discuss three Git branch management models tailored to specific approaches: Component-based Workflow, Maintenance Branch Workflow, and Task Branch Workflow. These models offer unique strategies for organizing development, maintenance, and individual task management. Component-based Workflow Key Features: Component-based Workflow is a Git model that uses component branches to organize modular development. Each component is developed on its own branch, allowing for independent tracking and management of each component. Objectives: Facilitate the development and maintenance of individual components within a project. Enable a modular approach where developers can focus on specific parts of the project. Limitations: This model may require rigorous coordination and management of component branches, especially for projects with many interdependent components. It may be less suitable for projects with strong dependencies between components or when changes require adjustments in multiple component branches. Maintenance Branch Workflow Key Features: Maintenance Branch Workflow is a Git model that uses maintenance branches for long-term bug fixes. Fixes are developed on separate maintenance branches and merged into the main branch as well as the appropriate version branches. Objectives: Ensure effective management of long-term bug fixes and updates. Enable rigorous testing and validation of fixes before integration into the main branch and version branches. Limitations: This model may require careful coordination and management of maintenance branches, especially for projects with multiple concurrently maintained versions. Urgent fixes may require additional operations to apply them to all relevant maintenance branches. Task Branch Workflow Key Features: Task Branch Workflow is a Git model that uses task branches to manage individual user stories or specific tasks. Each task is developed on its own separate branch before being merged into the main branch. Objectives: Enable granular management of individual tasks and features. Facilitate tracking and review of specific changes related to each task. Limitations: This model can result in multiple task branches, requiring effective management and coordination. Dependencies between tasks may require adjustments or conflict resolution when merging task branches into the main branch. These three models, Component-based Workflow, Maintenance Branch Workflow, and Task Branch Workflow, offer specific approaches to managing modular development, long-term bug fix maintenance, and individual task management. Conclusion We have explored various Git branch management models, each offering specific benefits based on the development and deployment needs of a project. These models include both popular approaches and more specific frameworks. It is important to note that these models are not mutually exclusive, and it is possible to adapt and combine them to meet the specific needs of your project. When selecting a model, consider the project goals, team size, preferred workflow, and quality and deployment requirements. The summary table of Git branches presented below provides an overview of branches to consider based on desired feature types. This can serve as a reference to understand the purpose and utility of each branch in the context of your project. Branch Desired Features main main version, in production develop stable releases feature/ development of new features release/ preparation of stable releases hotfix/ emergency bug fixes environment/ deployment in specific environments component/ development of specific components maintenance/ long-term bug fix maintenance task/ management of individual tasks Ultimately, the choice of Git branch management model will depend on the specificities of your project and the preferences of your team. The key is to find an approach that promotes consistency, code quality, and productivity throughout the development cycle."
    } ,
  
    {
      "title"    : "Accelerate: How to Measure the Performance of your DevOps with the Right KPIs",
      "url"      : "/en/accelerate-devops-kpi/",
      "img"      : "PHBpY3R1cmU+CiAgPHNvdXJjZSBzcmNzZXQ9Ii9hc3NldHMvaW1nL2FjY2VsZXJhdGUta3BpLTMwMC0yYWYxNzAyMjEuYXZpZiAxLjB4LCAvYXNzZXRzL2ltZy9hY2NlbGVyYXRlLWtwaS02MDAtMmFmMTcwMjIxLmF2aWYgMi4weCIgdHlwZT0iaW1hZ2UvYXZpZiI+CiAgPHNvdXJjZSBzcmNzZXQ9Ii9hc3NldHMvaW1nL2FjY2VsZXJhdGUta3BpLTMwMC03NmM1MGFjZjIuanBnIDEuMHgsIC9hc3NldHMvaW1nL2FjY2VsZXJhdGUta3BpLTYwMC03NmM1MGFjZjIuanBnIDIuMHgiIHR5cGU9ImltYWdlL2pwZWciPgogIDxpbWcgc3JjPSIvYXNzZXRzL2ltZy9hY2NlbGVyYXRlLWtwaS0zMDAtNzZjNTBhY2YyLmpwZyIgYWx0PSJBY2NlbGVyYXRlOiBIb3cgdG8gTWVhc3VyZSB0aGUgUGVyZm9ybWFuY2Ugb2YgeW91ciBEZXZPcHMgd2l0aCB0aGUgUmlnaHQgS1BJcyI+CjwvcGljdHVyZT4K",
      "isencoded" : "y",
      "tags"     : "Accelerate, KPI, DLT, DF, MTTR, CSR, DevOps",
      "content"  : "Modern software development is characterized by an increasing demand for speed, agility, and quality. To meet these requirements, many organizations are adopting DevOps, an approach that promotes collaboration and integration between development and operations teams. DevOps aims to break down silos between these teams and foster a culture of continuous collaboration throughout the software development lifecycle. This approach improves the efficiency, speed, and reliability of software deployments. In this article, we will explore the book “Accelerate: The Science of Lean Software and DevOps” written by Nicole Forsgren, Jez Humble, and Gene Kim. This book highlights the importance of measuring performance in a DevOps environment. Indeed, to assess the effectiveness and efficiency of their DevOps practices, organizations need to use the right key performance indicators (KPIs). KPIs help quantify the outcomes and identify areas for improvement. The book “Accelerate” offers a scientific approach to measuring DevOps performance and provides relevant KPIs for evaluating outcomes. These KPIs enable DevOps teams to make informed decisions and guide their efforts towards continuous improvement. In the following sections of this article, we will explore the key KPIs recommended by “Accelerate” for measuring DevOps performance. We will discuss how these KPIs can be used to assess and enhance the effectiveness of your DevOps practices. Understanding the Objectives of DevOps Introduction to KPIs in DevOps The Role of KPIs in Performance Measurement Advantages of KPIs in Evaluating DevOps Processes Recommended KPIs by “Accelerate” Deployment Lead Time Deployment Frequency Mean Time to Recover (MTTR) Change Success Rate Measuring KPIs and Interpreting Results Using KPIs to Improve DevOps Performance Conclusion Understanding the Objectives of DevOps DevOps aims to achieve several key objectives in software development. These objectives revolve around improving collaboration, accelerating software deployment, and enhancing overall product quality. It is crucial to measure the performance of DevOps to assess the achievement of these objectives. Improving Collaboration: DevOps seeks to foster collaboration between development teams, operations teams, and other stakeholders involved in the software development process. By aligning goals, processes, and responsibilities, DevOps encourages transparent communication and effective cooperation. Performance measurement helps verify if collaboration is improving over time and enables taking actions to strengthen cooperation. Accelerating Software Deployment: One of the primary goals of DevOps is to reduce the time required to deploy new features and software updates. By automating deployment processes and utilizing continuous integration and delivery pipelines, DevOps accelerates development and deployment cycles. Performance measurement allows quantifying deployment speed and identifying bottlenecks that slow down the process. Enhancing Quality: DevOps emphasizes software quality by integrating practices such as automated testing, code reviews, and quality control processes. The goal is to reduce errors, defects, and downtime while improving software stability and reliability. Performance measurement evaluates the impact of these practices on overall software quality and enables continuous improvement efforts. In summary, performance measurement is critical to assess whether the objectives of DevOps are being met. It quantifies progress in collaboration, accelerating software deployment, and improving quality. By monitoring and analyzing the appropriate KPIs, DevOps teams can make data-driven decisions and implement corrective actions to continuously enhance their practices. Introduction to KPIs in DevOps Key Performance Indicators (KPIs) play a crucial role in measuring the performance of DevOps. They enable the assessment of efficiency and effectiveness in DevOps processes. The Role of KPIs in Performance Measurement KPIs are quantifiable measures used to evaluate the performance of a process, team, or organization. In the context of DevOps, KPIs quantify achieved results and compare them against set objectives. They provide a clear and factual view of overall DevOps performance, enabling informed decision-making. KPIs can be defined to measure various aspects of the DevOps process, such as deployment speed, software quality, testing efficiency, system availability, and more. By selecting appropriate KPIs, DevOps teams gain valuable insights into strengths and areas for improvement in their practices. Advantages of KPIs in Evaluating DevOps Processes KPIs offer several benefits for objectively and measurably assessing DevOps performance: Goal achievement measurement: KPIs quantify progress towards set objectives. They help verify if DevOps performance aligns with expectations and facilitate corrective actions if needed. Identification of improvement areas: By measuring KPIs, areas in need of improvement become evident. KPIs help identify bottlenecks, inefficiencies, or quality issues, allowing efforts to be focused on critical aspects. Data-driven decision-making: KPIs provide tangible and factual data on DevOps performance. This enables decision-makers to make informed choices based on quantifiable indicators rather than conjecture or subjective impressions. Monitoring continuous improvement: KPIs measure the impact of improvement initiatives implemented within DevOps. By regularly monitoring KPIs, teams can evaluate the effectiveness of these initiatives and adjust their approach to achieve better results. In conclusion, the use of KPIs in DevOps offers numerous advantages by providing an objective and measurable assessment of performance. KPIs enable the evaluation of DevOps process efficiency and effectiveness, identification of improvement areas, and guidance for continuous practice enhancement. Recommended KPIs by “Accelerate” The book “Accelerate: The Science of Lean Software and DevOps” proposes several essential Key Performance Indicators (KPIs) to assess DevOps performance. Each of these KPIs provides valuable insights into different aspects of the DevOps process, enabling measurement and performance improvement. Here are the main KPIs recommended by “Accelerate”: Deployment Lead Time Deployment Lead Time represents the duration required to deploy a software change from conception to production. This KPI measures the effectiveness of the software deployment process. A short deployment lead time indicates increased agility and speed in deploying changes. Measurement of deployment lead time can be done by tracking each step of the deployment process and recording the time taken at each step. Best practices to improve deployment lead time include test automation, continuous integration and delivery (CI/CD), and implementing fast and efficient approval processes. Deployment Frequency Deployment Frequency measures the number of times software changes are deployed into production within a given period. This KPI is closely related to DevOps agility and flexibility. A high deployment frequency indicates the ability to rapidly deliver new features or fixes. Measurement of deployment frequency can be done by recording the number of deployments performed on a regular basis, such as weekly or monthly. Tools like version control systems and deployment automation can help collect this data. Mean Time to Recover (MTTR) Mean Time to Recover (MTTR) measures the time required to restore service after an incident. This KPI evaluates the DevOps’ ability to effectively manage incidents and reduce downtime. A short MTTR indicates better resilience and the ability to quickly restore services. To measure MTTR, it is necessary to record the moment an incident occurs, as well as the time required to resolve it and restore the service. To improve MTTR, it is recommended to invest in automation of recovery processes, well-defined business continuity plans, and foster a learning culture to avoid similar incidents in the future. Change Success Rate Change Success Rate measures the proportion of changes deployed into production without causing incidents or undesirable problems. This KPI assesses the quality of deployments and the DevOps’ ability to minimize risks associated with changes. A high change success rate indicates better stability and a reduction in errors. To calculate this rate, it is necessary to record the number of successfully deployed changes compared to the total number of changes deployed within a given period. To improve the change success rate, it is recommended to invest in rigorous testing, code reviews, pre-production environments, and effective configuration management. By using these KPIs recommended by “Accelerate,” DevOps teams can measure the performance of their process and identify areas for improvement. These KPIs provide concrete indicators to evaluate deployment effectiveness, deployment frequency, incident resilience, and change quality. Based on these measurements, teams can implement targeted corrective actions to continuously improve their DevOps practices. Measuring KPIs and Interpreting Results Measuring KPIs in the context of DevOps requires the collection of accurate and reliable data. Here are some practical tips for collecting the data needed to measure KPIs: Automation of data collection processes: It is crucial to automate data collection as much as possible to ensure accuracy and consistency. Use tools and systems that allow for automatic collection of data related to KPIs. For example, using monitoring and tracking tools can help gather data on system performance, while continuous integration can collect information on deployments and testing. Integration of KPIs into existing tools and processes: Integrate the collection of data related to KPIs into the tools and processes already used by DevOps teams. For example, use issue and request tracking systems to record deployment lead times, use version control tools to measure deployment frequency, or utilize monitoring tools to collect data on system availability and performance. Set thresholds and goals for KPIs: It is important to establish clear thresholds and goals for each KPI to be able to assess performance meaningfully. Thresholds help determine what is considered acceptable or not, while goals serve as a benchmark for measuring progress and continuous improvement. Once the data is collected, it is necessary to interpret the results of the KPIs to identify possible improvements. Here are some tips for interpreting KPI results: Comparison with goals and past benchmarks: Compare the results of KPIs with the set goals and previous measurements. Identify significant deviations and analyze possible causes for these variances. This will highlight areas that require particular attention. Analysis of trends and correlations: Analyze the trends of KPIs over a longer period to identify patterns and correlations. For example, if you observe an increase in deployment lead time simultaneously with an increase in the number of deployments, it may indicate performance or quality issues to address. Engage stakeholders: Involve various stakeholders, including DevOps team members, developers, operations, product managers, etc. Discuss the results of KPIs, share observations, and improvement suggestions. This will foster common understanding and collaboration to implement necessary improvements. Establish an improvement plan: Based on the analysis of KPI results, establish an improvement plan by identifying specific actions to undertake to enhance performance. Set realistic goals and define measures to track progress in implementing the improvements. By following these tips, DevOps teams can effectively measure KPIs and utilize the data to identify areas for improvement, implement corrective actions, and achieve continuous progress in their DevOps process. Using KPIs to Improve DevOps Performance The results of KPIs provide valuable insights into the performance of the DevOps process. They help identify strengths and weaknesses in the system and guide improvement efforts. Here’s how to use KPI results to enhance DevOps performance: Identify areas for improvement: By analyzing KPI results, identify areas where improvements are needed. For example, if deployment time is lengthy or the success rate of changes is low, it may indicate issues in the deployment process. If the Mean Time to Recovery (MTTR) is high, it could signal gaps in incident management. Identify KPIs with results outside the set goals and focus on these specific areas. Set improvement goals: Based on KPI results and the identification of areas to enhance, establish specific and measurable improvement goals. For instance, you could set a goal to reduce deployment time by 50% within six months, improve the success rate of changes to 95%, or reduce MTTR to less than 1 hour. These goals should be realistic and aligned with the needs of the business and users. Examples of strategies to improve DevOps performance based on KPIs: Automation of deployment process: If deployment time is lengthy, invest in automating the deployment process. Use Continuous Integration (CI) and Continuous Delivery (CD) tools to automate testing, validations, and deployment of changes. This will reduce deployment delays and enhance process efficiency. Implementation of rigorous testing: If the success rate of changes is low, consider strengthening testing practices. Introduce unit testing, functional testing, performance testing, and security testing to identify potential issues before production deployment. Thorough testing ensures better deployment quality and reduces the risk of incidents. Training and collaboration: If KPI results reveal issues related to collaboration or communication among teams, invest in training and coaching team members. Encourage a culture of collaboration and accountability to address issues more effectively and promptly. Improvement of incident recovery processes: If the Mean Time to Recovery (MTTR) is high, assess and optimize incident management processes. Identify steps that slow down the recovery process and explore ways to improve efficiency, such as automating recovery tasks, enhancing communication procedures, and establishing clear documentation. By using KPI results as a guide, DevOps teams can implement specific strategies to enhance their process performance. By focusing on areas for improvement and setting measurable goals, they can progress towards a more efficient, agile, and high-performing DevOps approach. Conclusion In this article, we have explored the importance of measuring DevOps performance using appropriate KPIs. We have presented the recommended KPIs from the book “Accelerate” by Nicole Forsgren, Jez Humble, and Gene Kim, which are concrete metrics to assess the effectiveness and efficiency of the DevOps process. It is important to note that measuring DevOps performance is not limited to the KPIs presented here. There are other areas to explore, such as user satisfaction, operational costs, security, and compliance. Each organization can adapt the KPIs according to its specific needs and objectives."
    } ,
  
    {
      "title"    : "Principles of Software Development: SOLID, DRY, KISS, and more",
      "url"      : "/en/solid-dry-kiss/",
      "img"      : "PHBpY3R1cmU+CiAgPHNvdXJjZSBzcmNzZXQ9Ii9hc3NldHMvaW1nL3NvbGlkLWRyeS1raXNzLTMwMC02ZmU2ZjMyZTUuYXZpZiAxLjB4LCAvYXNzZXRzL2ltZy9zb2xpZC1kcnkta2lzcy02MDAtNmZlNmYzMmU1LmF2aWYgMi4weCIgdHlwZT0iaW1hZ2UvYXZpZiI+CiAgPHNvdXJjZSBzcmNzZXQ9Ii9hc3NldHMvaW1nL3NvbGlkLWRyeS1raXNzLTMwMC1mNWE2MzBkYzAuanBnIDEuMHgsIC9hc3NldHMvaW1nL3NvbGlkLWRyeS1raXNzLTYwMC1mNWE2MzBkYzAuanBnIDIuMHgiIHR5cGU9ImltYWdlL2pwZWciPgogIDxpbWcgc3JjPSIvYXNzZXRzL2ltZy9zb2xpZC1kcnkta2lzcy0zMDAtZjVhNjMwZGMwLmpwZyIgYWx0PSJQcmluY2lwbGVzIG9mIFNvZnR3YXJlIERldmVsb3BtZW50OiBTT0xJRCwgRFJZLCBLSVNTLCBhbmQgbW9yZSI+CjwvcGljdHVyZT4K",
      "isencoded" : "y",
      "tags"     : "SOLID, DRY, KISS, Development, DevOps, YAGNI, CoC, LoD",
      "content"  : "In the field of software development, applying solid principles plays a crucial role in ensuring the quality, maintainability, and longevity of projects. These principles provide guidelines and best practices for designing and writing robust and efficient code. Among these principles, SOLID, DRY, and KISS hold a prominent position, but there are also other equally relevant principles. The purpose of this article is to delve into the details of the SOLID, DRY, and KISS principles, as well as other essential software development principles. We will examine how these principles can be applied in practice and the benefits they bring to development projects. By exploring these fundamental principles along with other relevant principles such as YAGNI, Convention over Configuration, Composition over Inheritance, and Law of Demeter, we will be able to develop high-quality, easily maintainable, and scalable code. Let’s discover how to apply them in our programming practices. SOLID Single Responsibility Principle (SRP) Open/Closed Principle (OCP) Liskov Substitution Principle (LSP) Interface Segregation Principle (ISP) Dependency Inversion Principle (DIP) DRY (Don’t Repeat Yourself) Reduction of Complexity Elimination of Duplicate Code Grouping by Functionality Code Reusability In Practice KISS (Keep It Simple, Stupid) Tips In Practice Other Important Principles YAGNI (You Ain’t Gonna Need It) Convention over Configuration (CoC) Composition over Inheritance Law of Demeter (LoD) Conclusion SOLID The SOLID principles are key concepts in software development. They promote the design of robust and scalable code. In this chapter, we will examine in detail the five SOLID principles and their respective advantages. Single Responsibility Principle (SRP) The Single Responsibility Principle (SRP) states that a class should have only one well-defined responsibility. In other words, a class should be responsible for only one task or one aspect of the system. This facilitates code understanding, maintenance, and reusability. For example, instead of having a class that handles both user authentication and notification sending, it is better to separate these responsibilities into two distinct classes. The benefits of applying SRP are numerous. First, it makes the code more modular, making it easier to make modifications and additions later on. Additionally, troubleshooting and issue resolution are simplified as each class focuses on a single responsibility. Finally, code reusability is promoted, as specialized classes can be used in different parts of the system. Let’s take the example of a library management application. By applying SRP, we can have a separate class for book management, another for users, and another for transactions. Each class will have its own responsibility, making the code clearer and more maintainable. Open/Closed Principle (OCP) The Open/Closed Principle (OCP) emphasizes designing code that is open for extension but closed for modification. In other words, when new features need to be added, it is better to extend the existing code rather than directly modifying it. The key advantage of applying OCP lies in its ability to make the code more flexible and extensible. By using mechanisms such as inheritance, polymorphism, and inversion of control, we can add new features without impacting the existing code. It also facilitates unit testing, as existing features are not altered when introducing new ones. For example, in a payment processing application, we can have a generic abstract class for payment methods, such as “PaymentMethod.” Each specific payment method (e.g., credit card, PayPal) can then be implemented by extending this abstract class while retaining the basic functionalities common to all payment methods. By following the OCP principle, the code remains stable and avoids regressions even when extended with new features. Liskov Substitution Principle (LSP) The Liskov Substitution Principle (LSP) highlights the importance of adhering to contracts when inheriting classes. Specifically, if a class B is a subclass of class A, then it should be able to be used as a replacement for A without affecting the system’s overall consistency. The main advantage of applying LSP is the ability to substitute objects of subclasses for objects of base classes without altering the overall behavior of the system. This promotes modularity and code reusability, as new subclasses can be added without disrupting existing parts of the system. For example, consider a hierarchy of classes for geometric shapes. If we have a base class “Shape” with specific subclasses such as “Circle” and “Rectangle,” LSP requires that instances of “Circle” and “Rectangle” can be used wherever an instance of “Shape” is expected without altering the expected behavior. By respecting LSP, we ensure consistency in the system and avoid surprises or unexpected behaviors when using inheritance. Interface Segregation Principle (ISP) The Interface Segregation Principle (ISP) advocates for defining specific interfaces for clients rather than having a monolithic interface. In other words, clients should not be forced to implement methods they don’t use. Applying ISP offers several benefits. Firstly, it makes interfaces clearer and more coherent as they only contain the necessary methods for a specific client. It also facilitates maintenance, as changes to an interface do not affect all clients but only those using the relevant methods. For example, in an e-commerce application, we can have a separate interface for online payment methods and another for offline payment methods. This way, classes handling online payments only implement the relevant methods for online payments, and vice versa. By respecting ISP, we create more concise interfaces tailored to the specific needs of clients, making our code more flexible and extensible. Dependency Inversion Principle (DIP) The Dependency Inversion Principle (DIP) encourages the use of abstract dependencies rather than relying on concrete classes. In other words, high-level modules should not depend directly on low-level modules but on common abstractions. Applying DIP brings several advantages. The first is modularity, as dependencies are defined on interfaces or abstract classes, making it easier to replace concrete implementations. The second is facilitating unit testing, as dependencies can be easily mocked or injected during tests. Finally, it enables reduced coupling between different modules, making the code more flexible and reusable. For example, instead of a high-level class directly depending on a low-level class, we can introduce an abstract interface between the two. This way, the high-level class depends on the interface rather than the concrete class, allowing for easier substitutions. By respecting DIP, we promote better separation of responsibilities and a more flexible and scalable design. DRY (Don’t Repeat Yourself) The DRY (Don’t Repeat Yourself) principle emphasizes the elimination of unnecessary code duplication in a software development project. According to this principle, each piece of knowledge or logic should have a single canonical representation within the system. Let’s explore the benefits offered by the DRY principle. Reduction of Complexity First and foremost, it reduces code complexity by avoiding unnecessary repetitions. This makes the code more readable, clear, and easier to understand for developers. Additionally, it simplifies code maintenance, as modifications and fixes only need to be made in one place rather than in multiple parts of the code. Finally, it promotes code reuse, as common functionalities or logics can be encapsulated into functions, classes, or modules that can be used in multiple places within the system. Elimination of Duplicate Code To avoid code duplication, there are several techniques that developers can apply. Firstly, extracting functions or methods allows grouping similar and repetitive code blocks into a reusable function. This way, the same code can be called in multiple places without needing to rewrite it. Grouping by Functionality Next, the use of classes and inheritance can help encapsulate common functionalities and reuse them in specific subclasses. This way, common functionalities can be defined once in a parent class and inherited in child classes. Code Reusability Finally, the use of libraries, modules, or frameworks can aid in reusing code that has already been written and tested by other developers, avoiding the need to reinvent the wheel. In Practice Let’s take a concrete example to illustrate the application of the DRY principle. Suppose we are developing a contact management application with features for adding, modifying, and deleting contacts. Instead of repeating the same data validation code in multiple places in the program, we can extract this validation logic into a separate function or utility class. This way, whenever we need to validate contact data, we simply call that function or utility class, avoiding code duplication. By applying the DRY principle, we reduce complexity, improve maintainability, and promote code reuse, leading to more efficient development and more robust systems. KISS (Keep It Simple, Stupid) The KISS (Keep It Simple, Stupid) principle emphasizes simplicity in code design and implementation. According to this principle, it’s better to maintain simple solutions rather than making them complex. Simplicity promotes understanding, maintenance, and problem-solving. Applying the KISS principle brings several advantages: Better Code Understanding: It facilitates code understanding for developers as simple solutions are clearer and more intuitive. Reduced Errors: It also reduces the risk of errors and bugs since simple solutions are easier to test and verify. More Scalable Code: Simplicity makes code more flexible and scalable as it’s easier to make modifications or add new features to simple code rather than complex code. Tips To maintain simplicity in code, it’s important to follow some practical tips. Firstly, avoid over-engineering and excessive abstractions. Look for simple and straightforward solutions that meet specific needs without adding unnecessary complexity. Also, avoid code repetition and duplication, in line with the DRY principle. By grouping common functionalities and avoiding redundancies, you keep the code clearer and more concise. Additionally, it’s important to keep variable, function, and class names clear and explicit. Well-chosen names facilitate code understanding and reduce the need for additional comments. Also, avoid premature optimizations and unnecessary complex features. Focus on solving specific problems and add additional features only when truly necessary. In Practice Let’s take a concrete example to illustrate the application of the KISS principle. Suppose we are developing a simple calculator program. Instead of creating a complex structure with sophisticated classes and interfaces, we can opt for a simple solution using direct functions or methods to perform basic operations such as addition, subtraction, multiplication, and division. This would make the code clearer, easier to understand, and easier to maintain. By applying the KISS principle, we prioritize simplicity and clarity in the code, which facilitates understanding, maintenance, and problem-solving, while promoting software flexibility and scalability. Other Important Principles The third part of this article highlights other important principles in software development, in addition to the SOLID, DRY, and KISS principles discussed earlier. These additional principles also contribute to improving code quality, maintainability, and scalability. By exploring these principles, we enhance our understanding of good development and software design practices. YAGNI (You Ain’t Gonna Need It) The YAGNI (You Ain’t Gonna Need It) principle emphasizes not implementing features or code that are not immediately necessary. According to this principle, it’s better to focus on essential features and avoid anticipating hypothetical future needs. Applying the YAGNI principle brings several advantages. Firstly, it reduces code complexity by avoiding the addition of unnecessary features. This makes the code clearer, lighter, and easier to maintain. Additionally, it saves time and resources by avoiding the development and testing of features that might never be used. Lastly, it promotes an iterative approach to development by focusing on the immediate needs of users and allowing the addition of additional features as they become genuinely necessary. To apply the YAGNI principle, it’s important to ask the question, “Do I really need it now?” before adding a new feature or developing additional code. Carefully evaluate the importance and urgency of the functionality and avoid preemptive additions based on uncertain assumptions. Prioritize essential features and focus on the real needs of users. Let’s take a concrete example to illustrate the application of the YAGNI principle. Suppose we are developing a task management application. Instead of implementing an advanced scheduling feature with customizable reminders right from the start, we could begin with a basic functionality of task creation and tracking. By focusing on essential features, we can quickly deliver an initial version of the application, gather user feedback, and iterate by adding additional features like advanced scheduling if it proves to be a genuine user demand. By applying the YAGNI principle, we avoid over-engineering, reduce complexity, and focus on the immediate needs of users, enabling more efficient development and better utilization of resources. Convention over Configuration (CoC) The Convention over Configuration (CoC) principle promotes the use of predetermined conventions rather than explicit configurations. By following these conventions, developers can reduce the amount of necessary configuration and automatically benefit from functionality, simplifying the development process and improving code readability. This principle is widely applied in many tools and frameworks, and developers often benefit from it without even realizing it. For example, the structure of a Java project with directories like src/main/java, src/main/resources, and src/test/java follows the CoC principle. By placing test files in the src/test/java directory, the tests are automatically executed when running the tests. Similarly, the “Test” suffix in JUnit file names also follows the Convention over Configuration principle. Applying the CoC principle also facilitates collaboration among team members as they share a common understanding of conventions and can focus on business logic rather than configuration details. Composition over Inheritance The Composition over Inheritance principle advocates for using class composition instead of inheritance to promote code reusability and avoid rigid dependencies between classes. According to this principle, it’s better to construct complex objects by combining simpler objects rather than creating a complex inheritance hierarchy. Applying the composition principle brings several advantages. Firstly, it allows greater flexibility in terms of code reuse. Instead of tightly binding a class to an inheritance hierarchy, composition enables the construction of objects by assembling reusable components. It also facilitates code modularity as components can be developed and tested independently before being combined to form more complex objects. Furthermore, applying composition reduces code complexity and avoids problems with deep and complex inheritance hierarchies. By avoiding excessive inheritance, the code becomes more readable, maintainable, and less prone to errors. Composition also allows focusing on relationships between objects rather than the details of internal implementation in a parent class. Let’s take a concrete example to illustrate the application of the composition principle. Suppose we are developing a file management system. Instead of creating a complex inheritance hierarchy with classes like “File,” “Folder,” and “Drive,” we can opt for a composition approach where each object has a list of simpler objects, such as “File” objects and “Folder” objects. This allows building flexible file structures and modular manipulation of objects, avoiding the constraints of inheritance. By applying the Composition over Inheritance principle, we promote code reusability, modularity, and object flexibility. This leads to clearer, more maintainable, and scalable code while avoiding issues related to complex inheritance hierarchies. Law of Demeter (LoD) The Law of Demeter (LoD), also known as the “Principle of Only Talking to Your Closest Friends,” is a software design principle that promotes decoupling and reducing dependencies between classes. According to this principle, a class should only interact with its immediate collaborators and not directly access members of objects it interacts with indirectly. Applying the LoD principle brings several advantages. Firstly, it promotes decoupling between classes, making the code more modular, flexible, and easier to maintain. By limiting direct interactions between classes, changes made to one class have minimal impact on other classes, facilitating code evolution and modification. Furthermore, applying the LoD improves the robustness of the code by reducing the cascading effects of changes. When a class depends only on its immediate collaborators, it becomes less sensitive to internal changes in the objects it interacts with indirectly. This helps reduce the risks of unintended side effects and facilitates the localization and correction of errors. Let’s take a concrete example to illustrate the application of the LoD. Suppose we have a “Client” class that interacts with a “Bank” class to perform financial transactions. Instead of directly accessing members of the “Bank” class such as bank accounts, the “Client” class can use methods provided by the “Bank” class that supply the necessary information. This way, the “Client” class depends only on the interface provided by the “Bank” class and doesn’t need to know the internal details of that class. By applying the LoD principle, we reduce dependencies between classes, improve code modularity and maintainability, and minimize the cascading effects of changes. This leads to more flexible, robust, and easily evolvable code. Conclusion The application of software development principles such as SOLID, DRY, KISS, CoC, Composition over Inheritance, and the Law of Demeter (LoD) is crucial to ensure high-quality software development. These principles are the result of years of experience and best practices shared by the developer community. Their use helps create robust, maintainable, scalable, and high-quality software. By adopting these principles, developers are able to build more flexible, reusable, and understandable software systems. Applying these principles promotes modularity, reduces complexity, facilitates collaboration among team members, and improves code maintainability. Additionally, it helps prevent common issues such as code duplication, excessive dependencies, and cascading effects. It is highly recommended for developers to further explore these principles and apply them appropriately in their projects. Each principle brings specific benefits and can be adapted based on the project’s needs and constraints. By understanding these principles and putting them into practice, developers can enhance their efficiency, productivity, and the quality of the software they create. It is also important to emphasize that these principles are not universal solutions. They should be applied judiciously, considering the context and specific requirements of the project. Developers must carefully evaluate each situation and find the right balance between applying these principles and other considerations such as performance, time constraints, and user needs."
    } ,
  
    {
      "title"    : "Beyond Code: How Craftsmanship Transforms Development and DevOps",
      "url"      : "/en/craftmanship/",
      "img"      : "PHBpY3R1cmU+CiAgPHNvdXJjZSBzcmNzZXQ9Ii9hc3NldHMvaW1nL2NyYWZ0bWFuc2hpcC0zMDAtOTZlMzhiMTI4LmF2aWYgMS4weCwgL2Fzc2V0cy9pbWcvY3JhZnRtYW5zaGlwLTYwMC05NmUzOGIxMjguYXZpZiAyLjB4IiB0eXBlPSJpbWFnZS9hdmlmIj4KICA8c291cmNlIHNyY3NldD0iL2Fzc2V0cy9pbWcvY3JhZnRtYW5zaGlwLTMwMC03ODgxMWRkYzMuanBnIDEuMHgsIC9hc3NldHMvaW1nL2NyYWZ0bWFuc2hpcC02MDAtNzg4MTFkZGMzLmpwZyAyLjB4IiB0eXBlPSJpbWFnZS9qcGVnIj4KICA8aW1nIHNyYz0iL2Fzc2V0cy9pbWcvY3JhZnRtYW5zaGlwLTMwMC03ODgxMWRkYzMuanBnIiBhbHQ9IkJleW9uZCBDb2RlOiBIb3cgQ3JhZnRzbWFuc2hpcCBUcmFuc2Zvcm1zIERldmVsb3BtZW50IGFuZCBEZXZPcHMiPgo8L3BpY3R1cmU+Cg==",
      "isencoded" : "y",
      "tags"     : "Craftsmanship, Development, TDD, Pair Programming, DevOps, Waterfall, Agile",
      "content"  : "Software development and DevOps are constantly evolving fields where quality, collaboration, and responsibility play a crucial role. It is in this context that Craftsmanship, or the craftsmanship of software development, emerges as a fundamental approach to creating robust and high-performing software solutions. Craftsmanship encompasses a set of principles that emphasize technical excellence, collaboration, and individual and collective responsibility. These principles aim to improve the quality of developed software, foster a culture of collaborative work, and encourage accountability from each team member. Technical excellence is a central pillar of Craftsmanship. This involves adopting rigorous development practices, mastering relevant programming languages, tools, and technologies, and continuously seeking to improve technical skills. By focusing on technical excellence, Craftsmanship aims to produce reliable, high-performing, and easily maintainable software. Collaboration is another essential facet of Craftsmanship. Working as a team, sharing knowledge and best practices, and promoting clear and open communication are crucial aspects for project success. Craftsmanship encourages the practice of pair programming, where two developers work together on the same piece of code, thus fostering knowledge sharing and continuous improvement. Responsibility, both individual and collective, is also a fundamental principle of Craftsmanship. Each team member is encouraged to take responsibility for the quality of the code they produce, the maintenance of the software, and the satisfaction of end users. Collective responsibility translates into the team’s commitment to work together to achieve project goals and make decisions that benefit the entire team and product. By adopting the principles of Craftsmanship, software development and DevOps teams can create superior quality software, improve productivity, and foster a culture of collaboration and responsibility. In this article, we will explore in more detail the specific practices of Craftsmanship as well as the key skills required to successfully implement it. Understanding Craftsmanship Craftsmanship Practices Key Skills of Craftsmanship Cultivating Craftsmanship in Teams Conclusion Understanding Craftsmanship Craftsmanship, or the craftsmanship of software development, is an approach that emphasizes quality, simplicity, and flexibility in software creation. It draws inspiration from traditional artisan trades, where craftsmen take pride in their work and strive to produce intricately crafted pieces of exceptional quality. Over time, Craftsmanship has evolved in the software industry to adapt to changing challenges and needs. It has become a response to the shortcomings of traditional approaches such as the Waterfall model, which emphasized rigid and linear planning and did not allow for easy adjustments during development. The fundamental values of Craftsmanship are built upon three essential pillars: quality, simplicity, and flexibility. Quality lies at the core of Craftsmanship, aiming to produce reliable, durable, and defect-free software. Practitioners of Craftsmanship focus on creating clean, well-structured, and well-tested code, employing practices like Test Driven Development (TDD) to ensure software quality throughout the development process. Simplicity is another key value of Craftsmanship. Rather than seeking complex and sophisticated solutions, software artisans prioritize simplicity, creating software that is easy to understand, maintain, and extend. They avoid unnecessary features and complexities, favoring a minimalist approach that results in more agile and adaptable software. Flexibility is also an essential value of Craftsmanship. Practitioners of Craftsmanship acknowledge that user needs and project requirements can evolve over time. Therefore, they aim to create flexible software capable of easily adapting to changes and market evolutions. This translates into modular architecture, well-structured code, and Agile development processes such as Continuous Integration (CI) and Continuous Deployment (CD). Compared to other development approaches such as the Waterfall model or Agile methodologies, Craftsmanship stands out with its commitment to quality, simplicity, and flexibility. While the Waterfall model emphasizes rigid planning and sequentiality, Craftsmanship promotes adaptability and responsiveness to change. In contrast to Agile methodologies, Craftsmanship underscores the importance of technical excellence and individual responsibility, emphasizing the creation of superior quality software. By understanding these fundamental principles of Craftsmanship, software development professionals can adopt a more holistic and quality-focused approach, leading to higher-performing, more maintainable, and user-oriented software solutions. Craftsmanship Practices Craftsmanship distinguishes itself through the adoption of practices that promote technical excellence, collaboration, and the delivery of quality software. Here are three key practices of Craftsmanship: Test-Driven Development (TDD) or How Tests Guide Software Development: Test-Driven Development (TDD) is a central practice of Craftsmanship that places tests at the core of the development process. With TDD, developers first write automated tests to define the expected behavior of the code and then implement the code to pass these tests successfully. This iterative approach ensures that each feature is rigorously tested and that the code meets the specifications. TDD promotes software quality by quickly identifying errors and ensuring code robustness throughout the development cycle. Pair Programming or Collaborating for Better Quality and Knowledge Sharing: Pair Programming is a practice of close collaboration between two developers working together on the same piece of code. One developer writes the code while the other observes, reflects, asks questions, and suggests improvements. By regularly switching roles, both developers benefit from a better understanding of the code, instant feedback, and continuous improvement of their skills. Pair Programming enhances code quality by detecting errors faster, sharing knowledge, and reinforcing collective responsibility within the team. Continuous Integration (CI) and Continuous Deployment (CD) or Automating Processes for Reliable Delivery: CI and CD are essential practices for ensuring reliable and regular software delivery. CI involves automating code integration processes by performing automated tests with every change to the source code. This allows for the rapid detection of compatibility, quality, and security issues, ensuring stable code. CD goes further by also automating the deployment of software to the production environment. This enables continuous delivery, reducing the risks of human errors and accelerating the development cycle. Continuous integration and continuous deployment facilitate collaboration, improve software quality, and enhance the development team’s agility. By adopting these Craftsmanship practices, software development teams can improve code quality and security, encourage collaboration, and ensure reliable software delivery. These practices contribute to strengthening the Craftsmanship culture and cultivating an approach focused on technical excellence and user satisfaction. Key Skills of Craftsmanship Craftsmanship demands software development professionals to possess a set of key skills to achieve technical excellence and foster collaboration within the team. Craftsmanship relies on three essential skills: Mastery of Programming Languages and Tools: Mastery of programming languages and tools is a fundamental aspect of Craftsmanship. Developers need to have an in-depth understanding of the programming languages used in their domain, as well as the frameworks, libraries, and associated tools. A solid understanding of these tools enables software artisans to choose the most appropriate solutions, code efficiently, optimize performance, and solve complex technical issues. Proficiency in development tools such as the version control system Git, integrated development environments (IDEs), and automation tools is also crucial to maximize productivity and code quality. Knowledge of Software Architecture and Design Patterns: Professionals practicing Craftsmanship must possess strong knowledge of software architecture and design patterns. A good understanding of design principles and architectural patterns allows for the creation of well-structured, scalable, and maintainable software. Proficiency in concepts like hexagonal architecture, proper usage of design patterns (creational, structural, behavioral), and functional programming are commonly employed in software development. This knowledge enables developers to make informed decisions when designing and organizing their software systems, promoting modular and flexible architecture. Ability to Work in a Team and Communicate Effectively: Craftsmanship emphasizes collaboration and collective responsibility. Therefore, it is essential for software artisans to have skills in teamwork and effective communication. This includes the ability to share ideas, listen to other team members, provide constructive feedback, and resolve conflicts collaboratively. Clear and open communication facilitates collaboration among developers, testers, project managers, and stakeholders, enabling a better understanding of project requirements and ensuring effective alignment. Willingness to work as a team also promotes knowledge sharing, mutual learning, and continuous improvement within the team. By developing and strengthening these key skills, software development professionals can embark on the path of Craftsmanship, ensuring technical excellence, robust architecture, and fruitful collaboration within their development team. Cultivating Craftsmanship in Teams Craftsmanship extends beyond individual skills and must also be cultivated within software development teams. Here are three key strategies to promote and develop Craftsmanship within a team: Encourage Continuous Learning and Knowledge Sharing: To foster Craftsmanship, it is essential to encourage continuous learning and knowledge sharing within the team. This can take the form of regular training sessions, technical conferences, online courses, or participation in developer communities. By providing learning opportunities, team members can stay up to date with new technologies, development best practices, and market trends. Knowledge sharing can occur through pair programming sessions, regular code reviews, or the creation of an internal knowledge base. The goal is to enable each team member to grow professionally and actively contribute to collective improvement. Create a Culture of Learning and Continuous Improvement: The culture of a team has a significant impact on the practice of Craftsmanship. It is important to create a culture focused on learning and continuous improvement. This can be achieved by encouraging experimentation, fostering open discussions about challenges and mistakes, and implementing regular retrospectives to assess team performance and identify improvement opportunities. It is also important to promote intellectual curiosity and innovation, allowing team members to explore new ideas and propose enhancements. By creating a culture that values learning and continuous improvement, Craftsmanship becomes a norm within the team. Recognize and Reward Craftsmanship Practices: To support Craftsmanship, it is essential to recognize and reward exemplary practices within the team. This can be done through formal or informal recognition systems, such as rewards, special mentions during team meetings, or professional growth opportunities. Recognizing individual efforts and achievements encourages team members to continue their Craftsmanship practices and strengthens the culture of technical excellence. By highlighting Craftsmanship practices, the team creates an environment where quality, collaboration, and accountability are valued and acknowledged. By implementing these strategies, software development teams can cultivate Craftsmanship and create a culture where technical excellence, collaboration, and continuous learning are essential values. This leads to higher-quality software, more effective teams, and increased end-user satisfaction. Conclusion Craftsmanship, with its principles of technical excellence, collaboration, and accountability, plays a crucial role in software development and DevOps. Throughout this article, we have explored the various facets of Craftsmanship, from its definition to identifying key skills and essential practices. In conclusion, it is essential to integrate the principles of Craftsmanship into our software development and DevOps practice. By doing so, we can aspire to deliver superior-quality products, build high-performing teams, and achieve increased end-user satisfaction. As software development professionals, by cultivating Craftsmanship, we can shape a future where excellence, quality, user satisfaction, and a commitment to doing good work guide software development and DevOps."
    } ,
  
    {
      "title"    : "Essential Git Commands: From Beginner to Expert",
      "url"      : "/en/complete-git-guide/",
      "img"      : "PHBpY3R1cmU+CiAgPHNvdXJjZSBzcmNzZXQ9Ii9hc3NldHMvaW1nL3R1dG8tZ2l0LTMwMC0zOGFhM2ExZWEuYXZpZiAxLjB4LCAvYXNzZXRzL2ltZy90dXRvLWdpdC02MDAtMzhhYTNhMWVhLmF2aWYgMi4weCIgdHlwZT0iaW1hZ2UvYXZpZiI+CiAgPHNvdXJjZSBzcmNzZXQ9Ii9hc3NldHMvaW1nL3R1dG8tZ2l0LTMwMC1jYTMxYTJjNmIuanBnIDEuMHgsIC9hc3NldHMvaW1nL3R1dG8tZ2l0LTYwMC1jYTMxYTJjNmIuanBnIDIuMHgiIHR5cGU9ImltYWdlL2pwZWciPgogIDxpbWcgc3JjPSIvYXNzZXRzL2ltZy90dXRvLWdpdC0zMDAtY2EzMWEyYzZiLmpwZyIgYWx0PSJFc3NlbnRpYWwgR2l0IENvbW1hbmRzOiBGcm9tIEJlZ2lubmVyIHRvIEV4cGVydCI+CjwvcGljdHVyZT4K",
      "isencoded" : "y",
      "tags"     : "Git, Tutorial, DevOps, Dev, Practice",
      "content"  : "In this Git command guide, we will explore the essential concepts necessary to master the most widely used version control tool. While most developers are familiar with basic commands, we will delve into real-life scenarios where utilizing Git’s advanced features can save time, solve complex issues, enhance your workflow, and ultimately make you a confident and expert Git user. Introduction to Git Key Concepts of Git Commit Hash Branches Different Git Spaces Basic Git Commands Creating a Git Repository with git init or git clone Adding Files with the git add Command Recording Changes with git commit Using git stash to Temporarily Set Aside Changes Managing Branches and Merges Creating Branches with git branch and git checkout Merging Branches with git merge Resolving Merge Conflicts Deleting Merged Branches Using git cherry-pick to Apply Specific Commits Resetting a Branch with git reset Reverting Changes with git revert Rebasing and Reviewing History Understanding Rebase and Using git rebase Exploring Commit History with git log Using the HEAD Pointer to Navigate History Special Operators ^ and ~ for Referencing Specific Commits Version Control with Git Comparing Differences with git diff Retrieving Previous Versions with git checkout Managing Remote Branches with git push and git pull Advanced Scenarios for Manipulating Git History How to Rewrite Multiple Commits into One? Modifying the Message of a Commit Modifying the Message of the Last Commit Modifying the Message of an Older Commit Modifying Files in a Previous Commit Conclusion Introduction to Git Git was developed by Linus Torvalds in 2005 to address the specific needs of the Linux kernel development. At the time, existing version control systems were slow and inefficient in managing a project of the size and complexity of the Linux kernel. Thus, Torvalds set out to create a tool that was fast, distributed, and capable of effectively handling parallel development branches. Since then, Git has become the de facto version control tool in the software development industry. Its flexibility, speed, and power make it an essential choice for collaborative development teams. InstallationIf you haven’t already, refer to the following page to install Git: https://git-scm.com/downloads. Key Concepts of Git Git is built on several key concepts that make it powerful and flexible. Understanding these concepts will help you grasp how Git works and use it effectively. Commit The commit is the central element of Git. It records a complete snapshot of the changes made to your code. From Git’s perspective, the history of your code is a series of interconnected commits, as depicted in the following representation: Representation of Git commit chains. Each commit references the previous commit. It only stores the delta compared to the previous commit. This delta can include file modifications as well as file additions, moves, or deletions. The commit is the visible part of Git’s internal representation structure. Note that there are other objects used by Git to store code changes, but from our perspective as Git users, the commit is the object we interact with. Hash When you make a commit, Git creates a unique identifier for that commit, commonly known as a “hash” or “SHA”. This hash is based on the content of the commit, including file modifications, the author, the commit message, and other metadata. It serves to uniquely identify the commit within the project’s history. Here’s an example of commit display, showing their respective hashes at the beginning of each line: 8634ee6 (HEAD -&amp;gt; main, origin/main, origin/HEAD) feat: Adds dark theme 🖤 (#32) aae8242 fix: CSS on phones d9bb54f refacto: Big CSS uniformization and refacto 🌟 4c77908 refacto: Tags, search results, and animation on articles (#31) fec3121 refacto: Adjusted image size, cropping, and resolution 📺 cd6a213 fix: GitHub Actions error RPC failed; HTTP 408 curl 18 HTTP/2 (#29) Output Branches Git commits allow you to track your project’s history in a clear and structured manner. However, they also enable you to work on different lines of development in isolation, which is referred to as branches. Let’s see how branches are visualized within a commit history: Commits forming 2 branches. We can observe that the sequence of these commits forms 2 branches, allowing you to work on multiple features or fixes simultaneously. We will explore how Git reconciles these branches using commits later on. Best PracticeRegular and meaningful commits are a recommended practice with Git. This helps maintain a clear history, facilitating collaboration, debugging, and tracking changes. Commits serve as a form of documentation for the evolution of your project, aiding developers in understanding the history of changes and reverting to previous states if necessary. Different Git Spaces The last concept to understand is the spaces within Git. A space is a specific working area where Git stores the different versions of your project’s files. Understanding this concept will help you know which command to apply and in what situation, whether it’s managing ongoing changes, preparing commits, or navigating between different versions of your code. Let’s explore the 5 spaces managed by Git: Representation of different Git spaces and actions of some commands. Stash or Stash Area: The stash area is a special place where you can ask Git to temporarily store changes from your working directory. The stash area provides flexibility to switch to another branch, work on a different task, or perform tests without creating a commit. Workspace or Working Directory: The workspace is the directory where you work on your files. It contains the current versions of the files and is modified as you make changes to your code. Index or Staging Area: The index is an intermediate space between the workspace and commits. It acts as a staging area where you select specific changes to include in the next commit. Local Repository: This is your local repository where Git stores the complete history of your project, including all the commits, branches, tags, and configuration information. It is the local copy of your Git source code, on which you work and perform versioning operations. Using the local repository allows you to perform operations independently, without the need for a network connection, before synchronizing with remote repositories if necessary. Remote Repository: The remote represents a remote repository where you can store your code, such as a Git repository on a hosting platform like GitHub or GitLab. The remote is used to facilitate collaboration with other developers, share your code, and synchronize changes among team members. By understanding these concepts, you will be able to navigate your project’s history more effectively, organize your work with branches, prepare your commits with the index, and collaborate with other developers using remotes. Basic Git Commands Now that we understand the concepts of Git, let’s dive into the basic commands that will help you effectively manage your source code. Creating a Git Repository with git init or git clone Scenario 1: Your project is not yet under Git version control: To start using Git in your project, you need to initialize a repository. This is done using the git init command in the root directory of your project. For example: cd /path/to/my_project git init Shell Scenario 2: Your project is already in a remote Git repository: Most of the time, a remote repository already exists, and you want to download it to your local machine. You can do this by running git clone &amp;lt;REPO URL&amp;gt; to clone the repository to your local machine. cd /path/to/a_directory git clone https://github.com/progit/progit2.git Shell Adding Files with the git add Command Once you have initialized a Git repository, you can add files to Git’s index using the git add command. This allows Git to track the changes in these files. For example, to add all modified files in your working directory to the index, you can run the following command: git add . Shell Recording Changes with git commit Once you have added the files to the index, you can save the changes by creating a commit using the git commit command. Each commit represents a snapshot of the state of your project at a given point in time. For example, to create a commit with a descriptive message, you can use the following command: git commit -m &quot;Add new feature...&quot; Shell Using git stash to Temporarily Set Aside Changes Sometimes, you may have unfinished changes in your working directory, but you need to quickly switch to another task or branch. In such cases, you can use the git stash command to temporarily set aside your changes. For example: git stash Shell The above example stashes your changes in a temporary area called the stash. Once your changes are “stashed away,” you can switch to another task or branch. Now, let’s say you have finished that task and want to resume your “stashed away” changes. You can apply them to your working directory using the git stash pop command. This command automatically applies the latest stash and removes it from the stash list. For example: git stash pop Shell This command applies the latest stash and restores your changes to your working directory. You can now continue working on your previous changes. Using git stash and git stash pop allows you to temporarily set aside your ongoing changes and easily reapply them when you’re ready to come back to them. This provides valuable flexibility when managing tasks and development branches. Managing Branches and Merges One of the powerful features of Git is its ability to manage parallel development branches. Branch and merge management is a key skill to acquire for efficient development. Let’s see how Git facilitates this management. Spoiler AlertAt the end of this article, I recommend a great tool for practicing Git commands while visualizing branch and commit actions. The following screenshots are actually taken with this tool https://learngitbranching.js.org/. Creating Branches with git branch and git checkout You can create a new branch in your Git repository using the git branch command. For example, to create a branch named “feat/new-functionality,” you can execute: git branch feat/new-functionality Shell To switch to this new branch, you will use the git checkout command. For example: git checkout feat/new-functionality Shell You are now on the “feat/new-functionality” branch and can start making changes specific to this feature. Another quicker way is to use the git checkout -b command, which creates the branch and switches to it in one step after creating it: git checkout -b feat/new-functionality Shell Merging Branches with git merge When you have finished developing a feature or fixing a bug in a branch, it’s time to merge those changes with another branch, often the main branch (e.g., main or master). This is where the git merge command comes into play. To merge one branch into another, you can use the git merge command, specifying the branch you want to merge. For example, to merge the “feat/new-functionality” branch into the main branch, you can use the following command: git checkout main git merge feat/new-functionality Shell Git will automatically attempt to merge the changes from the specified branch into the current branch. If conflicts arise, Git will inform you, and you will need to resolve those conflicts manually. Resolving Merge Conflicts When there are conflicts between the changes made in the branches being merged, Git cannot automatically resolve those conflicts. In such cases, you will need to manually resolve the conflicts. Git will mark the conflicting areas in the affected files, allowing you to see the differences and choose which modifications to keep. Once you have resolved the conflicts, you need to add the modified files to the index using git add and then commit to finalize the merge. Deleting Merged Branches After merging a branch and verifying that the changes have been successfully integrated, you can delete the merged branch to keep your project history clean. To delete a merged branch, you can use the git branch command with the -d option followed by the branch name. For example, to delete the “feat/new-functionality” branch after its merge, you can execute: git branch -d feat/new-functionality Shell Using git cherry-pick to Apply Specific Commits Sometimes, you may need to apply only certain commits from one branch to another. In such cases, you can use the git cherry-pick command. For example, to apply the commit with the hash “abcdef” to the current branch, you can execute: git cherry-pick abcdef Shell This will apply the specified commit to the current branch, creating a copy of the commit on that branch. Cherry-pick of commit C2 from the feature branch to the bugfix branch. Resetting a Branch with git reset If you need to go back to a previous state of your branch, you can use the git reset command. For example, to reset the current branch to a specific commit, you can execute: git reset &amp;lt;commit&amp;gt; Shell This will bring your branch back to the state of the specified commit, discarding all subsequent commits (note that the commit is not deleted). Reset of commit C3 from the feature branch. Reverting Changes with git revert If you want to undo one or more specific commits while keeping a record of that undo in the history, you can use the git revert command. For example, to revert the last commit, you can execute: git revert HEAD Shell This will create a new commit that undoes the changes made by the previous commit. Revert of commit C3, creating a new commit C3&#39;. Branch management is a key feature of Git, allowing you to work efficiently on different features or fixes in parallel. The git branch, git checkout, git cherry-pick, git reset, and git revert commands provide the necessary flexibility to manage branches and changes optimally. Rebasing and Reviewing History Rebasing is an advanced feature in Git that allows you to modify the commit history. In this section, we will explore rebasing along with other useful commands for examining and navigating your repository’s history. Understanding Rebase and Using git rebase Rebasing allows you to rearrange commits in your branch, either by placing them onto another branch or by reorganizing them in a linear manner. This can be helpful in maintaining a clean and easy-to-follow commit history. To perform an interactive rebase, use the git rebase -i command. For example: git rebase -i &amp;lt;destination-branch&amp;gt; Shell This command will open an editor with a list of commits that you can rearrange or modify as needed: Rebase to swap C3 and C4 of the feature branch Output Exploring Commit History with git log The git log command allows you to examine the commit history of your repository. By default, it displays essential information such as the author, date, and commit message. For example: $ git log commit 8634ee6a55086f6cf4ff7fa0ee4bbceb283d7c2c (HEAD -&amp;gt; main, origin/main, origin/HEAD) Author: Jean-Jerome Levy &amp;lt;jeanjerome@users.noreply.github.com&amp;gt; Date: Thu May 25 23:54:03 2023 +0200 feat: Adds dark theme 🖤 (#32) commit aae82424db11ad31a6aba2cb0c27a264e177b9a1 Author: Jean-Jerome Levy &amp;lt;jeanjerome@users.noreply.github.com&amp;gt; Date: Wed May 24 20:41:20 2023 +0200 fix: CSS on phones commit d9bb54f71bd3bf609cfd6ccfcfdd8df14bf5f06b Author: Jean-Jerome Levy &amp;lt;jeanjerome@users.noreply.github.com&amp;gt; Date: Tue May 23 22:59:36 2023 +0200 refacto: Big CSS uniformization and refacto 🌟 ... Output This command displays a detailed list of all commits, from the most recent to the oldest, allowing you to track the evolution of your code. You can format the log output to display only the information you are interested in. For example, to have a compact display, enter git log --oneline: $ git log --oneline 8634ee6 feat: Adds dark theme 🖤 (#32) aae8242 fix: CSS on phones d9bb54f refacto: Big CSS uniformization and refacto 🌟 ... Output git and viGit uses vi as the default text editor. Here are some commands to keep in mind: ESC : q to exit, ESC : i to edit, ESC : x to quit and save, ESC : s/x/y/g to replace all occurrences of x with y. Using the HEAD Pointer to Navigate History The HEAD pointer is a special pointer that refers to the current commit in your repository. You can use it to navigate through the commit history. For example, to display the details of the current commit, you can execute: $ git show HEAD commit 8634ee6 a55086f6cf4ff7fa0ee4bbceb283d7c2c Author: Jean-Jerome Levy &amp;lt;jeanjerome@users.noreply.github.com&amp;gt; Date: Thu May 25 23:54:03 2023 +0200 feat: Adds dark theme 🖤 (#32) diff --git a/_includes/head.html b/_includes/head.html index bf20ecf..2c3823d 100755 --- a/_includes/head.html +++ b/_includes/head.html @@ -109,6 +109,9 @@ font-display: swap; src: url(&quot;/assets/fonts/nunito-regular.woff2&quot;) format(&quot;woff2&quot;); } - &amp;lt;/style&amp;gt; + &amp;lt;/style&amp;gt; + &amp;lt;script&amp;gt; + localStorage.getItem(&#39;darkMode&#39;) === &#39;true&#39; &amp;amp;&amp;amp; document.documentElement.setAttribute(&#39;data-mode&#39;, &#39;dark&#39;); + &amp;lt;/script&amp;gt; ... Output This command will display detailed information about the current commit, including the modifications made. Special Operators ^ and ~ for Referencing Specific Commits The ^ and ~ operators allow you to reference specific commits using relative notations. For example: ^ refers to the parent commit (the previous one). ~ refers to the commit preceding the parent (the second-to-last one). For instance, to display the details of the direct parent commit of the current commit, you can use: git show HEAD^ Shell These operators are useful for quickly navigating through the commit history without having to know their exact identifiers. Rebasing and reviewing history are advanced features of Git that enable you to manage and structure your commit history effectively. The git rebase, git log, HEAD^, and HEAD~ commands provide you with the necessary tools to explore, manipulate, and understand your Git repository’s history. Version Control with Git One of the fundamental aspects of Git is its version control system, which allows you to manage different versions of your project effectively. In this section, we will explore commands for comparing differences between versions, retrieving previous versions, and managing remote branches. Comparing Differences with git diff The git diff command allows you to visualize differences between versions of the source code. For example, to display the modifications between the current state and the last commit, you can execute: $ git diff HEAD diff --git a/_posts/2023-05-28-tuto-git.markdown b/_posts/2023-05-28-tuto-git.markdown new file mode 100644 index 0000000..22b5ca1 --- /dev/null +++ b/_posts/2023-05-28-tuto-git.markdown @@ -0,0 +1,509 @@ +--- +layout: post +title: &quot;Guide Complet de Git : Maîtrisez ses Commandes Essentielles&quot; Output This command displays the lines that have been modified, added, or deleted between the two versions. Here, it indicates that I added a new file and provides its content. Retrieving Previous Versions with git checkout If you need to go back to a previous version of your project, you can use the git checkout command. For example, to go back to a specific commit with the identifier “abcdef,” you can execute: git checkout abcdef Shell This will set your working directory to the state of that commit, allowing you to work with that specific version. Checkout main branch to create a bugfix branch. Managing Remote Branches with git push and git pull Git allows you to work with remote repositories, such as those hosted on platforms like GitHub or GitLab. To push your local changes to a remote repository, use the git push command. For example: git push origin feat/my-feature Shell This command sends the changes from your local branch to the corresponding branch in the remote repository. To retrieve the changes made in the remote repository and merge them into your local branch, use the git pull command. For example: git pull origin bugfix/the-fix Shell This command retrieves the changes from the corresponding branch in the remote repository and automatically merges them into your local branch. These commands allow you to synchronize your local repository with remote repositories, facilitating collaborative work and version tracking. Advanced Scenarios for Manipulating Git History In this tutorial, our main goal is to teach you how to handle the cases we will discuss in this chapter. You will apply the concepts we have just explored to manipulate commit history and solve complex problems that developers often encounter in their projects. By acquiring these skills, you will become an experienced developer, distinguishing yourself from those who rely solely on basic Git commands. How to Rewrite Multiple Commits into One? To rewrite multiple commits into one, you can use the git rebase -i &amp;lt;commit&amp;gt; command, where &amp;lt;commit&amp;gt; is the commit prior to the ones you want to rewrite. Here are the steps to follow: First, use the git log --oneline command to identify the number of commits you want to rewrite into a single one by counting the last commit. Once you have identified the number, proceed with the rebase. For example, if you want to rewrite the last three commits, use git rebase -i HEAD~3. This will open the default text editor with a list of commits to be rewritten. In the text editor, replace the word pick (or p) with squash or simply s for the commits you want to merge into one. For example, if you have three commits and you want to rewrite them into one, you will modify the second and third commits using squash or s. Again, knowledge of vi commands can make this task easier: ESC : s/p/s/g Save and close the text editor (via the vi command ESC : x). Another editor window will open, allowing you to modify the message of the final commit. You can keep the message from the first commit or modify it as needed. Save and close this editor window as well. Git will then perform the rebase and merge the selected commits into one commit. Make sure you understand the implications of rebasing, as it modifies commit history. If you have already pushed these commits to a remote repository, you will need to perform a git push --force to update the remote repository with the rewritten history. Note that rewriting shared history can have consequences for other developers working on the same project. Caution It is important to communicate with your team and follow best collaboration practices when rewriting commits. In general, it is recommended to proceed this way when working alone on your branch. Modifying the Message of a Commit Sometimes you may commit with an incorrect, incomplete, or poorly formatted message. In such cases, Git provides a simple solution to modify the message of a previous commit. Here’s how to do it. Modifying the Message of the Last Commit Use the git commit --amend command followed by the -m option and the new message you want to use: git commit --amend -m &quot;New commit message&quot; Shell This will modify the message of the last commit using the specified new message. Modifying the Message of an Older Commit If you want to modify the message of an older commit, you can use the git rebase -i &amp;lt;commit&amp;gt; command, where &amp;lt;commit&amp;gt; is the commit prior to the one you want to modify. git rebase -i HEAD~3 Shell In the text editor that opens, replace “pick” with “reword” or simply “r” in front of the commit you want to modify the message for. This will open the text editor with a list of commits. Modify the word “pick” to “reword” or “r” in front of the appropriate commit, then save and close the editor. Once you have modified the commit message, save the changes and close the editor. Git will then perform the rebase and allow you to modify the message of the selected commit. It’s important to note that if you have already pushed the commit you are modifying the message for to a remote repository, you will need to perform a git push --force to update the remote repository with the new message. The ability to modify the message of a previous commit allows you to correct errors or improve the clarity of messages for a more accurate and informative commit history. Caution Make sure to communicate with other developers working on the same project, as this can affect their commit history. In general, it is recommended to proceed this way when working alone on your branch. Modifying Files in a Previous Commit There may be cases where you need to modify files in a previous commit in Git. This could be due to content errors, forgetting certain files, or other reasons requiring retroactive changes. While Git encourages preserving commit history integrity, there are methods to make changes to past commits. Here are some steps to modify files in a previous commit: Use the git rebase -i &amp;lt;commit&amp;gt; command, where &amp;lt;commit&amp;gt; is the commit prior to the one you want to make changes to. This will open the text editor with the list of commits in reverse chronological order. Locate the commit you want to modify and replace the word “pick” in front of that commit with “edit”. Save the changes and close the editor. Git will then perform the rebase and pause the process after applying the commit you want to modify. Use the git checkout &amp;lt;commit&amp;gt; -- &amp;lt;file&amp;gt; command to extract the specific file version you want to modify from the previous commit. For example, git checkout HEAD~1 -- file.txt extracts the file.txt version from the previous commit. Make the necessary changes to the file according to your needs. Once you have made the modifications, use the git add &amp;lt;file&amp;gt; command to update the changes in Git’s index. Use the git commit --amend command to create a new commit with the modifications. You can modify the commit message if necessary. Repeat steps 4 to 7 for each file you want to modify in this commit. When you have finished modifying the files, use the git rebase --continue command to proceed with the rebase and apply the changes. It’s important to note that if you have already pushed the commit you are modifying the files for to a remote repository, you will need to perform a force push (git push --force) to update the remote repository with the modifications. The ability to modify files in a previous commit allows you to correct errors or make retroactive changes when needed. However, be cautious when modifying commit history as it can lead to inconsistencies and conflicts if misused. Caution Make sure to communicate with other developers working on the same project, as this can affect their commit history. In general, it is recommended to proceed this way when working alone on your branch. Conclusion We have covered the essential concepts of Git and explored a set of key commands to help you master this powerful tool. By understanding commits, workspaces, the index, stash, and local and remote repositories, you are now ready to optimize your work and make the most of Git. If you want to further expand your knowledge of Git, I recommend checking out the following resources: Official Git Documentation: The official Git documentation, available in multiple languages, is a reliable, comprehensive, and clear source to learn more about advanced Git features. Learn Git Branching: A web application, with its code available on GitHub, that offers interactive tutorials and allows you to visualize the impact of a command on branches and commits in your Git repository. I recommend trying it out to test your new knowledge. By exploring these additional resources and continuing to practice, you will be able to deepen your understanding of Git and become an expert."
    } ,
  
    {
      "title"    : "Unmask the Bullshit in Agility to Stop Getting Screwed Over",
      "url"      : "/en/agility-bullshit/",
      "img"      : "PHBpY3R1cmU+CiAgPHNvdXJjZSBzcmNzZXQ9Ii9hc3NldHMvaW1nL2FnaWxpdHktYnVsbHNoaXQtMzAwLWNiNzM0ZjZlMy5hdmlmIDEuMHgsIC9hc3NldHMvaW1nL2FnaWxpdHktYnVsbHNoaXQtNjAwLWNiNzM0ZjZlMy5hdmlmIDIuMHgiIHR5cGU9ImltYWdlL2F2aWYiPgogIDxzb3VyY2Ugc3Jjc2V0PSIvYXNzZXRzL2ltZy9hZ2lsaXR5LWJ1bGxzaGl0LTMwMC03M2IzYjVmNmQuanBnIDEuMHgsIC9hc3NldHMvaW1nL2FnaWxpdHktYnVsbHNoaXQtNjAwLTczYjNiNWY2ZC5qcGcgMi4weCIgdHlwZT0iaW1hZ2UvanBlZyI+CiAgPGltZyBzcmM9Ii9hc3NldHMvaW1nL2FnaWxpdHktYnVsbHNoaXQtMzAwLTczYjNiNWY2ZC5qcGciIGFsdD0iVW5tYXNrIHRoZSBCdWxsc2hpdCBpbiBBZ2lsaXR5IHRvIFN0b3AgR2V0dGluZyBTY3Jld2VkIE92ZXIiPgo8L3BpY3R1cmU+Cg==",
      "isencoded" : "y",
      "tags"     : "Agile, DevOps, Leadership, Well-Being",
      "content"  : "Agility is a widely adopted concept in the software development industry, offering benefits such as flexibility, efficiency, and collaboration. However, it is important to recognize that sometimes agility is used abusively or even detrimentally against developers. In this article, we will explore the concept of “bullshit” surrounding agility, shedding light on questionable practices and providing advice on how to avoid them. The False Promises of Agility Critically Analyzing the Often Exaggerated Promises of Agility The Importance of Transparency and Honest Communication The Tyranny of Velocity How Velocity Becomes an End Goal, Sacrificing Quality The Importance of Balancing Velocity and Quality The Illusion of Autonomy How Agility can be Used to Mask Micromanagement Practices How to Foster Genuine Autonomy in Agile Teams Exploiting Agility to Justify Excessive Working Hours The Dangers of “Always-On” Agility and the Expectation of Permanent Availability How to Establish Clear Boundaries to Prevent Burnout Managerial Disengagement through Agility How Agility can be Used to Avoid Managerial Responsibilities The Importance of Genuine Agile Management and Engaged Leadership Conclusion The False Promises of Agility Agility is often presented as a miracle solution, promising quick results, improved collaboration, and increased satisfaction among team members. However, it is crucial to adopt a critical perspective and recognize the false promises that can accompany agility. Critically Analyzing the Often Exaggerated Promises of Agility One of the main false promises is the guarantee of rapid and frequent delivery of features. While agility promotes iterative and incremental delivery, it is essential to understand that constraints may arise. Unforeseen obstacles, external dependencies, or technical difficulties can slow down the pace of delivery. Teams can become frustrated and disappointed when the promises of continuous delivery are not fulfilled. Another false promise of agility is that of harmonious collaboration within the team. The underlying idea is that a self-organized and multidisciplinary team will work seamlessly and transparently. However, the reality can be different, with conflicts of interest, differences in priorities, and communication challenges. Employees can feel overwhelmed by endless meetings and frustrated by the lack of clarity and coordination. Moreover, agility is often presented as a magical solution to meet all stakeholders’ needs. This can lead to unrealistic expectations and increased pressure on developers to deliver quick results. When these expectations are not managed realistically, employees can feel overwhelmed and overworked, leading to burnout and decreased motivation. The Importance of Transparency and Honest Communication It is therefore crucial to promote transparency and honest communication from the start, highlighting the realities and challenges that Agile teams may face. Exaggerated promises should be avoided, and realistic expectation management should be put in place. It is also important to provide adequate support to developers, giving them the necessary resources to cope with difficulties and recognizing their successes and efforts. Agility should be perceived as a means to improve work practices, acknowledging both its advantages and limitations, and ensuring that developers are not exploited in its name. The Tyranny of Velocity One of the common pitfalls of agility is the excessive focus on velocity at the expense of work quality and employee well-being. In many Agile environments, velocity has become a standalone objective, closely measured and monitored. This creates constant pressure on teams to increase their velocity, disregarding other important aspects of software development. How Velocity Becomes an End Goal, Sacrificing Quality When velocity becomes the ultimate priority, employees may be incentivized to take shortcuts, sacrifice quality, and overlook good development practices. Rigorous testing, thorough code review, and validation processes can be neglected as they might slow down velocity. This approach jeopardizes product stability, user experience, and customer satisfaction. Moreover, the constant pressure to increase velocity can have detrimental effects on employee well-being. Team members can feel stressed and overloaded, leading to burnout and decreased motivation. Agility, which should foster collaboration and autonomy, can quickly turn into a toxic environment where employees constantly feel pressured to meet unrealistic velocity targets. The Importance of Balancing Velocity and Quality To avoid the tyranny of velocity, it is crucial to establish a balance between velocity and quality. Emphasis should be placed on delivering high-quality features rather than mere quantity. Teams should be encouraged to take the necessary time to perform tasks correctly, adhering to good development practices. It is also crucial to foster a work environment that takes into account employee well-being, avoiding work overload and maintaining a healthy work-life balance. Furthermore, in some Agile teams, it is even common not to measure velocity at all. Other factors such as customer satisfaction, product quality, and team health are taken into account. By adopting a thoughtful approach to agility, we can mitigate the detrimental effects of velocity and create a healthier and more effective work environment. The Illusion of Autonomy Agility is often presented as a means to provide teams with greater autonomy. However, it is all too common for this autonomy to be an illusion, masking micromanagement practices that go against the Agile spirit. How Agility can be Used to Mask Micromanagement Practices In many cases, Agile teams are assigned specific goals and tasks, only to be subjected to strict directives on how they should be carried out. Team members may find themselves with little room for decision-making and exercising their creativity. This approach limits their actual autonomy and reduces them to mere task executors. When employees do not feel truly autonomous, their motivation and commitment can suffer. They may feel disempowered in their work and lack accountability, which hinders their professional growth. Autonomy is one of the pillars of intrinsic motivation, and depriving employees of it can lead to decreased productivity and job satisfaction. How to Foster Genuine Autonomy in Agile Teams To promote genuine autonomy in Agile teams, it is essential to establish an environment of trust. Managers and leaders must demonstrate transparency and open communication, explaining goals and expectations while allowing teams the freedom to choose how to achieve them. It is important to create a space where team members can express their ideas, take initiatives, and make their own decisions. Moreover, managers should adopt a servant leadership approach, supporting team members rather than exerting authoritative control. They should encourage collaboration, foster learning, and provide opportunities for professional development. By empowering employees to grow and actively contribute to the team’s success, real autonomy can be nurtured. However, it is important to recognize that autonomy does not mean lack of accountability. Teams should always be responsible for their results and commitments to stakeholders. However, this responsibility should be balanced with the trust and freedom granted to team members to make decisions and shape their work. Exploiting Agility to Justify Excessive Working Hours Agility, when misunderstood or misapplied, can be exploited to justify excessive working hours and a constant expectation of availability. Under the pretext of quickly responding to changing project needs, some organizations adopt an “always-on” mentality that keeps employees in a perpetual state of work. The Dangers of “Always-On” Agility and the Expectation of Permanent Availability This expectation of permanent availability can have detrimental effects on employees’ mental and physical health. It can lead to burnout, an imbalance between work and personal life, as well as deteriorating interpersonal relationships and overall quality of life. Employees constantly feel pressured to immediately respond to demands, even outside scheduled working hours. It is important to maintain a healthy work-life balance, even in an Agile environment. This requires establishing clear boundaries in terms of working hours and availability. Employees should be encouraged to disconnect and rest to maintain their well-being and long-term productivity. How to Establish Clear Boundaries to Prevent Burnout This is the essential role of managers. They ensure the value of balance and respect for boundaries. They accept flexible working hours that take into account the workload peaks the team has experienced and refrain from sending messages or requests outside of those times. They actively encourage breaks and recovery periods. This approach ensures full team commitment during workload surges and the full involvement of each team member when necessary. Managerial Disengagement through Agility Agility, when misunderstood or improperly implemented, can lead to managerial disengagement. Instead of taking on their management and leadership responsibilities, some managers may use agility as an excuse to avoid obligations and decision-making. How Agility can be Used to Avoid Managerial Responsibilities In some cases, managers may hide behind Agile principles to avoid making difficult decisions or confronting problems. They may transfer the responsibility onto the teams, leaving them to fend for themselves without providing the necessary guidance and support. This creates an environment where teams feel abandoned, and communication and collaboration are hindered. When managers disengage, it has consequences on team effectiveness and employees’ professional development. Team members may lack clear direction, constructive feedback, and growth opportunities. They may also feel a lack of trust in their managers, which affects collaboration and individual commitment. The Importance of Genuine Agile Management and Engaged Leadership To prevent this disengagement of managers, it is crucial to promote genuine Agile management and engaged leadership. Managers should be encouraged to take on their management responsibilities and play an active role in the team’s success. This involves providing a clear vision, setting goals, and defining realistic expectations for team members. Furthermore, managers should actively support team members by providing regular and constructive feedback, assisting in problem-solving, and guiding their professional development. They should also be open to communication and collaboration, creating an environment where employees feel comfortable expressing their concerns and ideas. It is also important for managers to recognize and take responsibility for their mistakes. Agility encourages learning and continuous improvement, and managers should lead by example by admitting their mistakes and using them as learning opportunities for the team. Conclusion It is crucial to recognize the abuses and “bullshit” that can accompany agility in work environments. By shedding light on these questionable practices, we can work towards a more responsible and ethical implementation of agility, ensuring the well-being of employees while leveraging its real benefits. Let us remember that agility should be a means to improve our work and lives, and not a tool to exploit employees."
    } ,
  
    {
      "title"    : "Towards DevOps Excellence: Explore Docker Update Automation",
      "url"      : "/en/update-docker-image/",
      "img"      : "PHBpY3R1cmU+CiAgPHNvdXJjZSBzcmNzZXQ9Ii9hc3NldHMvaW1nL3VwZGF0ZS1kb2NrZXItaW1hZ2UtMzAwLWI2NGQ3ZWNmNy5hdmlmIDEuMHgsIC9hc3NldHMvaW1nL3VwZGF0ZS1kb2NrZXItaW1hZ2UtNjAwLWI2NGQ3ZWNmNy5hdmlmIDIuMHgiIHR5cGU9ImltYWdlL2F2aWYiPgogIDxzb3VyY2Ugc3Jjc2V0PSIvYXNzZXRzL2ltZy91cGRhdGUtZG9ja2VyLWltYWdlLTMwMC1iNTEwY2EzNTMuanBnIDEuMHgsIC9hc3NldHMvaW1nL3VwZGF0ZS1kb2NrZXItaW1hZ2UtNjAwLWI1MTBjYTM1My5qcGcgMi4weCIgdHlwZT0iaW1hZ2UvanBlZyI+CiAgPGltZyBzcmM9Ii9hc3NldHMvaW1nL3VwZGF0ZS1kb2NrZXItaW1hZ2UtMzAwLWI1MTBjYTM1My5qcGciIGFsdD0iVG93YXJkcyBEZXZPcHMgRXhjZWxsZW5jZTogRXhwbG9yZSBEb2NrZXIgVXBkYXRlIEF1dG9tYXRpb24iPgo8L3BpY3R1cmU+Cg==",
      "isencoded" : "y",
      "tags"     : "DevOps, DevSecOps, Docker, Update, Security, Automation",
      "content"  : "In the DevOps ecosystem, containerizing applications with Docker has become a common practice. Indeed, Docker images provide a convenient solution for packaging applications and dependencies, enabling quick and easy deployment. However, keeping these images up to date can be a significant challenge, especially when managing a large number of images and ensuring the absence of vulnerabilities. In this article, we will explore how to automate Docker image updates by leveraging DevOps practices to streamline and secure this essential process. Manual Management of Updates Challenges of Manual Management Security Risks Associated with Failure to Update Images Impacts on Deployment Efficiency and Reliability Automation of Updates Implementing CI/CD Monitoring Updates and Vulnerabilities Other Measures to Take Using a Centralized Registry Integration of Vulnerability Analysis Tools Regular Update Scheduling Automated Testing Reliable External Sources Rollback and Previous Versions Conclusion Manual Management of Updates Manual management of Docker image updates can lead to various challenges, security risks, and impacts on the efficiency and reliability of an application. Let’s examine the different issues posed by manual management of these updates. Challenges of Manual Management Manual management of Docker image updates can become complex and tedious, especially when dealing with a large number of images and their dependencies. Manual tasks such as searching, downloading, and implementing updates can be time-consuming and require significant effort. Moreover, human errors can easily occur during this process, resulting in inconsistencies or incompatibilities between different versions of images. Security Risks Associated with Failure to Update Images The initial reaction may be to postpone this topic for later. However, it is crucial to be aware that failing to regularly update Docker images exposes applications to security risks. Outdated images may contain known vulnerabilities that can be exploited by attackers. Every day, new security vulnerabilities are discovered, and it is highly likely that your containerized application is affected by at least one of these vulnerabilities every month. Therefore, not updating your images exposes you to risks such as confidentiality breaches, injection attacks, denial of service, etc. It is essential to keep your images up to date to ensure a minimum level of security. Impacts on Deployment Efficiency and Reliability Manual management of Docker image updates can have a negative impact on deployment efficiency and reliability. Delays in updating images can lead to compatibility issues with new versions of applications or their dependencies, resulting in errors or malfunctions during deployments. Additionally, in cases where urgent deployment is required, manual management can slow down the overall process and cause delays in the production release of new features or fixes. Therefore, it is essential to adopt automated approaches and integrate DevOps practices (specifically DevSecOps in this case) to simplify and secure the update of your Docker images. Automation of Updates Automating Docker image updates provides an efficient solution to simplify and secure the image management process. Let’s explore different aspects of automation in the context of DevOps. Implementing CI/CD Continuous Integration (CI) and Continuous Deployment (CD) are key practices in DevOps. They enable extensive automation of the development and deployment process. By integrating Docker image updates into CI/CD pipelines, you can automate the building, testing, and deployment of security patches in your application images. This ensures a consistent and reproducible approach to image management throughout an application’s lifecycle. Monitoring Updates and Vulnerabilities It is equally crucial to monitor Docker image updates and associated vulnerabilities. Automated monitoring tools can be used to track official image sources, private registries, or security notifications. These tools can report on new available versions and security patches, enabling quick responsiveness for updates. By monitoring known vulnerabilities, you can also take proactive measures to minimize security risks by identifying and resolving vulnerabilities in the used images. Other Measures to Take Automating Docker image updates is just the beginning of a broader approach to implement. Here are some best practices that go hand in hand with CI/CD. Using a Centralized Registry Using a centralized registry for your Docker images facilitates the management and distribution of updated images. You can configure a private registry, such as Docker Hub, or deploy your own internal registry. By centralizing the images, you can ensure consistency in the versions used across different deployment environments. Additionally, a centralized registry makes it easier to access, search, and manage images, which is essential when it comes to keeping images up to date. Integration of Vulnerability Analysis Tools It is crucial to integrate vulnerability analysis tools into your CI/CD pipeline to identify and resolve potential security issues. These tools perform automated analysis of Docker images, identifying known vulnerabilities in the application code, dependencies, and system components used. By integrating these tools, you receive detailed reports on faulty implementations, potential attack sources, detected vulnerabilities, as well as resolution guidance and version numbers of fixed dependencies. This allows you to have a clear overview and take appropriate measures to apply patches. Regular Update Scheduling Regular and automatic Docker image updates can be executed feasibly. You can define deployment strategies, such as progressive deployments or parallel updates, to minimize service disruptions. Automated Testing Automated testing plays a crucial role in validating a Docker image update. It allows you to verify the compatibility, stability, and performance of the application deployed on the new version of an image or application dependencies. Tests can include integration testing, load testing, and security testing to ensure that updates do not cause regressions or unexpected issues. They ensure the quality of deployments and help detect potential problems before they affect end users. In conclusion, automating Docker image updates relies on these best practices. By implementing them, you can ensure efficient, secure, and reliable management of image updates in your DevOps environment. This will improve security, reduce human errors, optimize deployments, and increase application availability. Automating Docker image updates is an essential element in maintaining an up-to-date, safe, and scalable development and deployment environment. Reliable External Sources When using external sources, such as public registries or third-party repositories, it is essential to establish control and validation mechanisms: Regularly verify the authenticity and integrity of image sources to avoid security issues or compromised images. Use verification mechanisms such as key fingerprints or digital signatures to ensure the origin and integrity of downloaded images. Also, ensure compliance with your organization’s security policies when using external sources. Rollback and Previous Versions Although these automated mechanisms ensure reproducibility, it is important to have rollback mechanisms in place for reverting to a previous version. Sometimes, an update may cause compatibility issues or malfunctions that were not detected during testing. By having rollback strategies in place, you can revert to a previous version of your application image, minimizing service disruptions. Additionally, it may be useful to retain previous versions of images for a certain period for testing, auditing, or compatibility with other system components. By considering these additional considerations, you can strengthen and enhance your Docker image update process. By combining these practices with the previously mentioned best practices, you will be able to establish an efficient, secure, and reliable automatic image update process. Conclusion Automatic Docker image updates have become an essential practice in DevOps environments. It offers numerous benefits, such as maintaining security, reducing human errors, and optimizing deployments. However, it requires the adoption of several DevOps practices, such as using a centralized image registry, integrating vulnerability analysis tools, scheduling and executing updates with CI/CD pipelines, as well as implementing automated testing. Securing applications is a crucial topic that has been neglected for too long. Securing the deployment process offers other advantages, such as greater maintainability and compatibility of applications within the IT infrastructure. It is a significant undertaking that is better to start before being caught off guard."
    } ,
  
    {
      "title"    : "Developer&#39;s Survival Manual for Making the Right Choices",
      "url"      : "/en/complete-developer-guide/",
      "img"      : "PHBpY3R1cmU+CiAgPHNvdXJjZSBzcmNzZXQ9Ii9hc3NldHMvaW1nL2RldmVsb3Blci1ndWlkZS1mZWF0dXJlLWRlbGl2ZXJ5LTMwMC0zMzgwMWVkMzQuYXZpZiAxLjB4LCAvYXNzZXRzL2ltZy9kZXZlbG9wZXItZ3VpZGUtZmVhdHVyZS1kZWxpdmVyeS02MDAtMzM4MDFlZDM0LmF2aWYgMi4weCIgdHlwZT0iaW1hZ2UvYXZpZiI+CiAgPHNvdXJjZSBzcmNzZXQ9Ii9hc3NldHMvaW1nL2RldmVsb3Blci1ndWlkZS1mZWF0dXJlLWRlbGl2ZXJ5LTMwMC1mNjE4Y2I4ZjcuanBnIDEuMHgsIC9hc3NldHMvaW1nL2RldmVsb3Blci1ndWlkZS1mZWF0dXJlLWRlbGl2ZXJ5LTYwMC1mNjE4Y2I4ZjcuanBnIDIuMHgiIHR5cGU9ImltYWdlL2pwZWciPgogIDxpbWcgc3JjPSIvYXNzZXRzL2ltZy9kZXZlbG9wZXItZ3VpZGUtZmVhdHVyZS1kZWxpdmVyeS0zMDAtZjYxOGNiOGY3LmpwZyIgYWx0PSJEZXZlbG9wZXIncyBTdXJ2aXZhbCBNYW51YWwgZm9yIE1ha2luZyB0aGUgUmlnaHQgQ2hvaWNlcyI+CjwvcGljdHVyZT4K",
      "isencoded" : "y",
      "tags"     : "Development, Guide, Practice, Abstraction, Refactoring, Unit-Test, Software-Design, DevOps",
      "content"  : "Navigating the intricacies of software development requires a deep understanding of various concepts and practices. This article explores the common decisions that developers face when building features, examining the benefits and drawbacks associated with each choice. From abstraction and refactoring decisions to implementation choices, as well as unit testing, and software design solutions, understanding the possible trade-offs can help developers navigate these challenges more effectively. Code Abstraction How to Process? Pros and Cons Refactoring Decisions What should you Do? Pros and Cons Implementation Choices What should it be Based on? Pros and Cons Unit Test Implementation and Coverage Why is it Essential? Pros and Cons Software Design Solutions Why is it Important? Pros and Cons Conclusion Code Abstraction Code abstraction involves hiding complex implementation details behind simpler and higher-level interfaces that shield the implementation details, enabling developers to work with simpler and more manageable abstractions that are easier to understand and handle. Abstraction is a fundamental principle in programming as it helps manage the complexity of large software projects. It allows developers to focus on the essential aspects of a system while ignoring low-level implementation details. By abstracting away unnecessary complexity, developers can create code that is more readable, maintainable, and reusable. How to Process? Depending on the language and the technology, there can be various ways to achieve code abstraction: Classes: Object-oriented programming (OOP) uses classes and objects to represent real-world entities and their behaviors. Classes encapsulate data and methods, providing a clear abstraction of the underlying data structures and operations. Interfaces or Abstract Classes: Interfaces define a contract that specifies the methods a class must implement. Abstract classes provide a partial implementation, allowing subclasses to provide specific details. Both mechanisms enable developers to work with generalized, abstract types rather than concrete implementations. Libraries: Breaking down a large codebase into modular components or libraries abstracts away the internal workings and provides a higher-level API for other parts of the system to interact with. Functions: Encapsulating a set of instructions into functions abstracts the internal logic and provides a higher-level interface for the rest of a program. Pros and Cons It is important to recognize that while abstraction offers benefits, there is always a price to be paid: Pros Cons Improved readability: Abstraction enhances code clarity and makes it easier to understand. Increased complexity: Introducing abstraction layers adds a certain level of complexity to the codebase. Modularity and maintainability: Well-abstracted code promotes modularity, enabling easier maintenance and future enhancements. Learning curve: Developers new to the codebase may need time to understand the abstractions and how they fit together. Encapsulation of complexity: Abstraction allows developers to manage complex logic in a more manageable and organized manner.   Refactoring Decisions Refactoring is the process of restructuring or improving existing code without changing its external behavior. It focuses on improving the internal structure, design, and overall quality of the codebase while preserving the functionality and expected output. The primary goal of refactoring is to make the code more readable, maintainable, and efficient. What should you Do? Code Organization: Refactoring involves rearranging code elements to improve its organization and readability. This includes renaming variables, classes, or functions to have more descriptive names, reordering code blocks for better flow, and grouping related code together. Code Duplication: Refactoring aims to eliminate code duplication by extracting common functionality into reusable functions or classes. By consolidating duplicated code, developers can improve code maintainability, reduce the chances of bugs, and enhance overall code quality. Simplification: Refactoring often involves simplifying complex or convoluted code. This can be achieved by removing unnecessary or redundant code, simplifying conditional statements, and reducing the overall complexity of algorithms or data structures. Performance Optimization: Refactoring can also be used to improve the performance of code. This includes identifying and replacing inefficient algorithms or data structures with more efficient alternatives, optimizing database queries, or improving memory management. Design Patterns: Refactoring often involves applying design patterns to improve the overall architecture and structure of the code. This includes identifying opportunities to use creational, structural, or behavioral design patterns to enhance code modularity, reusability, and maintainability. Unit Testing: Refactoring should be done in conjunction with a comprehensive suite of unit tests. By ensuring that the existing tests pass after each refactoring step, developers can confidently make changes without introducing regressions or breaking the existing functionality. It is this very powerful and useful concept that underlies TDD. Refactoring is an iterative process that should be performed regularly throughout the development lifecycle. It helps developers maintain a clean, well-structured codebase, making it easier to understand, modify, and extend. By continuously refactoring code, developers can avoid the accumulation of technical debt, improve overall productivity, and create a solid foundation for future development. Pros and Cons It is crucial to acknowledge that while Refactoring Decisions offer advantages, there are always associated costs to consider: Pros Cons Improved code quality: Refactoring eliminates duplication, improves readability, and enhances maintainability. Time and effort: Refactoring requires dedicated time and effort, which can delay feature delivery. Bug prevention: Refactoring can help identify and eliminate potential bugs before they manifest. Risk of introducing bugs: Inexperienced refactoring may introduce bugs if not done carefully. Scalability and extensibility: Well-refactored code is more scalable and easier to extend. Balancing priorities: Refactoring decisions must consider the trade-off between short-term feature delivery and long-term code health. Implementation Choices Developers face numerous decisions regarding algorithms, data structures, libraries, and frameworks throughout the software development process. These decisions play a crucial role in determining the efficiency, scalability, and maintainability of the software solution. What should it be Based on? Algorithms: Developers need to choose appropriate algorithms to solve specific problems efficiently. They consider factors such as time complexity, space complexity, and the nature of the problem itself. They evaluate different algorithmic approaches, analyze their pros and cons, and select the one that best suits the requirements of the project. Data Structures: Selecting the right data structure is essential for efficient data management and manipulation. Developers consider factors like the type of data, required operations (insertion, retrieval, deletion), memory usage, and access patterns. They choose data structures such as arrays, linked lists, hash tables, trees, or graphs to ensure optimal performance and ease of implementation. Libraries: Developers often rely on external libraries to leverage pre-existing functionality and save development time. They evaluate available libraries based on factors like community support, documentation, performance, security, and compatibility with their programming language or framework. Choosing reliable and well-maintained libraries can accelerate development, provide robust features, and enhance the overall quality of the software. Frameworks: Frameworks provide a structured approach to building applications and offer ready-to-use components, tools, and conventions. Developers must decide on the most suitable framework for their project, considering factors like the programming language, project requirements, scalability, ease of maintenance, community support, and learning curve. The choice of framework can significantly impact development speed, code organization, and long-term project sustainability. Integration and Interoperability: When developing complex systems, developers face decisions about integrating various components or third-party services. They evaluate integration options, APIs, and protocols to ensure seamless communication between different parts of the system. They also consider interoperability with existing systems, ensuring data exchange, compatibility, and smooth collaboration between different technologies. Performance and Scalability: Developers must make decisions that optimize performance and scalability. This includes selecting efficient algorithms and data structures, utilizing caching mechanisms, optimizing database queries, and considering distributed computing or parallel processing techniques. They also evaluate the scalability potential of libraries and frameworks to ensure the software can handle increasing workloads or user demands. Trade-offs and Constraints: Developers often face trade-offs and constraints when making these decisions. They consider factors like project deadlines, budget limitations, team expertise, maintenance costs, and compatibility with existing codebases. They weigh the benefits and drawbacks of different options to make informed decisions that align with project goals and constraints. Based on multiple parameters, it is evident that making a choice is not simple. It is a combination of technical expertise, understanding project requirements, and knowledge of current industry trends. This involves careful evaluation, experimentation, and also making trade-offs. Pros and Cons It is essential to recognize that while Implementation Choices provide benefits, there are always accompanying costs to be taken into account: Pros Cons Performance optimization: Choosing efficient algorithms and data structures can enhance the feature’s performance. Learning curve: Adopting new technologies or libraries may require additional learning time. Utilizing existing resources: Leveraging well-established libraries and frameworks can save development time and effort. Integration challenges: Third-party libraries or frameworks may introduce integration complexities. Future-proofing: Selecting technologies with active communities and long-term support helps ensure ongoing maintenance and updates. Balancing trade-offs: Some implementation choices may sacrifice performance for maintainability or vice versa. Unit Test Implementation and Coverage Unit testing plays a crucial role in ensuring code quality and preventing regressions in software development. It involves writing automated tests for individual units of code, such as functions, methods, or classes, to verify that they behave as expected. Why is it Essential? Code Quality Assurance: Unit tests act as a safety net, helping developers catch bugs and errors early in the development process. By writing tests that cover different scenarios and edge cases, developers can identify and fix issues before they impact the overall system. Unit testing promotes code quality by enforcing adherence to specifications, validating input/output behavior, and preventing the introduction of new bugs during code changes. Regression Prevention: Unit tests serve as a form of regression testing, ensuring that changes or additions to the codebase do not break existing functionality. By running unit tests after each code modification, developers can quickly detect any unintended side effects or regressions caused by the changes. This helps maintain stability and reliability in the software system, allowing developers to refactor or extend code with confidence. Documentation and Understanding: Unit tests serve as executable documentation for the codebase. By reading the tests, developers can understand the expected behavior and usage of various code units without diving into the implementation details. Unit tests also act as living examples that demonstrate how to interact with and utilize different parts of the codebase, facilitating collaboration among team members and making it easier to onboard new developers. Continuous Integration and Delivery: Unit testing plays a vital role in the continuous integration and delivery (CI/CD) process. By automating tests and incorporating them into the CI/CD pipeline, developers can ensure that code changes pass all relevant unit tests before being deployed to production. This helps maintain a stable and deployable codebase, reduces the risk of introducing bugs into the live environment, and enables faster and more reliable software releases. Refactoring and Code Maintainability: Unit tests provide developers with confidence when refactoring or modifying existing code. As long as the unit tests pass, developers can make changes to improve code structure, readability, or performance without worrying about unintended consequences. Unit tests act as a safety net, preventing code degradation during refactoring and ensuring that the modified code behaves correctly. Test-Driven Development (TDD): Unit testing is a fundamental aspect of the Test-Driven Development approach. TDD involves writing tests before writing the actual code. By focusing on test cases upfront, developers clarify the expected behavior and design of their code, leading to cleaner, more modular, and more maintainable codebases. Coverage Analysis: Unit testing allows developers to measure code coverage, which is the percentage of code that is exercised by tests. Code coverage analysis helps identify areas of the codebase that are not adequately covered by tests. By aiming for high code coverage, developers can ensure that a significant portion of the codebase is thoroughly tested, reducing the risk of undiscovered bugs and increasing confidence in the code’s reliability. Code coverage analysis acts as a metric for evaluating the effectiveness of unit testing efforts and provides insights into areas that may require additional test cases. In summary, unit testing is an essential practice in software development. It helps ensure code quality, prevents regressions, acts as living documentation, facilitates collaboration, supports continuous integration and delivery, and enables confident refactoring. By investing time and effort into writing comprehensive unit tests, developers can build robust, reliable, and maintainable software systems. Pros and Cons It is crucial to recognize that while implementing Unit Tests offers advantages, there are always associated costs to consider: Pros Cons Code quality assurance: Unit tests verify the correctness of individual components, reducing the likelihood of introducing bugs. Time and effort: Writing comprehensive unit tests requires additional time and effort. Improved maintainability: Well-tested code is easier to refactor, modify, and extend with confidence. Test maintenance: As the codebase evolves, unit tests may require updates to reflect changes. Documentation: Unit tests serve as living documentation, providing examples of how code should be used. False sense of security: High test coverage doesn’t guarantee bug-free code, and gaps in coverage can lead to undetected issues. Software Design Solutions Choosing appropriate software design solutions is crucial for ensuring long-term maintainability and extensibility of a software system. The design decisions made early in the development process can have a significant impact on the ease of future maintenance and the ability to accommodate future changes and enhancements. Why is it Important? Code Understandability: A well-designed software system follows established design principles and patterns, making it easier for developers to understand and navigate the codebase. Clear and concise design solutions enhance code readability, reducing the time and effort required for maintenance tasks. By choosing appropriate design solutions, developers can create a codebase that is intuitive and easily comprehensible to both current and future development teams. Modularity and Reusability: Good software design solutions emphasize modularity and reusability. Modular designs break down the system into smaller, self-contained components that can be easily understood, modified, and reused. By selecting design solutions that promote modularity, developers can isolate and update specific parts of the system without affecting others. This enhances maintainability, as changes can be made in a targeted manner and shared functionality can be reused across different modules, leading to increased development efficiency. Scalability: Software systems often need to accommodate future growth and changes in requirements. Design solutions that support scalability allow the system to handle increased workloads, data volumes, and user demands without requiring major rework. By choosing appropriate design solutions, developers can ensure that the system can be extended and scaled without compromising its stability, performance, or maintainability. Reduced Technical Debt: Good design decisions help minimize technical debt, which refers to the accumulated cost of deferred maintenance or suboptimal design choices. By selecting appropriate design solutions, developers can avoid architectural flaws, overly complex designs, and shortcuts that may lead to increased technical debt. This reduces the need for extensive rework in the future, improves code quality, and makes it easier to maintain and extend the system over time. Ease of Collaboration: Choosing suitable design solutions promotes better collaboration among team members. When a software system follows consistent design patterns and principles, it becomes easier for developers to communicate and understand each other’s work. It facilitates collaboration, knowledge sharing, and smoother integration of code contributions from multiple team members, leading to a more cohesive and efficient development process. In summary, selecting appropriate software design solutions is essential for achieving long-term maintainability and extensibility. It improves code understandability, promotes modularity and reusability, supports scalability, enhances flexibility and adaptability, reduces technical debt, and facilitates collaboration among developers. By investing time and effort into thoughtful design decisions, developers can build software systems that are easier to maintain, extend, and adapt to future requirements. Pros and Cons Pros Cons Scalability and extensibility: Well-designed software architecture allows for easy addition of new features and enhancements. Time and complexity: Designing a robust architecture may require more time upfront and can introduce additional complexity. Modularity and Reusability: Well-designed software solutions promote modularity, allowing the system to be divided into independent and reusable components. Over-engineering: Overly complex designs can be difficult to understand and maintain, especially for small or straightforward features. Maintainability: Effective software design solutions contribute to the maintainability of a system. They promote clean code organization, proper encapsulation, and separation of concerns, making it easier to understand and update the codebase over time. Balancing trade-offs: Design decisions often involve trade-offs between flexibility, performance, and simplicity. Testability: Good software design solutions facilitate testability by promoting loose coupling and dependency injection.   Conclusion As software developers, the choices we make when implementing a new feature have a profound impact on the overall success of a project. By carefully evaluating the benefits and drawbacks of code abstraction, refactoring decisions, implementation choices, unit test implementation and coverage, and software architecture, you must strike the right balance between delivering short-term features and ensuring long-term code quality. It is important to remember that there is no universal solution, and the best choice may vary depending on the specific project requirements, timeline, and team dynamics. Engaging in constructive discussions with your team, gathering feedback, and making informed decisions based on project goals and constraints are essential. This last point is often the most challenging to implement, as everyone has their own perspective based on their experience and preferences. Another crucial point to remember is that there is always a significant cost associated with writing clean code, and too often, the priority of adding new features overshadows the work of refactoring and resolving technical debt, which is normal in an iterative development process. By advocating for these viewpoints, you as developers can contribute to the creation of well-designed, maintainable, and high-quality software that meets user expectations and promotes the overall success of a project."
    } ,
  
    {
      "title"    : "DevOps Coach and DevOps Engineer: Understanding their Roles in DevOps Transformation",
      "url"      : "/en/coach-devops-vs-engineer-devops/",
      "img"      : "PHBpY3R1cmU+CiAgPHNvdXJjZSBzcmNzZXQ9Ii9hc3NldHMvaW1nL2NvYWNoLWRldm9wcy12cy1lbmdpbmVlci1kZXZvcHMtMzAwLTNjNWMzNjY1ZS5hdmlmIDEuMHgsIC9hc3NldHMvaW1nL2NvYWNoLWRldm9wcy12cy1lbmdpbmVlci1kZXZvcHMtNjAwLTNjNWMzNjY1ZS5hdmlmIDIuMHgiIHR5cGU9ImltYWdlL2F2aWYiPgogIDxzb3VyY2Ugc3Jjc2V0PSIvYXNzZXRzL2ltZy9jb2FjaC1kZXZvcHMtdnMtZW5naW5lZXItZGV2b3BzLTMwMC1jNjJiMThjYzguanBnIDEuMHgsIC9hc3NldHMvaW1nL2NvYWNoLWRldm9wcy12cy1lbmdpbmVlci1kZXZvcHMtNjAwLWM2MmIxOGNjOC5qcGcgMi4weCIgdHlwZT0iaW1hZ2UvanBlZyI+CiAgPGltZyBzcmM9Ii9hc3NldHMvaW1nL2NvYWNoLWRldm9wcy12cy1lbmdpbmVlci1kZXZvcHMtMzAwLWM2MmIxOGNjOC5qcGciIGFsdD0iRGV2T3BzIENvYWNoIGFuZCBEZXZPcHMgRW5naW5lZXI6IFVuZGVyc3RhbmRpbmcgdGhlaXIgUm9sZXMgaW4gRGV2T3BzIFRyYW5zZm9ybWF0aW9uIj4KPC9waWN0dXJlPgo=",
      "isencoded" : "y",
      "tags"     : "DevOps, Coach, Engineer, Transformation, Responsibility",
      "content"  : "In the world of IT, the DevOps methodology has become increasingly popular due to its advantages in terms of collaboration, efficiency, and deployment speed. Two key roles are emerging within organizations that seek to adopt DevOps: the DevOps coach and the DevOps engineer. While their responsibilities may overlap, they have distinct roles that contribute in a complementary manner to the success of an organization’s DevOps transformation. The Role of the DevOps Coach Promoting DevOps Culture Training and Awareness Change Facilitation Cross-Functional Collaboration Assessment and Continuous Improvement The Role of the DevOps Engineer Infrastructure Automation Management of DevOps Tools Security and Compliance Troubleshooting and Incident Management Performance Optimization Integration of DevOps Coach and DevOps Engineer in DevOps Transformation Close Collaboration Training and Awareness Change Management Measurement and Improvement Conclusion The Role of the DevOps Coach The DevOps coach is a professional responsible for guiding, training, and supporting teams in the adoption of DevOps principles and practices. Their main role is to create an environment conducive to collaboration and continuous improvement. Here are the key responsibilities of a DevOps coach: Promoting DevOps Culture The DevOps coach is responsible for disseminating the DevOps culture within the organization. They help teams understand the core values of DevOps, such as collaboration, transparency, shared responsibility, and customer orientation. Training and Awareness The DevOps coach provides training and awareness to teams on DevOps practices and tools. They help strengthen the skills and knowledge of team members to facilitate the adoption of DevOps. Change Facilitation DevOps coaches facilitate transition and change by working closely with teams to identify obstacles, solve problems, and foster acceptance of new practices. Cross-Functional Collaboration An essential aspect of DevOps is close collaboration between development and operations teams. DevOps coaches facilitate this collaboration by encouraging communication, eliminating organizational silos, and promoting collective decision-making. Assessment and Continuous Improvement DevOps coaches assess the organization’s DevOps performance, identify areas for improvement, and recommend adjustments. They use metrics and indicators to evaluate the effectiveness of the DevOps transformation. The Role of the DevOps Engineer The DevOps engineer is a technical expert who focuses on setting up and managing the infrastructure and tools required for practicing DevOps. Here is an overview of the responsibilities of a DevOps engineer: Infrastructure Automation The DevOps engineer designs and implements automated infrastructures, such as development environments, deployment systems, and continuous integration and continuous deployment pipelines. They use automation tools to enhance the efficiency and reproducibility of deployments. Management of DevOps Tools The DevOps engineer is responsible for selecting, configuring, and managing the tools and technologies used in the DevOps ecosystem. This may include version control tools, automated deployment tools, monitoring tools, and containerization platforms. Security and Compliance The DevOps engineer incorporates security and compliance practices throughout the lifecycle of applications and infrastructures. They ensure that appropriate security controls are in place and compliance standards are met. Troubleshooting and Incident Management As a technical expert, the DevOps engineer plays a crucial role in troubleshooting and managing incidents related to infrastructures and DevOps tools. They are responsible for diagnosing and resolving operational issues to ensure optimal availability and performance. Performance Optimization The DevOps engineer works towards continuous improvement of system performance and DevOps processes. They perform performance analysis, identify bottlenecks, and implement solutions to optimize deployment times, scalability, and overall infrastructure efficiency. Integration of DevOps Coach and DevOps Engineer in DevOps Transformation The DevOps coach and the DevOps engineer work together to facilitate DevOps transformation within an organization. Their roles complement each other to achieve the goals of the DevOps transition. Here’s how they integrate: Close Collaboration The DevOps coach and the DevOps engineer work closely together to align processes, tools, and practices with the DevOps culture. They share their respective expertise to ensure a smooth and effective adoption of DevOps. Training and Awareness The DevOps coach trains teams on DevOps principles and practices, while the DevOps engineer provides specific technical training on DevOps tools and technologies. Together, they create a comprehensive learning environment for team members. Change Management The DevOps coach facilitates cultural and organizational change, while the DevOps engineer ensures that technical changes are smoothly implemented. They work together to minimize disruptions and address issues that arise during the transformation. Measurement and Improvement The DevOps coach defines metrics and indicators to assess the progress of the DevOps transformation, while the DevOps engineer provides the technical data needed to measure performance and identify improvement opportunities. Conclusion In conclusion, the DevOps coach and the DevOps engineer are key players in DevOps transformation. While the DevOps coach focuses on culture, processes, and organizational change, the DevOps engineer brings technical expertise to implement the necessary infrastructures and tools. Their close collaboration combines the human and technical aspects of DevOps, thus fostering a successful transformation. Together, the DevOps coach and the DevOps engineer guide teams towards a culture of collaboration, automation, and continuous improvement. The DevOps coach plays a vital role in adopting DevOps principles, training teams, and creating an environment conducive to change. On the other hand, the DevOps engineer focuses on process automation, tool and infrastructure management, as well as technical performance optimization. It is important to recognize that the success of a DevOps transformation relies on the collaboration and coordination between these two roles. Together, they support teams in adopting DevOps, foster a culture of collaboration and continuous improvement, and contribute to accelerating development and deployment processes. By integrating the skills and contributions of the DevOps coach and the DevOps engineer, organizations can fully leverage the benefits of the DevOps methodology to achieve faster, more reliable, and more innovative outcomes."
    } ,
  
    {
      "title"    : "Supercharge your DevOps: Proven Practices to Avoid Sinking",
      "url"      : "/en/dev-ops-collaboration/",
      "img"      : "PHBpY3R1cmU+CiAgPHNvdXJjZSBzcmNzZXQ9Ii9hc3NldHMvaW1nL2Rldi1vcHMtY29sbGFib3JhdGlvbi0zMDAtZWZlN2NlZjM5LmF2aWYgMS4weCwgL2Fzc2V0cy9pbWcvZGV2LW9wcy1jb2xsYWJvcmF0aW9uLTYwMC1lZmU3Y2VmMzkuYXZpZiAyLjB4IiB0eXBlPSJpbWFnZS9hdmlmIj4KICA8c291cmNlIHNyY3NldD0iL2Fzc2V0cy9pbWcvZGV2LW9wcy1jb2xsYWJvcmF0aW9uLTMwMC0xMjIxNzJkYTEuanBnIDEuMHgsIC9hc3NldHMvaW1nL2Rldi1vcHMtY29sbGFib3JhdGlvbi02MDAtMTIyMTcyZGExLmpwZyAyLjB4IiB0eXBlPSJpbWFnZS9qcGVnIj4KICA8aW1nIHNyYz0iL2Fzc2V0cy9pbWcvZGV2LW9wcy1jb2xsYWJvcmF0aW9uLTMwMC0xMjIxNzJkYTEuanBnIiBhbHQ9IlN1cGVyY2hhcmdlIHlvdXIgRGV2T3BzOiBQcm92ZW4gUHJhY3RpY2VzIHRvIEF2b2lkIFNpbmtpbmciPgo8L3BpY3R1cmU+Cg==",
      "isencoded" : "y",
      "tags"     : "DevOps, Practices, Collaboration, IaC, SRE",
      "content"  : "DevOps goes beyond a mere methodology; it’s a culture of collaboration and efficiency that transforms how development and operations teams work together. Its goal is to create synergy between developers and operations for faster deployments, streamlined infrastructure management, and improved software quality. Let’s explore essential practices, indispensable tools, and crucial strategies to foster successful collaboration within your organization. Relevant Practices Establishing Common Goals and Objectives Fostering Cross-Functional Teams Encouraging Communication and Transparency Implementing Continuous Feedback Loops Automating and Standardizing Processes Netflix, a Forced Collaboration Etsy, Sharing through Chat Case Studies Amazon, the Proven Model Target, IT Serving the Business Specific Techniques Infrastructure as Code (IaC) Blameless Post-Mortems Site Reliability Engineering (SRE) Value Stream Mapping Conclusion Relevant Practices Establishing Common Goals and Objectives To ensure effective collaboration, it is crucial to align the goals and objectives of development and operations teams. By setting common goals, such as faster time to market, improved quality, and increased customer satisfaction, teams can converge towards a shared objective and achieve better results. Fostering Cross-Functional Teams DevOps encourages the formation of cross-functional teams, where members from both development and operations work together. By bringing together diverse skills and perspectives, cross-functional teams can collaborate effectively, share knowledge, and collectively address challenges. Encouraging Communication and Transparency Clear and frequent communication is key to successful collaboration. Development and operations teams should establish open and transparent communication channels, such as regular meetings, progress updates, and shared documentation. Tools like messaging platforms and issue tracking systems can also facilitate real-time communication and visibility into project progress. Implementing Continuous Feedback Loops Feedback loops play a crucial role in improving collaboration. Regular feedback sessions help identify areas for improvement, resolve conflicts, and celebrate successes. By fostering a culture of learning and continuous improvement, teams can strengthen their collaboration and enhance overall productivity. Automating and Standardizing Processes Automation and standardization of processes are essential to DevOps practices. By automating repetitive tasks and standardizing workflows, development and operations teams can streamline their collaboration. This allows each team member to focus on higher-value activities and reduces the risks of errors or miscommunication. Here are some examples of companies that have succeeded in their DevOps transformation. Netflix, a Forced Collaboration Netflix has a highly successful DevOps culture where development and operations teams collaborate seamlessly. They have developed a tool called “Chaos Monkey” to intentionally introduce failures into their systems, allowing both teams to work together to quickly identify and resolve issues, thereby improving both team collaboration and system reliability. Etsy, Sharing through Chat Etsy, an e-commerce platform, practices “ChatOps” to foster collaboration. They use chat platforms like Slack to enable real-time communication between development and operations teams. This approach helps maintain transparency, quickly resolve issues, and facilitate knowledge sharing. Case Studies Here are some additional examples that highlight the potential of effective collaboration in a DevOps transformation. Amazon, the Proven Model Amazon’s migration to a DevOps culture has led to significant improvements. By removing silos between teams, they have managed to accelerate deployment cycles and reduce downtime. The collaboration efforts between development and operations teams have enhanced the scalability, availability, and performance of their systems. Target, IT Serving the Business Target, the retail giant, has implemented a DevOps approach to improve collaboration among their teams. By establishing common goals and enabling continuous feedback loops, they have successfully accelerated delivery cycles and improved software quality. Their collaborative practices have resulted in increased customer satisfaction and better alignment with business objectives. Specific Techniques Here are concrete examples of methods and techniques that promote effective collaboration, process improvement, and optimized outcomes. Infrastructure as Code (IaC) Infrastructure as Code (IaC) accelerates infrastructure management and provisioning by replacing manual processes with code. This approach enables efficient, reproducible, and collaborative infrastructure management, offering significant benefits in speed, reliability, and consistency. Blameless Post-Mortems Conducting blameless post-mortems after incidents or failures encourages open and honest communication. This approach helps teams focus on learning from mistakes rather than assigning blame. It fosters collaboration, continuous improvement, and shared accountability. Site Reliability Engineering (SRE) Site Reliability Engineering (SRE) principles advocate for close collaboration between development and operations teams. SRE teams work alongside development teams to ensure system reliability, performance, and scalability. This collaboration facilitates proactive monitoring, incident management, and ongoing optimization. Value Stream Mapping Value stream mapping is a technique used to visualize and analyze the end-to-end software delivery process. By involving both development and operations teams in the mapping exercise, teams can identify bottlenecks, streamline workflows, and optimize collaboration across the entire value stream. Conclusion Effective collaboration between development and operations teams is crucial for successful DevOps implementation. By establishing common goals, fostering cross-functional teams, promoting communication and transparency, implementing continuous feedback loops, and automating processes, organizations can achieve improved collaboration, faster delivery cycles, and better overall outcomes in their DevOps initiatives. Without it, any DevOps practice is destined to fail."
    } ,
  
    {
      "title"    : "Functional Programming in Bash: Harnessing the Power of Simplicity",
      "url"      : "/en/bash-functional-programming/",
      "img"      : "PHBpY3R1cmU+CiAgPHNvdXJjZSBzcmNzZXQ9Ii9hc3NldHMvaW1nL2Z1bmN0aW9uYWwtcHJvZ3JhbW1pbmctMzAwLWFmYzY0YzYwMy5hdmlmIDEuMHgsIC9hc3NldHMvaW1nL2Z1bmN0aW9uYWwtcHJvZ3JhbW1pbmctNjAwLWFmYzY0YzYwMy5hdmlmIDIuMHgiIHR5cGU9ImltYWdlL2F2aWYiPgogIDxzb3VyY2Ugc3Jjc2V0PSIvYXNzZXRzL2ltZy9mdW5jdGlvbmFsLXByb2dyYW1taW5nLTMwMC1lNTE3ODRhNWMuanBnIDEuMHgsIC9hc3NldHMvaW1nL2Z1bmN0aW9uYWwtcHJvZ3JhbW1pbmctNjAwLWU1MTc4NGE1Yy5qcGcgMi4weCIgdHlwZT0iaW1hZ2UvanBlZyI+CiAgPGltZyBzcmM9Ii9hc3NldHMvaW1nL2Z1bmN0aW9uYWwtcHJvZ3JhbW1pbmctMzAwLWU1MTc4NGE1Yy5qcGciIGFsdD0iRnVuY3Rpb25hbCBQcm9ncmFtbWluZyBpbiBCYXNoOiBIYXJuZXNzaW5nIHRoZSBQb3dlciBvZiBTaW1wbGljaXR5Ij4KPC9waWN0dXJlPgo=",
      "isencoded" : "y",
      "tags"     : "FP, Bash, DevOps, Scripting",
      "content"  : "Bash, the command-line shell and scripting language commonly used in Unix-based systems, is primarily known for its imperative and procedural style. However, with a little creativity, it is possible to apply the principles of functional programming to write elegant and powerful scripts in Bash. In this article, we will explore how the concepts of functional programming can be utilized in Bash scripts, enabling cleaner code, improved modularity, and increased readability. Reminder of Functional Programming Concepts Pure Functions Immutability of Data Higher-Order Functions Recursion Function Composition Decomposition into Smaller Functions Lazy Evaluation Basic Functions in a Functional Language Advantages of Functional Programming in Bash Scripts Functional Programming Concepts Applied to Bash Pure Functions in Bash Data Immutability in Bash Higher-Order Functions in Bash Recursion in Bash Function Composition in Bash Lazy Evaluation in Bash Basic Function of Functional Programming in Bash The Map Function The Filter Function The Reduce Function The Zip Function Conclusion Reminder of Functional Programming Concepts Functional programming is a programming paradigm based on the use of functions in the mathematical sense as a central element of software development. Here are some key concepts of functional programming. Pure Functions Pure functions are functions (in the mathematical sense) that do not modify the global state and always produce the same result for the same inputs. They have no undesirable side effects, making them predictable and easy to understand. Immutability of Data Immutability is the principle that data cannot be changed once it is created. Instead, new data is created during transformations. This avoids side effects and facilitates reasoning about the behavior of functions. Higher-Order Functions Higher-order functions are functions that can take other functions as arguments or return them as results. They enable powerful abstraction and code reuse by allowing the manipulation of functions as first-class values. Recursion Recursion is a technique where a function calls itself to solve a problem iteratively instead of using loops. It allows solving complex problems by breaking them down into smaller, repetitive problems. Recursion is often used for traversing data structures. Function Composition Function composition involves combining multiple functions to form new, more complex functions. This allows the creation of data processing pipelines where the output of one function becomes the input of the next one. Function composition promotes a modular and declarative approach to development. Decomposition into Smaller Functions Functional programming encourages the decomposition of complex problems into smaller, specialized functions. This promotes code reuse, improves readability, and facilitates maintenance. Lazy Evaluation This is another key concept of functional programming. Lazy evaluation is an approach where expressions are only evaluated when their value is actually needed. This helps save resources by avoiding the evaluation of unnecessary expressions. In summary, these key concepts of functional programming allow for the creation of more readable, predictable, modular, and reusable code. They promote a declarative approach to development, focusing on the “what” rather than the “how” of the code. Basic Functions in a Functional Language The basic functions in a functional language may vary from one language to another, but there are generally a few commonly used functions in most functional languages. Moreover, most of these functions are higher-order functions, meaning they can take other functions as parameters and/or return functions as results. Here are some examples of basic functions: map: Applies a function to each element of a list or a similar data structure, returning a new list containing the results. filter: Filters the elements of a list based on a condition specified by a function, returning a new list that only contains the elements satisfying the condition. reduce (or fold): Combines the elements of a list by applying a cumulative operation. For example, summing, multiplying, or concatenating. zip: Combines two or more lists into a list of pairs, taking one element from each list at a time. curry: Transforms a function that takes multiple arguments into a sequence of functions, with each function accepting only one argument at a time. compose: Allows the composition of multiple functions together to form a new function. The outputs of one function become the inputs of the next function. These basic functions enable functional data manipulation, avoiding the use of loops and mutable variables, for example. Therefore, if we manage to implement such functions in Bash, we should be able to program following functional concepts. Advantages of Functional Programming in Bash Scripts Functional programming brings several advantages to programming in Bash, which is commonly used following the procedural paradigm: Improved Readability: By focusing on concise, self-contained, and single-purpose functions, functional programming promotes code that is easier to read and understand, making it more maintainable over time. Modularity and Reusability: Functions in functional programming are designed to be composable and reusable. This allows you to build complex scripts by combining smaller, self-contained functions, promoting code modularity and reusability. Fewer Side Effects: Functional programming discourages the use of mutable state and encourages immutability. This reduces the likelihood of introducing side effects, greatly facilitating testing and understanding of the scripts. Functional Programming Concepts Applied to Bash Pure Functions in Bash Pure functions are the foundation of functional programming. They take input parameters and produce an output without any side effects. In Bash, we can create pure functions by ensuring that they only use input parameters and local variables, without modifying the global state or relying on external dependencies. Example: #!/bin/bash # Pure function to calculate the square of a number square() { local num=$1 echo $((num * num)) } # Example usage result=$(square 2) echo &quot;$result&quot; # Output: 4 Bash In the example above, the square function is a pure function that calculates the square of a number. It takes an input parameter, num, and returns the squared value without modifying any global state. This demonstrates the concept of pure functions in Bash. Data Immutability in Bash Immutability means that data cannot be modified once it is created. In Bash, this can be achieved by avoiding direct modifications of existing variables and instead favoring the creation of new variables during transformations. It remains the responsibility of the developer to enforce immutability. In Bash, the local keyword is commonly used to declare a local variable within a function. This is a common practice in functional programming to avoid side effects and maintain data encapsulation. The -r option can be used to define an immutable variable, which is essentially a constant. Let’s see an example of its usage: #!/bin/bash my_function() { local var=&quot;Local&quot; local -r read_only_var=&quot;Read-only&quot; var=&quot;Modified&quot; # Modifying a local variable read_only_var=&quot;Modified&quot; # Attempting to modify an immutable variable } my_function # Output: bash: read_only_var: readonly variable Bash In this example, the variable declared with local is not immutable, while the one declared with local -r is immutable. Higher-Order Functions in Bash Higher-order functions take one or more functions as input parameters or return a function as output. In Bash, we can pass functions as arguments or store them in variables, enabling us to create higher-order functions. Example: #!/bin/bash # Higher-order function to apply a given function to each element of an array map() { local func=$1 local array=(&quot;${@:2}&quot;) local result=() for element in &quot;${array[@]}&quot;; do result+=(&quot;$(&quot;$func&quot; &quot;$element&quot;)&quot;) done echo &quot;${result[@]}&quot; } # Example usage square() { local num=$1 echo $((num * num)) } array=(1 2 3 4 5) result=($(map square &quot;${array[@]}&quot;)) echo &quot;${result[@]}&quot; # Output: 1 4 9 16 25 Bash Recursion in Bash Recursion is a powerful technique in functional programming. Bash, although not optimized for recursion, can still handle it effectively for certain use cases. However, since recursion in Bash can be resource-intensive, it is important to be mindful of the algorithm’s complexity. Recursion allows you to solve problems by breaking them down into smaller problems, resulting in more concise and expressive code. Example: #!/bin/bash # Recursive function to calculate the factorial of a number factorial() { local num=$1 if ((num &amp;lt;= 1)); then echo 1 else local sub_factorial=$(factorial $((num - 1))) echo $((num * sub_factorial)) fi } # Example usage echo $(factorial 5) # Output: 120 Bash Function Composition in Bash Composition is a fundamental concept in functional programming that involves combining multiple functions to create a new function. The idea is to take the result of one function and use it as the input for another function, forming a chain of transformations. This allows breaking down a complex problem into smaller, simpler steps and seamlessly connecting them. Example: #!/bin/bash # Function 1: Convert text to uppercase to_uppercase() { echo &quot;$1&quot; | tr &#39;[:lower:]&#39; &#39;[:upper:]&#39; } # Function 2: Add a prefix to the text add_prefix() { echo &quot;Prefix $1&quot; } # Function 3: Display the final text display_text() { echo &quot;Final text: $1&quot; } # Composition of functions compose_functions() { local result=&quot;$1&quot; shift for func in &quot;$@&quot;; do result=&quot;$($func &quot;$result&quot;)&quot; done echo &quot;$result&quot; } # Using function composition text=&quot;example text&quot; result=$(compose_functions &quot;$text&quot; to_uppercase add_prefix display_text) echo &quot;$result&quot; # Output: Final text: Prefix EXAMPLE TEXT Bash Lazy Evaluation in Bash In Bash, although it is not a native feature of the language, it is possible to adopt a simple approach to simulate lazy evaluation: using generator functions. Instead of generating and storing all the values of a sequence, we can generate the values on-demand, when they are needed, by calling the function. Example of lazy evaluation: #!/bin/bash # Lazy function: Calculates and returns the list of even numbers up to a certain threshold get_even_numbers_lazy() { local threshold=$1 local numbers=() local current=0 while (( current &amp;lt; threshold )); do numbers+=($current) current=$((current + 2)) done echo &quot;${numbers[@]}&quot; } # Using the lazy function numbers=$(get_even_numbers_lazy 10) echo &quot;Even numbers up to 10: ${numbers[@]}&quot; # Output: Even numbers up to 10: 0 2 4 6 8 Bash Basic Function of Functional Programming in Bash The Map Function #!/bin/bash # Definition of the map function map() { local -n input_list=$1 local -r transform_fn=$2 local mapped_list=() for element in &quot;${input_list[@]}&quot;; do mapped_list+=(&quot;$(&quot;$transform_fn&quot; &quot;$element&quot;)&quot;) done echo &quot;${mapped_list[@]}&quot; } # Example transformation function square() { local input=$1 echo &quot;$((input * input))&quot; } declare -a my_list=(1 2 3 4 5) mapped_list=$(map my_list square) # Display the result echo &quot;Original list: ${my_list[@]}&quot; echo &quot;Transformed list: ${mapped_list[@]}&quot; # Output: Original list: 1 2 3 4 5 # Output: Transformed list: 1 4 9 16 25 Bash The Filter Function #!/bin/bash # Definition of the filter function filter() { local -n input_list=$1 local -r predicate=$2 local filtered_list=() for element in &quot;${input_list[@]}&quot;; do if &quot;$predicate&quot; &quot;$element&quot;; then filtered_list+=(&quot;$element&quot;) fi done echo &quot;${filtered_list[@]}&quot; } # Example filtering function is_even() { local input=$1 ((input % 2 == 0)) } declare -a my_list=(1 2 3 4 5) filtered_list=$(filter my_list is_even) # Display the result echo &quot;Original list: ${my_list[@]}&quot; echo &quot;Filtered list (even elements): ${filtered_list[@]}&quot; # Output: Original list: 1 2 3 4 5 # Output: Filtered list (even elements): 2 4 Bash The Reduce Function #!/bin/bash # Definition of the reduce function reduce() { local -n input_list=$1 local -r accumulate_fn=$2 local initial_value=$3 local accumulator=$initial_value for element in &quot;${input_list[@]}&quot;; do accumulator=&quot;$(&quot;$accumulate_fn&quot; &quot;$accumulator&quot; &quot;$element&quot;)&quot; done echo &quot;$accumulator&quot; } # Example aggregation function sum() { local accumulator=$1 local element=$2 echo &quot;$((accumulator + element))&quot; } declare -a my_list=(1 2 3 4 5) result=$(reduce my_list sum 0) # Display the result echo &quot;Original list: ${my_list[@]}&quot; echo &quot;Reduction result (sum): $result&quot; # Output: Original list: 1 2 3 4 5 # Output: Reduction result (sum): 15 Bash The Zip Function #!/bin/bash # Definition of the zip function zip() { local -n input_list1=$1 local -n input_list2=$2 local zipped_list=() local length=${#input_list1[@]} for ((i=0; i&amp;lt;length; i++)); do zipped_list+=(&quot;${input_list1[$i]},${input_list2[$i]}&quot;) done echo &quot;${zipped_list[@]}&quot; } # Example usage declare -a list1=(&quot;a&quot; &quot;b&quot; &quot;c&quot;) declare -a list2=(&quot;x&quot; &quot;y&quot; &quot;z&quot;) zipped_list=$(zip list1 list2) # Display the result echo &quot;List 1: ${list1[@]}&quot; echo &quot;List 2: ${list2[@]}&quot; echo &quot;Zipped list: ${zipped_list[@]}&quot; # Output: List 1: a b c # Output: List 2: x y z # Output: Zipped list: a,x b,y c,z Bash Conclusion Indeed, although Bash is primarily an imperative language, functional programming concepts can be effectively applied to write cleaner and more modular scripts. By leveraging pure functions, higher-order functions, and recursion, you can harness the simplicity and power of functional programming within the Bash scripting environment. So, the next time you write a Bash script, consider applying the principles of functional programming to enhance your code."
    } ,
  
    {
      "title"    : "AWS Streaming - Amazon Kinesis",
      "url"      : "/en/aws-streaming-kinesis/",
      "img"      : "PHBpY3R1cmU+CiAgPHNvdXJjZSBzcmNzZXQ9Ii9hc3NldHMvaW1nL2F3cy1zdHJlYW1pbmcta2luZXNpcy1kb2N1bWVudGF0aW9uLTMwMC04YWE3YTBiNDcuYXZpZiAxLjB4LCAvYXNzZXRzL2ltZy9hd3Mtc3RyZWFtaW5nLWtpbmVzaXMtZG9jdW1lbnRhdGlvbi02MDAtOGFhN2EwYjQ3LmF2aWYgMi4weCIgdHlwZT0iaW1hZ2UvYXZpZiI+CiAgPHNvdXJjZSBzcmNzZXQ9Ii9hc3NldHMvaW1nL2F3cy1zdHJlYW1pbmcta2luZXNpcy1kb2N1bWVudGF0aW9uLTMwMC1mMzhkMGRhM2EuanBnIDEuMHgsIC9hc3NldHMvaW1nL2F3cy1zdHJlYW1pbmcta2luZXNpcy1kb2N1bWVudGF0aW9uLTYwMC1mMzhkMGRhM2EuanBnIDIuMHgiIHR5cGU9ImltYWdlL2pwZWciPgogIDxpbWcgc3JjPSIvYXNzZXRzL2ltZy9hd3Mtc3RyZWFtaW5nLWtpbmVzaXMtZG9jdW1lbnRhdGlvbi0zMDAtZjM4ZDBkYTNhLmpwZyIgYWx0PSJBV1MgU3RyZWFtaW5nIC0gQW1hem9uIEtpbmVzaXMiPgo8L3BpY3R1cmU+Cg==",
      "isencoded" : "y",
      "tags"     : "AWS, Kinesis, Streams, Firehose, Analytics, Documentation",
      "content"  : "Amazon Kinesis makes it easy to collect, process and analyze data, streaming and real-time, so you can react quickly. It is capable of integrating real-time data, such as video, audio, application logs, website click streams, or IoT telemetry data. Amazon Kinesis allows you to process and analyze data as it is received and react instantly instead of waiting for all data to be collected to start processing. There are different Kinesis services depending on your need: Amazon Kinesis Data Streams (KDS): continuously captures gigabytes of data per second and makes them available on Amazon S3 or AWS Lambda Functions Amazon Kinesis Data Firehose: Reliably loads streaming data into data lakes, data stores and analytics services Amazon Kinesis Data Analytics: transforms and analyzes live streaming data with SQL and Apache Flink Amazon Kinesis Video Streams: securely streams video to AWS Let’s look at these services in detail. Amazon Kinesis Data Streams General Architecture Records Fan-Out Security Kinesis Procucers ProvisionedThroughputExceed error Records Kinesis Consumers Shared vs Enhanced Kinesis Client Library (KCL) Sizing of Shards Shard Splitting Shard Merging Kinesis Data Firehose Summary Kinesis Data Analytics Amazon Kinesis Data Streams General Architecture It is composed of Shards whose number must be initiated at creation The number of Shards defines the input and output flow of the Streams Message retention can range from 1 to 365 Days The messages are immutable, they cannot be erased which allows to replay the data if needed Main components of Amazon Kinesis Data Streams One or more Producers send Records to Kinesis Data Streams The Kinesis Data Streams is a set of Shards, a Shard being a single sequence of data in a stream In Write, each Shard supports up to 1,000 records/sec and up to 1 MB of data/sec In Reading, this depends on the Consumer Mode: Shared: flow shared by ALL Consumers of 2 MB/sec by Shard Enhanced: flow rate for EACH Consumer of 2 MB/sec by Shard Records Fan-Out Allocation of Records in the Shards A Producer generates a Record with a Partition Key and content of 1MB max Based on the *Record** MD5 Hash, it is directed to one of the Shards The Record receives an additional field, the Sequence Number, indicating its order of passage in the Shard The Consumer receives the Record according to the delivery method Shared or Enhanced Fan-Out Hot PartitionTo avoid Shard receiving all Records, you must ensure that the Partition Key values are distributed! Security Access control and permissions by IAM Policies Encryption in transit by HTTPS and rest by KMS Kinesis Data Streams can be accessed through a VPC Endpoint, ensuring private communication, without going through the Internet Kinesis API calls are logged in CloudTrail Kinesis Procucers Producers send data to Kinesis Data Streams as Records They may be: An application A Client Mobile or Desktop An application using AWS SDK (Low Level API) or KPL (Kinesis Producer Library, High Level API with batch, compression, retries) A Kinesis Agent installed on a server that sends, for example, logs Using Batch with the PutRecords API reduces costs and increases throughput ProvisionedThroughputExceed error When a Shard receives more data than it can process, an error ProvisionedThroughputExceed is returned to the Producer. In order to guard against this, it is necessary to: Make sure to use a distributed value of Partition Key Implement a Retry mechanism with an exponential backoff Increase number of Shards Records It is made of 3 elements: The Partition Key: defined by the Producer, it determines in which Shard will pass the record (according to its Hash) The Data Blob: the content of 1 MB message at most The Sequence Number: defined in Kinesis Data Streams, it indicates the pass order in the Shard Kinesis Consumers Consumers receive data from Kenesis Data Streams as Records They may be: An application using the AWS SDK or the KCL API (for Kinesis Client Library) A Lambda Function Kinesis Data Firehose Kinesis Data Analytics SDK vs KCL The AWS SDK is a low-level API that requires more implementation than the API KCL of high-level that implements mechanisms of Retry, checking of Records processed, reacting to Resharding, … These 2 APIs take into account the Shared and Enhanced modes, however note that, for KCL, only the V2 version supports them Shared vs Enhanced The table below summarizes the key differences between the Shared and Enhanced modes of a Consumer: Feature Shared fan-out Enhanced fan-out Read rate of a Shard A maximum of 2 MB/s, spread over all Consumers A maximum of 2 MB/s, for each Consumer Delivery model Pull on HTTP, uses GetRecords Kinesis Data Streams sends you records via HTTP/2, uses SubscribeToShard Message spread time About 200 ms on average per Consumer so only 5 calls/sec for the GetRecords API About 70 ms on average whether there are 1 or 5 Consumers (limited to 5 Consumers but can be extended) Cost N/A Additional cost for data recovery and use of Shards Kinesis Client Library (KCL) Help to consume and process Kinesis Data Streams by supporting many complex tasks associated with distributed cloud architecture This is a Java library but can interface with other languages Each Shard can be read by a KCL instance at most Only version KCL 2.x supports Enhanced Requires a table in Amazon DynamoDB to trace the Shards processed by each KCL instance and manage resumes KCL instance can be EC2, On Premise or Elastic Beanstalk (as long as IAM accesses are properly configured) Sizing of Shards Shard Splitting Increases Streaming Capacity (Throughput) by increasing the number of Shards (and the cost!) Divides a Hot Shard The operation consists of: Creating 2 new Shards Closing the old Shard and deleting it once its data has expired This is a manual operation only (no Auto Scaling possible) and only 1 split per operation Shard Merging Decreases streaming capacity (throughput) and reduces costs Merges 2 Cold Shards with low traffic The operation consists of: Creating a new Shard Closing the old Shards and deleting them once their data has expired This is a manual operation only (no Auto Scaling possible) and only 1 merge per operation Kinesis Data Firehose General pipeline architecture with Kinesis Data Firehose Kinesis Data Firehose allows data flows to be loaded into data lakes, data stores, and analytics services with transformations as required: It is an AWS Service fully managed, serverless and with automatic sizing Cost is a function of data passing through Kinesis Data Firehose Loading quasi-real-time: Within 60 seconds of being sent to service As soon as the Buffer is filled (the size depends on the destination service and its configuration) Supports many data formats, their conversion, transformation, compression Supports custom transformations (on AWS Lambda Functions) Can send data in error or for backup in a S3 Bucket Summary Kinesis Data Streams Kinesis Data Firehose Large-scale Streaming Service Streamed Data Loading Service Requires implementation of Producer/Consumer Fully managed Real time (~200ms) “Near” real time (min of 60s) Manual scaling Automatic scaling Retention of data from 1 to 365 daysPossible replay No retention of datasNo replay possible Kinesis Data Analytics General pipeline architecture with Kinesis Data Analytics Amazon Kinesis Data Analytics allows you to query and analyze data flows in real time. It is useful for real-time analysis of time series, metrics or generation of dashboards: Based on Apache Flink, it supports SQL, Java, Scala and Python applications It is an AWS Service fully managed, serverless and with automatic sizing Cost is function of the data passing through Kinesis Data Analytics"
    } ,
  
    {
      "title"    : "AWS Messages - Amazon SQS and SNS",
      "url"      : "/en/aws-messages-sqs-sns/",
      "img"      : "PHBpY3R1cmU+CiAgPHNvdXJjZSBzcmNzZXQ9Ii9hc3NldHMvaW1nL2F3cy1tZXNzYWdlcy1zcXMtc25zLWRvY3VtZW50YXRpb24tMzAwLWI4OTViMmZkZC5hdmlmIDEuMHgsIC9hc3NldHMvaW1nL2F3cy1tZXNzYWdlcy1zcXMtc25zLWRvY3VtZW50YXRpb24tNjAwLWI4OTViMmZkZC5hdmlmIDIuMHgiIHR5cGU9ImltYWdlL2F2aWYiPgogIDxzb3VyY2Ugc3Jjc2V0PSIvYXNzZXRzL2ltZy9hd3MtbWVzc2FnZXMtc3FzLXNucy1kb2N1bWVudGF0aW9uLTMwMC1jNDExNDRkNGQuanBnIDEuMHgsIC9hc3NldHMvaW1nL2F3cy1tZXNzYWdlcy1zcXMtc25zLWRvY3VtZW50YXRpb24tNjAwLWM0MTE0NGQ0ZC5qcGcgMi4weCIgdHlwZT0iaW1hZ2UvanBlZyI+CiAgPGltZyBzcmM9Ii9hc3NldHMvaW1nL2F3cy1tZXNzYWdlcy1zcXMtc25zLWRvY3VtZW50YXRpb24tMzAwLWM0MTE0NGQ0ZC5qcGciIGFsdD0iQVdTIE1lc3NhZ2VzIC0gQW1hem9uIFNRUyBhbmQgU05TIj4KPC9waWN0dXJlPgo=",
      "isencoded" : "y",
      "tags"     : "AWS, SQS, SNS, Queue, Topic, Message, Notification, FIFO, Documentation",
      "content"  : "A particularly well-suited architecture for the Cloud is the distributed architecture: each application component can be scaled according to needs. One variation of this type of architecture is microservices. This raises the problem of communication between these components: coupled interactions are difficult to maintain and problematic in the case of different scaling sizes. An intermediate software brick (Middleware) quickly appeared to ensure decoupling and asynchronous communication between these components. We will see here 2 types of Message-Oriented Middleware (MOM): Amazon SQS which is a Message Queue Amazon SNS following a Publish/Subscribe template of Messages Amazon Simple Queue Service Characteristics Standard vs FIFO How it works Security Auto Scaling Other Parameters Dead Letter Queue Delay Queue Long Polling FIFOs’ Specific Parameters Duplication Message Grouping Amazon Simple Notification Service Fan Out architecture pattern Fan Out FIFO Fan Out Amazon Simple Queue Service Example of using a SQS Queue Characteristics Amazon Simple Queue Service or SQS is a message queue service, entirely managed by AWS: Unlimited queues and messages Message size up to 256 KB If you need larger messages, use Amazon SQS Extended Client Library for Java which stores the content of the message in Amazon S3 and just returns its reference Default 4-day message retention (up to 14 days) Standard vs FIFO It supports 2 types of Queues with different characteristics: Standard Queue FIFO Queue Unlimited Throughput: supports virtually unlimited number of transactions per second (TPS) High Throughput: By default, supports up to 300 messages per second (300 sending, receiving or deleting operations per second). If you batch 10 messages per operation (maximum), FIFO queues can support up to 3000 messages per second. At-Least-Once Delivery A message is given at least once, but may be given in several copies. Exactly-Once Processing : a message is delivered once and remains available until it is processed and deleted by a recipient. No duplicate. Best-Effort Ordering Messages may be handed out in a different order than they were sent. First-In-First-Out Delivery: the order in which messages are sent and received is rigorously maintained (First In, First Out). How it works Generating a message by calling the SendMessage API using the SDK Message consumption by calling ReceiveMessage (possibility to receive 1 to 10 messages per call) The message becomes invisible for the time defined in Message visibility timeout (default 30s) At the end of Message visibility timeout, the message becomes visible again in the Queue and can be consumed again by another consumer if it has not been deleted in the meantime or if this time has not been modified by the consumer with the API ChangeMessageVisibility Delete message by calling DeleteMessage Security Encryption in transit (HTTPS) and at rest (AWS KMS) Access control of the SQS API by an IAM Policy SQS Access Policy for: Cross-account Queue access Give access to the Queue, in writing, to other AWS services Auto Scaling A classic architecture is the Auto Scaling of consumers by the Queue itself: Example of Auto Scaling of a SQS Queue Other Parameters Dead Letter Queue When a message returns to the Queue one MaximumReceives time, it is possible to redirect it to another Queue, the Dead Letter Queue for further analysis Delay Queue When it arrives in the Queue, a message can be hidden from consumers during the Delay Queue of a maximum of 15 min Long Polling When a consumer requests a message from an empty Queue, they can wait Long Polling sec. for a message to arrive. Its possible value ranges from 1 to 20 sec FIFOs’ Specific Parameters Duplication There are 2 methods of duplicate calculation in the FIFO Queue: Hash SHA-256 message content Deduplication ID indicated in the generated message explicitly Message Grouping In the same FIFO, it is possible to group messages by specifying a MessageGroupID. In this case, each Message Group can only be consumed by one consumer and the order of the messages is specific to each group: this allows a parallel consumption of the messages of a FIFO. Amazon Simple Notification Service Example of using a SNS Queue Amazon Simple Notification Service (Amazon SNS) is an automatically managed AWS service that delivers messages from Publishers to Subscribers (model Pub/Sub also known as Producers/Consumers): A SNS Queue is made up of Topics and it is called Notification rather than Message There may be 100.000 Topics per SNS Queue A Subscriber subscribes to a Topic and receives all Notifications (Messages) posted on it It can also filter Topic Notifications to receive only certain There may be up to 10,000,000 Subscribers per Topic Publishers that can issue notifications in SNS are: Many AWS Services: CloudWatch and its alarms Auto Scaling Groups Amazon S3 events The CloudFormation events Subscribers may be of the following type: Application-To-Application (A2A) for inter-application messages to: Amazon Kinesis Data Firehose Lambda functions SQS tails HTTP/S endpoints AWS Event Fork Pipelines Application-To-Person (A2P) to notify users using: Mobile applications Mobile phone number Email addresses We find many of the same features as Amazon SQS especially in terms of Security and types of Standard Queue or FIFO Fan Out architecture pattern The use of Amazon SNS + Amazon SQS makes it possible to design perfectly decoupled architectures, without loss of data, parallel and asynchronous, called Fan Out (to break down a large processing into several smaller ones for example) Fan Out Example of Fan Out SNS + SQS architecture (extract from AWS documentation) FIFO Fan Out Example of FIFO Fan Out SNS + SQS architecture (extract from AWS documentation)"
    } ,
  
    {
      "title"    : "Monitoring and Audit in AWS - CloudWatch, X-Ray et CloudTrail",
      "url"      : "/en/aws-monitoring-auditing-cloudwatch-xray-cloudtrail/",
      "img"      : "PHBpY3R1cmU+CiAgPHNvdXJjZSBzcmNzZXQ9Ii9hc3NldHMvaW1nL2F3cy1tb25pdG9yaW5nLWF1ZGl0aW5nLWNsb3Vkd2F0Y2gteHJheS1jbG91ZHRyYWlsLWRvY3VtZW50YXRpb24tMzAwLTE5OTljN2UzNy5hdmlmIDEuMHgsIC9hc3NldHMvaW1nL2F3cy1tb25pdG9yaW5nLWF1ZGl0aW5nLWNsb3Vkd2F0Y2gteHJheS1jbG91ZHRyYWlsLWRvY3VtZW50YXRpb24tNjAwLTE5OTljN2UzNy5hdmlmIDIuMHgiIHR5cGU9ImltYWdlL2F2aWYiPgogIDxzb3VyY2Ugc3Jjc2V0PSIvYXNzZXRzL2ltZy9hd3MtbW9uaXRvcmluZy1hdWRpdGluZy1jbG91ZHdhdGNoLXhyYXktY2xvdWR0cmFpbC1kb2N1bWVudGF0aW9uLTMwMC0zYTI4NzliZGEuanBnIDEuMHgsIC9hc3NldHMvaW1nL2F3cy1tb25pdG9yaW5nLWF1ZGl0aW5nLWNsb3Vkd2F0Y2gteHJheS1jbG91ZHRyYWlsLWRvY3VtZW50YXRpb24tNjAwLTNhMjg3OWJkYS5qcGcgMi4weCIgdHlwZT0iaW1hZ2UvanBlZyI+CiAgPGltZyBzcmM9Ii9hc3NldHMvaW1nL2F3cy1tb25pdG9yaW5nLWF1ZGl0aW5nLWNsb3Vkd2F0Y2gteHJheS1jbG91ZHRyYWlsLWRvY3VtZW50YXRpb24tMzAwLTNhMjg3OWJkYS5qcGciIGFsdD0iTW9uaXRvcmluZyBhbmQgQXVkaXQgaW4gQVdTIC0gQ2xvdWRXYXRjaCwgWC1SYXkgZXQgQ2xvdWRUcmFpbCI+CjwvcGljdHVyZT4K",
      "isencoded" : "y",
      "tags"     : "AWS, CloudWatch, X-Ray, CloudTrail, Monitoring, Documentation",
      "content"  : "One of the important aspects when deploying an application in the Cloud is monitoring and supervision in order to ensure that all application services work well and to be able to react in the event of a failure. AWS offers several tools to accomplish these two tasks: AWS CloudWatch Metrics: collection of metrics integrated into AWS services and those of your application Logs: collects and stores log files (logs) Events: sends notification in response to certain events Alarms: sets activation thresholds (alarms) that trigger an action AWS X-Ray Help in the analysis and debugging of applications even those distributed Graphically produces the path of a query and the components it passes through with the associated errors AWS CloudTrail Monitoring calls to APIs Compliance Analysis Operational Audit AWS CloudWatch Metrics Custom Metrics AWS CloudWatch Logs General CloudWatch Log Agent CloudWatch Metric Filters and CloudWatch Alarms CloudWatch Events and Amazon EventBridge AWS X-Ray How it works X-Ray SDK X-Ray Daemon AWS CloudTrail CloudTrail Trail CloudTrail Events AWS CloudWatch Metrics Metrics are data about the activity of your systems. By default, many AWS services provide metrics. Free metrics have a Frequency of 5 min, it is possible to reduce it by activating the Detailed Monitoring option but for an additional cost Metrics are time-stamped Metrics are grouped first by namespace, then by the different combinations of dimensions (resource attributes) within each namespace. For example, you can display all EC2 metrics, EC2 metrics grouped by instance, or EC2 metrics grouped by auto-scaling group. Only the AWS services you use send metrics to Amazon CloudWatch. For a list of AWS services that send metrics to CloudWatch, see AWS services that publish CloudWatch metrics. From this page you can also see the metrics and dimensions published by each of these services. Custom Metrics You can publish your own metrics (Custom Metrics) in CloudWatch using the put-metric-data command on the AWS CLI or PutMetricData API: Periodicity can be Standard (1 min or more) or High resolution (1, 5, 10 or 30s) Up to 10 dimensions can be defined AWS CloudWatch Logs General CloudWatch can log most AWS services and applications that use the SDK There is a Log Group that represents the application and a Log Stream that represents each service There is an expiry policy (30, 90 days, never, …) These logs can be exported to S3 (to be saved) or to a Elastic Search Cluster for analysis Specific IAM permissions are required to allow CloudWatch to retrieve logs and they can be encrypted using AWS KMS (Log Group level) CloudWatch Log Agent For EC2 instances and on-premise servers, it is necessary to install an Agent. There are two versions: CloudWatch Log Agent: An older version that can only handle logs CloudWatch Unified Agent: The latest version that can handle logs but also detailed metrics on CPU, RAM, Disk, NetStat, Process, Swap,… of the server and SSM Parameter Store CloudWatch Metric Filters and CloudWatch Alarms CloudWatch Metric Filters can filter logs with expressions (IP, RegExp,…) to trigger Alarms CloudWatch Alarm can trigger notifications to an ASG, AWS SNS, EC2 Action based on a metric CloudWatch Events and Amazon EventBridge CloudWatch Events : Reacts from rules to what a service does (terminating an instance,…) Creates an event in the form of a JSON message containing what caused it to react Can interface to AWS SQS, SNS or Kinesis Amazon EventBridge: It takes into account 3 Event Bus: Default Event Bus for AWS Services Partner Event Bus for SaaS Services and AWS Partners Custom Event Bus for your application It also works with rules just like CloudWatch Event EventBridge can deduce the structure of the message and the Schema Registry generates the application code required for your application To be notedCloudWatch Events and Amazon EventBridge are based on the same AWS infrastructure but Amazon EventBridge is the latest version and offers more features than CloudWatch Events. AWS X-Ray AWS X-Ray is a service that collects data requests for your applications. It also allows to display and filter them in order to identify problems or optimization possibilities. For any traced request in your application, you can see detailed information not only on the request and response, but also on the calls your application makes to the downstream AWS resources, microservices, databases and HTTP Web APIs. How it works X-Ray Service Architecture and Components Each component involved in the request sends a trace to the X-Ray API: The application code by integrating the X-Ray SDK specific to its language (supported Java, Node.js, .NET,…) and the X-Ray Daemon installed on the server Scripts via AWS SDK or AWS CLI through X-Ray Daemon Some AWS services automatically if *option is enabled, except in special cases for EC2 and On-Premise All requests can be sent or only a sample Requires IAM authorization and is encrypted by AWS KMS X-Ray SDK The X-Ray SDK integration requires some changes to the application code. Example of a Java application Dependency Management &amp;lt;dependencyManagement&amp;gt; &amp;lt;dependencies&amp;gt; &amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;com.amazonaws&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;aws-xray-recorder-sdk-bom&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;2.9.0&amp;lt;/version&amp;gt; &amp;lt;type&amp;gt;pom&amp;lt;/type&amp;gt; &amp;lt;scope&amp;gt;import&amp;lt;/scope&amp;gt; &amp;lt;/dependency&amp;gt; &amp;lt;/dependencies&amp;gt; &amp;lt;/dependencyManagement&amp;gt; &amp;lt;dependencies&amp;gt; &amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;com.amazonaws&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;aws-xray-recorder-sdk-core&amp;lt;/artifactId&amp;gt; &amp;lt;/dependency&amp;gt; &amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;com.amazonaws&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;aws-xray-recorder-sdk-apache-http&amp;lt;/artifactId&amp;gt; &amp;lt;/dependency&amp;gt; &amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;com.amazonaws&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;aws-xray-recorder-sdk-aws-sdk&amp;lt;/artifactId&amp;gt; &amp;lt;/dependency&amp;gt; &amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;com.amazonaws&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;aws-xray-recorder-sdk-aws-sdk-instrumentor&amp;lt;/artifactId&amp;gt; &amp;lt;/dependency&amp;gt; &amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;com.amazonaws&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;aws-xray-recorder-sdk-sql-postgres&amp;lt;/artifactId&amp;gt; &amp;lt;/dependency&amp;gt; &amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;com.amazonaws&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;aws-xray-recorder-sdk-sql-mysql&amp;lt;/artifactId&amp;gt; &amp;lt;/dependency&amp;gt; &amp;lt;/dependencies&amp;gt; Pom DynamoDB Client import com.amazonaws.xray.AWSXRay; import com.amazonaws.xray.handlers.TracingHandler; public class SessionModel { private AmazonDynamoDB client = AmazonDynamoDBClientBuilder.standard() .withRegion(Constants.REGION) .withRequestHandlers(new TracingHandler(AWSXRay.getGlobalRecorder())) .build(); private DynamoDBMapper mapper = new DynamoDBMapper(client); Java X-Ray Daemon The X-Ray Daemon is an application that listens to traffic on port UDP 2000, collects data from Segments and transmits it to the API X-Ray It is already integrated with many AWS services but needs to be installed on EC2 instances or On-Premise servers EC2 instances of an ECS clusterThere are 2 possibilities for integrating the X-Ray Daemon: Deploy a Daemon container amazon/aws-xray-daemon on each EC2 instance Create SideCar container containing an image of the X-Ray Daemon and an image of the application code AWS CloudTrail AWS CloudTrail is an AWS service that assists you in the governance, compliance and operational and security audit of your AWS account: Each action taken by a user, role or AWS service is recorded as event in CloudTrail. Events include actions taken in the AWS management console, the AWS CLI and the AWS SDK and API. CloudTrail is enabled by default on your AWS account CloudTrail Trail Only 90 last days of activity in your AWS account is saved Create a Trail in CloudTrail to archive, analyze and react to changes in your AWS resources: A Trail is a configuration that allows to send the activity recorded by CloudTrail on a S3 Bucket You can also deliver and analyze events in CloudWatch Logs and CloudWatch Events A Trail is applied, by default, on all regions but can be applied on only one CloudTrail Events A Event in CloudTrail is the record of an activity. They are categorized into 3 types: Management Events: Provides information on management operations performed on an AWS account Configured by default Examples: IAM operations, recording devices such as a VPC, creating Trail in CloudTrail Logs, … Data Events: Provides information on operations performed on or in a resource Not active by default because it produces a very large amount of Events Examples: Amazon S3 Get/Put/Delete, AWS Lambda function activity, Amazon DynamoDB Get/Put/Delete,… Insights Events: Capture unusual activity from an AWS account Disabled by default Examples: any use that differs significantly from typical account usage patterns EventBridge IntegrationIntegration from CloudTrail to EventBridge allows automated actions to be launched in response to events on API calls (currently at a Region level)"
    } ,
  
    {
      "title"    : "AWS CI/CD: CodeCommit, CodeBuild, CodeDeploy, and CodePipeline",
      "url"      : "/en/aws-codecommit-codebuild-codedeploy-codepipeline/",
      "img"      : "PHBpY3R1cmU+CiAgPHNvdXJjZSBzcmNzZXQ9Ii9hc3NldHMvaW1nL2F3cy1jb2RlY29tbWl0LWNvZGVidWlsZC1jb2RlZGVwbG95LWNvZGVwaXBlbGluZS1kb2N1bWVudGF0aW9uLTMwMC0zNTY4NDNlM2UuYXZpZiAxLjB4LCAvYXNzZXRzL2ltZy9hd3MtY29kZWNvbW1pdC1jb2RlYnVpbGQtY29kZWRlcGxveS1jb2RlcGlwZWxpbmUtZG9jdW1lbnRhdGlvbi02MDAtMzU2ODQzZTNlLmF2aWYgMi4weCIgdHlwZT0iaW1hZ2UvYXZpZiI+CiAgPHNvdXJjZSBzcmNzZXQ9Ii9hc3NldHMvaW1nL2F3cy1jb2RlY29tbWl0LWNvZGVidWlsZC1jb2RlZGVwbG95LWNvZGVwaXBlbGluZS1kb2N1bWVudGF0aW9uLTMwMC1iZTM0NDEwM2EuanBnIDEuMHgsIC9hc3NldHMvaW1nL2F3cy1jb2RlY29tbWl0LWNvZGVidWlsZC1jb2RlZGVwbG95LWNvZGVwaXBlbGluZS1kb2N1bWVudGF0aW9uLTYwMC1iZTM0NDEwM2EuanBnIDIuMHgiIHR5cGU9ImltYWdlL2pwZWciPgogIDxpbWcgc3JjPSIvYXNzZXRzL2ltZy9hd3MtY29kZWNvbW1pdC1jb2RlYnVpbGQtY29kZWRlcGxveS1jb2RlcGlwZWxpbmUtZG9jdW1lbnRhdGlvbi0zMDAtYmUzNDQxMDNhLmpwZyIgYWx0PSJBV1MgQ0kvQ0Q6IENvZGVDb21taXQsIENvZGVCdWlsZCwgQ29kZURlcGxveSwgYW5kIENvZGVQaXBlbGluZSI+CjwvcGljdHVyZT4K",
      "isencoded" : "y",
      "tags"     : "AWS, CodeCommit, CodeBuild, CodeDeploy, CodePipeline, CI/CD, CodeStar, STS, SNS, EC2, ASG, Lambda, CloudWatch, SSM, Documentation",
      "content"  : "When it comes to cloud application development, Amazon Web Services (AWS) offers a comprehensive range of tools that facilitate continuous integration and continuous deployment (CI/CD). These essential processes enable development teams to deliver applications quickly and regularly with high quality. In this article, we will explore the fundamental principles and key concepts of the following AWS development tools: CodeCommit, CodeBuild, CodeDeploy, and CodePipeline. Each of these services plays a specific role in the lifecycle of an application and provides powerful features to automate various stages of development and deployment. By understanding these services, you will be able to effectively utilize AWS’s continuous integration and continuous deployment tools, enabling you to accelerate development, improve code quality, and deploy applications reliably and consistently. AWS CodeCommit AWS CodeBuild Defining a Build Key Benefits of CodeBuild: AWS CodeDeploy AWS CodeStar AWS CodePipeline Conclusion AWS CodeCommit Code Commit Illustration (Credit Mystique on https://github.com/miztiik/setup-aws-code-commit) AWS CodeCommit is a Git-based version control service hosted by AWS. It provides a secure and scalable platform for storing and managing the source code, binary files, and documents of your projects in the cloud. Key features of CodeCommit: Code storage and management: CodeCommit allows you to easily store and manage the source code of your applications, as well as other types of files such as binary files or documents. You can organize your code repositories based on your projects and access them centrally. Security and protected access: CodeCommit ensures the security of your resources through integration with AWS Identity and Access Management (IAM). You can define granular authorization policies to control access to the source code and repositories. This allows you to finely manage the permissions of users, groups, and roles within your organization. High availability and automatic scaling: CodeCommit is a fully managed service by AWS, which means it offers high availability without any repository size limits. You don’t have to worry about setting up or managing the underlying infrastructure. The service automatically scales to meet your storage and performance needs. Data encryption and security: CodeCommit encrypts your data during transfers via HTTPS or SSH. Additionally, you can choose to encrypt the contents of your repositories using AWS Key Management Service (KMS). This ensures the confidentiality of your source code and files. Trigger actions based on events: CodeCommit can trigger actions based on specific events. For example, you can configure notifications to Amazon Simple Notification Service (SNS), execute AWS Lambda functions, or trigger events in AWS CloudWatch Events. This allows you to automate actions in response to events related to your code repositories. By using AWS CodeCommit, you gain a robust and secure service for managing your code versions, offering advanced features for authentication, authorization, encryption, and event-based action triggering. Granting access to your repositoryUse an IAM Role and grant access to your role using AWS Security Token Service (STS) AWS CodeBuild CodeBuild Operation Example (Credit AWS on https://docs.aws.amazon.com/codebuild/latest/userguide/concepts.html) AWS CodeBuild is a service that facilitates code compilation and testing in the application development process. It offers seamless integration with various code sources, including CodeCommit, CodePipeline, GitHub, and S3. Defining a Build To define a build in CodeBuild, you use a configuration file called buildspec.yml. This file specifies the specific instructions to be executed during the application build process. Here is an example structure of a buildspec.yml file: version: 0.2 run-as: Linux-user-name env: shell: shell-tag variables: key: &quot;value&quot; phases: install: commands: - command finally: - command pre_build: commands: - command build: run-as: Linux-user-name on-failure: ABORT commands: - command post_build: commands: - command reports: report-group-name-or-arn: files: - location - location base-directory: location discard-paths: no | yes file-format: report-format artifacts: files: - location name: artifact-name s3-prefix: prefix cache: paths: - path Yaml env: During the build definition, you have the flexibility to define environment variables to configure the behavior of the build. This allows you to customize the build process according to your specific needs. phases: A build in CodeBuild is organized into different phases that define the steps of the build process. Here are some commonly used phases: install: This phase is dedicated to installing dependencies and tools required for building your application. You can specify the commands and steps needed to set up the build environment. pre_build: In this phase, you can perform preliminary actions before code compilation, such as configuring additional environment variables or running preparation scripts. build: This is the main phase where the source code is compiled and transformed into an executable artifact. You can specify compilation commands, unit tests, validations, and other necessary build tasks. post_build: After the successful build of the application, this phase allows you to perform additional actions such as packaging the application, generating reports, or archiving the generated artifacts. artifacts: The artifacts generated during the build, such as binary files or packages, can be stored in an S3 bucket. This facilitates their retrieval or use in subsequent deployment steps. cache: To speed up builds, CodeBuild supports caching of dependencies. This means that if a dependency has already been downloaded in a previous build, it can be retrieved from the cache instead of being downloaded again. This saves time and optimizes build performance. Finally, CodeBuild offers the ability to launch a build locally using Docker. This feature is useful for debugging and troubleshooting build errors as it allows you to run the build process on your development machine. Key Benefits of CodeBuild: Fully managed service: CodeBuild does not require any provisioning of build servers. It is a fully managed service by AWS, allowing you to focus on developing your application without worrying about infrastructure management. Auto-scalability and no limits: CodeBuild can handle variable workloads through its auto-scaling capability. It can run multiple builds simultaneously to accelerate the development process. Additionally, there are no predefined limits on project size or the number of builds. Usage-based cost: You are only charged for the actual build time used. This enables flexible and cost-effective pricing based on your needs. Customization of Docker images: CodeBuild relies on CloudWatch Events and AWS Lambda and uses Docker images to execute builds. You can customize these images to support different programming languages and specific dependencies for your project. Images exist for Java, Python, Ruby, Go, Node.js, PHP, .NET, and more. Integration with security services: CodeBuild tightly integrates with other AWS security services such as AWS Key Management Service (KMS) for artifact encryption, IAM Roles for build permission management, VPC Network Security for running tests in your VPC, and AWS CloudTrail for logging API calls. Integration within the AWS ecosystem: CodeBuild offers close integration with the AWS ecosystem, enhancing visibility and management of builds. Here are some key integration features: Build logs can be stored in locations such as S3 or AWS CloudWatch, providing full traceability of build activities. CodeBuild generates metrics that allow you to track build performance and gain insights into execution times, potential errors, and more. With integration with CloudWatch Alarms, you can set up failure detection thresholds for builds. When these thresholds are reached, notifications are triggered to alert you of potential issues. Using Amazon Simple Notification Service (SNS), CodeBuild can send customized notifications to inform team members or stakeholders about important build events. With AWS CodeBuild, you have a powerful and flexible service to automate the compilation and testing of your code, benefiting from simplified management, usage-based pricing, and tight integration with other AWS services. CodeBuild or CodePipelineBuild instructions can be defined in both CodeBuild and CodePipeline. It’s important to understand that defining these instructions in both tools can lead to unintended and difficult-to-interpret behaviors. VPCBy default, CodeBuild runs outside of your VPC. To access resources such as a load balancer, a database, or an EC2 instance, you need to configure a VPC configuration (VPC ID, subnet IDs, security group ID). AWS CodeDeploy AWS CodeDeploy is an automated application deployment service that simplifies and automates the deployment process for a wide range of instances, including EC2 instances and Lambda functions. When using CodeDeploy, it is necessary to install the CodeDeploy Agent on the target instances. This agent plays a key role in executing deployments and facilitates the coordination of deployment actions on the relevant instances. CodeDeploy operates based on the use of a configuration file called appspec.yml. This file specifies detailed instructions for each deployment. CodeDeploy organizes instances into groups, typically by environment (e.g., development, testing, production). This organization allows for consistent application deployments on specific target instances for each environment. Here are some important terms to know in the context of CodeDeploy: Application: The name of the application or application component you want to deploy. Compute Platform: EC2 target instances, instances belonging to an Auto Scaling Group (ASG), on-premises instances, and Lambda functions. Deployment Configuration: For EC2 and on-premises instances, you can specify a minimum percentage of instances in a healthy state required for deployment. For Lambda functions, you can define traffic routing to the newly deployed versions. Deployment Type: CodeDeploy supports In-Place deployments (updating the application on existing instances) as well as Blue/Green deployments, which involve launching new EC2 instances and bringing them into service before switching traffic to them. IAM Instance Profile: IAM permissions for EC2 instances to read artifacts from S3 or GitHub during deployment. Application Revision: The source code revision and appspec.yml file. Service Role: An IAM role that allows CodeDeploy to perform necessary actions for deployment. Target Revision: The version of the application once the deployment process is completed. Defining a deployment in CodeDeploy involves several key elements: File Section: Instructions for copying the artifact from S3 to the file system of each target instance. Hooks: Instructions for deploying the new version of the application on the instances. Hooks are divided into different ordered phases, including: ApplicationStop: Stopping the currently running application on the target instances. DownloadBundle: Downloading the new version of the application from S3. BeforeInstall: Performing specific actions before installing the new version. Install: Installing the new version of the application. AfterInstall: Performing additional actions after installation. ApplicationStart: Starting the new version of the application. ValidateService: Checking the functionality of the new version of the application using a health check. In the case of Blue/Green deployments with EC2 instances, CodeDeploy allows for launching new EC2 instances, deploying the new version of the application on those instances, bringing them into service, and switching traffic to them while retaining the previous version of active instances for possible rollback. AWS CodeDeploy offers a comprehensive solution for automated application deployment on various platforms and configurations, providing flexibility and ease of use for continuous deployment. AWS CodeStar AWS CodeStar is an integrated CI/CD (Continuous Integration and Continuous Deployment) solution offered by AWS. It brings together a set of services and tools that facilitate application development and deployment. CodeStar provides a comprehensive overview of CI/CD services and allows for their management from a single entry point. This simplifies the configuration and management of the entire development process, from source code to application delivery. One of the key features of CodeStar is its integration of CI/CD services with other popular tools. For example, certain regions offer integration with the Cloud9 IDE, a cloud-based development environment, as well as with JIRA or GitHub Issues, ticket management tools. This integration allows developers to access these tools from the same dashboard and benefit from a smoother development experience. CodeStar also offers a development and CI/CD environment tailored to the application’s language. Whether you are using Java, Go, Python, Node.js, HTML5, or other languages, CodeStar provides templates and preconfigured settings to facilitate the creation of an appropriate development environment. This saves time and simplifies the initial setup by providing a project structure tailored to the chosen language. AWS CodePipeline AWS CodePipeline is an orchestration service for Continuous Integration and Continuous Deployment (CI/CD) offered by AWS. It enables the setup and management of fully automated deployment pipelines. CodePipeline acts as an orchestrator to coordinate the different stages of the deployment process, from source code retrieval to application delivery. It facilitates continuous integration by automating build, testing, and subsequent deployments, ensuring fast and reliable application delivery. CodePipeline offers seamless integration with third-party tools such as GitHub, Jenkins, and AWS services like Elastic Beanstalk, CloudFormation, and ECS. This allows developers to use their preferred tools and easily integrate them into their deployment pipelines. Exemple de pipeline de CICD avec CodePipeline (Crédit AWS à https://aws.amazon.com/fr/blogs/devops/complete-ci-cd-with-aws-codecommit-aws-codebuild-aws-codedeploy-and-aws-codepipeline/) A concrete example of implementing a CI/CD pipeline with CodePipeline could look like this: The developer pushes code to CodeCommit. An event is triggered to AWS CloudWatch Event, which automatically triggers AWS CodePipeline. It initiates the build process using AWS CodeBuild. The source code is retrieved, compiled, tested, and artifacts are delivered to an S3 bucket. AWS CodePipeline then triggers a deployment with AWS CodeDeploy, which launches the deployment of applications on EC2 instances using the previously installed CodeDeploy Agent. This implementation scenario demonstrates how CodePipeline can orchestrate the entire CI/CD process, from source code management to application delivery, by automating the different steps and facilitating integration with various tools and services. Of course, there are more complex scenarios involving other AWS or external services. In summary, AWS CodePipeline is a powerful tool for orchestrating CI/CD pipelines. It provides flexibility and ease of use, allowing developers to create automated and customizable deployment workflows while integrating third-party tools and AWS services to meet the specific needs of their development process. Conclusion We encourage you to further explore each service and use them in real projects. Each of these tools has its own features and benefits, and by using them appropriately, you can accelerate development, improve application quality, and optimize your deployment processes on AWS. In summary, with AWS CodeCommit, CodeBuild, CodeDeploy, and CodePipeline, you have a set of powerful tools for application development and deployment on AWS. By mastering them, you can create efficient workflows and enhance your entire development process."
    } ,
  
    {
      "title"    : "AWS Elastic Beanstalk",
      "url"      : "/en/aws-elastic-beanstalk/",
      "img"      : "PHBpY3R1cmU+CiAgPHNvdXJjZSBzcmNzZXQ9Ii9hc3NldHMvaW1nL2F3cy1lbGFzdGljLWJlYW5zdGFsay1kb2N1bWVudGF0aW9uLTMwMC00MzIxNGI3N2UuYXZpZiAxLjB4LCAvYXNzZXRzL2ltZy9hd3MtZWxhc3RpYy1iZWFuc3RhbGstZG9jdW1lbnRhdGlvbi02MDAtNDMyMTRiNzdlLmF2aWYgMi4weCIgdHlwZT0iaW1hZ2UvYXZpZiI+CiAgPHNvdXJjZSBzcmNzZXQ9Ii9hc3NldHMvaW1nL2F3cy1lbGFzdGljLWJlYW5zdGFsay1kb2N1bWVudGF0aW9uLTMwMC03ZmYxYWQ1NmQuanBnIDEuMHgsIC9hc3NldHMvaW1nL2F3cy1lbGFzdGljLWJlYW5zdGFsay1kb2N1bWVudGF0aW9uLTYwMC03ZmYxYWQ1NmQuanBnIDIuMHgiIHR5cGU9ImltYWdlL2pwZWciPgogIDxpbWcgc3JjPSIvYXNzZXRzL2ltZy9hd3MtZWxhc3RpYy1iZWFuc3RhbGstZG9jdW1lbnRhdGlvbi0zMDAtN2ZmMWFkNTZkLmpwZyIgYWx0PSJBV1MgRWxhc3RpYyBCZWFuc3RhbGsiPgo8L3BpY3R1cmU+Cg==",
      "isencoded" : "y",
      "tags"     : "AWS, Beanstalk, EB, DevOps, Automatisation, Amazon-SQS, Deployment, Docker, ECS, Documentation",
      "content"  : "AWS Elastic Beanstalk is an AWS orchestration service used to deploy applications. He knows how to manage different AWS services such as EC2, S3, Simple Notification Service (SNS), CloudWatch, AutoScaling and Elastic Load Balancers. Principles Deployment strategies Integrated into Beanstalk All at once Rolling Rolling with additional batch Immutable Traffic Splitting Déploiement Blue / Green Other consideration Automation Docker container Custom Platform Principles AWS Beanstalk is based on the definition of 3 objects: an application in a given version, a configuration which contains the definition of the AWS Services forming the infrastructure architecture, an environment that combines application version and configuration (dev, test, int, prod,…) Therefore, it simplifies the deployment of an application: The developer handles the application code and versions Beanstalk automates the unfolding and configuration of LB, AutoScaling, Security Groups, EC2 instances, Cloudwatch monitoring, Subnets,… Elastic Beanstalk is part of the DevOps toolkit available in AWS It supports a multitude of application languages (Java, .NET, Python, Node.js, Ruby, Docker Containers,…) It manages 2 types of application architecture: Server Web or Worker application architectures Web Server Tier: for application that serves HTTP requests Worker Tier: for Backend application that extracts its tasks from an Amazon Simple Queue Service (Amazon SQS) message Deployment strategies Every company has the same deployment strategies. They vary depending on the purpose. Let’s look at what Beanstalk is proposing and how to implement the current strategies. Integrated into Beanstalk Beanstalk proposes different deployment strategies: All at once Deployment on All at once mode Fast deployment but application has downtime No additional cost Ideal for non-productive environments Rolling Deployment on Rolling mode Longer deployment (the Bucket Size must be adapted to the number of instances) but the application has no interruption 2 versions of the cohesive application No additional cost Rolling with additional batch Deployment on Rolling with additional batch mode Longer deployment (the Bucket Size must be adapted to the number of instances) but the application is uninterrupted and runs at full capacity during the deployment process 2 versions of the cohesive application Small extra cost (extra bucket size) Immutable Deployment on Immutable mode Longer deployment but application is uninterrupted and runs at full capacity during deployment process New release deployment runs in a temporary ASG 2 versions of the cohesive application High cost (number of instances doubles) Traffic Splitting Equivalent of Canary Testing: an increasing number of users are automatically redirected to the new application at regular time intervals Application health is monitored and a very fast rollback is performed in case of failure Longer deployment but application is uninterrupted and runs at full capacity during deployment process New release deployment runs in a temporary ASG 2 versions of the cohesive application Déploiement Blue / Green Blue/Green Deployment It is not really supported by Beanstalk but it is possible to do it using manual actions: N Version’s application is deployed on environment Blue The N+1 application is deployed on the Green environment with exactly the same configuration Opening of the Green environment at Route 53 for Testing teams Tests on the Green environment: Tests OK: Switch all traffic on the Green with Route 53 and remove the Blue version by setting the ASG to min. capacity = 0 Tests KO: Green version removed and stay on Blue Other consideration Dev vs Prod In a Development environment, it is often necessary to have only one application instance: the DNS name of the application is mapped to an Elastic IP of the EC2 instance In a Production environment, you want High Availability: The DNS name of the app is mapped to the IP address of a Load Balancer that will redirect requests to a Auto Scaling Group that will spread EC2 instances over different Availability Zones Automation You can automate deployments with Beanstalk by adding configuration files to the application sources: They must be located at the root of the application in a directory .ebextensions/ (based on AWS CloudFormation templates) Each configuration file must have the .config extension and be in JSON format or YAML They make it possible to specify : additional resources such as a RDS BDD, a S3 bucket,… (any AWS services) A SSL certificate for the LB to be configured either in the securelistener-alb.config file or via the AWS Certificate Manager (ACM) service Instance or ALB level HTTP to HTTPS redirects Optional variables with option_settings Docker container Beanstalk knows how to handle Docker containers. To do this, it is possible to provide a file: Dockerfile: it will be used to build and launch the Docker image Dockerrun.aws.json version v1: Single Docker Mode (1 image only) It refers to an already built Docker image and configuration items Beanstalk does not create an ECS instance but simply an EC2 instance with Docker Dockerrun.aws.json version v2: Multi Docker Mode (multiple images) Contains the definition of an ECS Task Beanstalk creates an ECS Cluster containing ECS instances, an LB in HA mode and the ECS Task Docker images must already be built and present in AWS ECR or DockerHub Custom Platform In case the language of your application is not supported by Beanstalk, it is possible to build a custom Beanstalk platform. This requires: Building an AMI with a Platform.yaml file Building the Platform with software Packer"
    } ,
  
    {
      "title"    : "Amazon Containers - ECS, ECR and Fargate",
      "url"      : "/en/aws-ecs-ecr-fargate/",
      "img"      : "PHBpY3R1cmU+CiAgPHNvdXJjZSBzcmNzZXQ9Ii9hc3NldHMvaW1nL2F3cy1lY3MtZWNyLWZhcmdhdGUtZG9jdW1lbnRhdGlvbi0zMDAtOGQyM2QyOTc4LmF2aWYgMS4weCwgL2Fzc2V0cy9pbWcvYXdzLWVjcy1lY3ItZmFyZ2F0ZS1kb2N1bWVudGF0aW9uLTYwMC04ZDIzZDI5NzguYXZpZiAyLjB4IiB0eXBlPSJpbWFnZS9hdmlmIj4KICA8c291cmNlIHNyY3NldD0iL2Fzc2V0cy9pbWcvYXdzLWVjcy1lY3ItZmFyZ2F0ZS1kb2N1bWVudGF0aW9uLTMwMC05NjA3NzgxMDUuanBnIDEuMHgsIC9hc3NldHMvaW1nL2F3cy1lY3MtZWNyLWZhcmdhdGUtZG9jdW1lbnRhdGlvbi02MDAtOTYwNzc4MTA1LmpwZyAyLjB4IiB0eXBlPSJpbWFnZS9qcGVnIj4KICA8aW1nIHNyYz0iL2Fzc2V0cy9pbWcvYXdzLWVjcy1lY3ItZmFyZ2F0ZS1kb2N1bWVudGF0aW9uLTMwMC05NjA3NzgxMDUuanBnIiBhbHQ9IkFtYXpvbiBDb250YWluZXJzIC0gRUNTLCBFQ1IgYW5kIEZhcmdhdGUiPgo8L3BpY3R1cmU+Cg==",
      "isencoded" : "y",
      "tags"     : "AWS, ECS, ECS-Task, Cluster, Docker, ECS-Service, ECR, IAM-Role, Instance-Profile, Fargate, AMI, Auto-Scaling, ASG, ELB, Documentation",
      "content"  : "AWS offers several Container Management Services. These include: Amazon ECS for Elastic Container Service Amazon Fargate to run Amazon ECS in Serverless Amazon EKS for Amazon Elastic Kubernetes Service to manage a Kubernetes Cluster In this article, we will talk about the first 2 Services, Amazon EKS deserving a chapter of its own. Amazon ECS Composition of ECS Cluster ECS Task Placement of Tasks in the ECS Cluster Placement Strategy ECS Service ECS Auto Scaling Amazon ECR Command Line Usage The Roles behind it all Amazon Fargate Amazon ECS Amazon Elastic Container Service (Amazon ECS) is a highly scalable and fast container management service It’s a cluster of EC2 instances : each instance runs an ECS Agent (the Docker container) The ECS Agent is also used to record the instance in the ECS Cluster There are AMIs (Amazon Machine Image) that contain the ECS Agent Composition of ECS Cluster An ECS Cluster contains: One or more EC2 instances: Instances of type AMI amazon-ami-ecs-optimized based on the Docker image amazon/amazon-ecs-agent:latest With EBS or EFS storage And a Key Pair to access it It integrates into a VPC on one or more Subnets to be highly available (multi-AZ): An ASG is automatically created and contains a Launch Configuration that configures the ECS Cluster name in each /etc/ecs/ecs.config file of each EC2 instance Each EC2 instance is protected by a Security Group On each EC2 instance is added the Role ecsInstanceRole which is based on the Policy AmazonEC2ContainerServiceforEC2Role, managed by AWS, and which allows the ECS Agent to make calls to the ECS API ECS Task This is the AWS formalism (in JSON) to define what the Dockerfile file contains and which will be used to define a Docker container It contains the name of the image to run, the ports mapping between the host and the container, the amount of memory and CPU that the container will have available, the environment variables to pass to the container,… Security GroupIn the Security Group of EC2 instances, remember to allow the port of the application defined for the host in order to be able to access the application in the container Placement of Tasks in the ECS Cluster Tasks are placed in best-effort. Amazon ECS always tries to place tasks even when the most optimal placement option is unavailable. However, the placement constraints of the Tasks are restrictive, and they may prevent the placement of the Tasks. When Amazon ECS places Tasks, it uses the following process to select a container instance: Identifies instances that meet processor, memory and port requirements in the task definition It identifies instances that meet the placement constraints of the Tasks Identifies instances that meet the Tasks placement strategies Placement Strategy There are different placement strategies: binpack: Tasks are placed on container instances in a way that leaves the least amount of CPU or memory unused This strategy minimizes the number of instances in the ECS Cluster and therefore the associated costs random: Tasks are randomly placed on the available instances spread: Tasks are distributed according to a specified value. The accepted values are instanceId or any custom attribute that can be applied to an ECS instance, such as the attribute: ecs.availability-zone In the case of an attribute ecs.availability-zone*, the Tasks will be distributed across all AZ of the ECS Cluster ECS Service ECS Service defines how to start an ECS Task: it is the equivalent of Docker’s SWARM file or Kubernetes Service/Replica/Deployment… configuration files It states: How many Tasks should be started (min, max, desired): ECS Service Auto Scaling allows automatic scaling of Tasks How to distribute them between the EC2 instances of the Cluster ECS Cluster Auto Scaling (CAS) allows you to automatically add or remove EC2 instances to the ECS Cluster Access to the application through a Elastic Load Balancer (ALB, NLB, CLB to choose): The dynamic port forwarding of the LB does not specify a Port on the Host side and the LB will do the mapping automatically This functionality requires modifying the Security Group of EC2 instances (authorization of all ports for the SG of the Load Balancer) ECS Auto Scaling Auto Scaling As can be seen, the AWS documentation mentions the Auto Scaling in numerous terms: here ECS Service Auto Scaling, ECS Cluster Auto Scaling,… In fact, AWS Auto Scaling (the generic service) is organized into 2 categories: Amazon EC2 Auto Scaling, when it concerns an EC2 instance Application Auto Scaling, for anything other than EC2 In ECS, there is a Cluster Capacity Provider that is combined with an Auto Scaling Group to manage the sizing of Cluster instances Amazon ECR Amazon Elastic Container Registry (Amazon ECR) is an AWS-managed container image registry that is secure, scalable and robust. Amazon ECR relies on AWS IAM permissions to push and retrieve images from its registry. It offers different functions such as image security scan or Cross-Region and Cross-Account replication Command Line Usage Authentication of the Docker Client to the ECR registry with AWS CLI v1 or v2: $(aws ecr get-login-password --no-include-email --region &amp;lt;region&amp;gt;) Cli-v1 aws ecr get-login-password --region &amp;lt;region&amp;gt; | docker login --username AWS --password-stdin &amp;lt;aws_account_id&amp;gt;.dkr.ecr.&amp;lt;region&amp;gt;.amazonaws.com Cli-v2 Docker image tag with registry URL: docker tag &amp;lt;docker-image-name&amp;gt;:latest &amp;lt;aws_account_id&amp;gt;.dkr.ecr.&amp;lt;region&amp;gt;.amazonaws.com/&amp;lt;docker-image-name&amp;gt;:latest Zsh Push the Docker image into the register: docker push &amp;lt;aws_account_id&amp;gt;.dkr.ecr.&amp;lt;region&amp;gt;.amazonaws.com/&amp;lt;docker-image-name&amp;gt;:latest Zsh The Roles behind it all It is important to keep in mind the use of Roles that is made by ECS and what remains to be configured by the user: The EC2 instances that are part of an ECS Cluster have implicitly Roles allowing them to interact with: ECS Service: used by the ECS Agent to make calls to the ECS Service API CloudWatch Logs: to send usage logs to the CloudWatch API ECR Service: to retrieve the Docker image from the ECR repository But the user must specify the necessary Roles for the ECS Tasks, for example: AmazonS3ReadOnlyAccess for an application defined in a Task that will access a S3 Bucket AmazonEC2FullAccess for an application that will access the EC2 API IAM Role and Instance ProfileThe two terms are used to designate, as appropriate, the same thing: The Instance Profile is the instantiation, in an EC2 instance, of permissions defined in IAM Role The IAM Role being just the definition of this permissions Amazon Fargate AWS Fargate allows you to create an Amazon ECS Cluster without worrying about Amazon EC2 instances. This is the Serverless version of Amazon ECS The creation of the Tasks is similar to those in ECS but Fargate is responsible for providing them with an execution environment So it’s much simpler."
    } ,
  
    {
      "title"    : "Amazon CloudFront",
      "url"      : "/en/aws-amazon-cloudfront/",
      "img"      : "PHBpY3R1cmU+CiAgPHNvdXJjZSBzcmNzZXQ9Ii9hc3NldHMvaW1nL2F3cy1jbG91ZGZyb250LWRvY3VtZW50YXRpb24tMzAwLTdhMmQ3MmUwOC5hdmlmIDEuMHgsIC9hc3NldHMvaW1nL2F3cy1jbG91ZGZyb250LWRvY3VtZW50YXRpb24tNjAwLTdhMmQ3MmUwOC5hdmlmIDIuMHgiIHR5cGU9ImltYWdlL2F2aWYiPgogIDxzb3VyY2Ugc3Jjc2V0PSIvYXNzZXRzL2ltZy9hd3MtY2xvdWRmcm9udC1kb2N1bWVudGF0aW9uLTMwMC1hMDQzYWZmNTQuanBnIDEuMHgsIC9hc3NldHMvaW1nL2F3cy1jbG91ZGZyb250LWRvY3VtZW50YXRpb24tNjAwLWEwNDNhZmY1NC5qcGcgMi4weCIgdHlwZT0iaW1hZ2UvanBlZyI+CiAgPGltZyBzcmM9Ii9hc3NldHMvaW1nL2F3cy1jbG91ZGZyb250LWRvY3VtZW50YXRpb24tMzAwLWEwNDNhZmY1NC5qcGciIGFsdD0iQW1hem9uIENsb3VkRnJvbnQiPgo8L3BpY3R1cmU+Cg==",
      "isencoded" : "y",
      "tags"     : "AWS, CloudFront, Origin, Geo-Restriction, S3, ALB, EC2, Protocol-Policy, Signed-URL, Signed-Cookie, Trusted-Key-Group, Cache, TTL, Documentation",
      "content"  : "Amazon CloudFront speeds up the distribution of static and dynamic web content such as html, css, php, images and media files. When users request content, CloudFront broadcasts it through a global network of Edge Locations that offer low latency and high performance. Let us look at how it works in detail. Presentation Advanced mechanisms Architectures S3 Bucket HTTP End-Point (ALB, EC2) Security Geo Restriction HTTPS Signed URL / Signed Cookie Generating from Trusted Key Groups CloudFront Caching Price Classes Presentation Amazon CloudFront is a Content Delivery Network (CDN), which is based on a system of Caches distributed over AWS’s 230 PoPs and interconnected via AWS backbone. Worldwide locations of Edge Locations CloudFront offers security features as: Network Attack Protection and anti-DDoS HTTPS Protocol *Field Level Encryption Integrates with AWS Shield, AWS Web Application Firewall and Amazon Route 53 It works with any origin: All origins AWS Amazon S3 Bucket or WebSite, Amazon EC2, Elastic Load Balancing Any on-premise HTTP termination Advanced mechanisms CloudFront allows you to configure different Origins (Multiple Origins) depending on the content’s type content’s path (against pattern). Similarly, an Origin Group consisting of Primary and Secondary Origins allows to set up a failover mechanism in case the Primary Origin returns an error. Finally, there is an asymmetric field encryption mechanism (Field Level encryption) that allows a form field encryption from the Edge Location to the final HTTP endpoint which can decrypt the field. Architectures Depending on the origin, CloudFront integrates into 2 types of architecture. S3 Bucket CloudFront architecture with S3 Bucket as Origin and an Origin Access Identity HTTP End-Point (ALB, EC2) CloudFront architecture with an ALB (HTTP) as Origin Security Geo Restriction CloudFront allows filter users by Country. A database linking IP addresses and country of membership allows only certain countries (whitelist) to access a resource or, on the contrary, blocks access to certain countries (blacklist) HTTPS CloudFront allows you to control the transport protocol used between the different points with Protocol Policy: Viewer Protocol Policy: From Customer to Edge Location Used to force HTTPS protocol or redirect HTTP calls to HTTPS Origin Protocol Policy: From Edge Location to Origin (S3 Bucket or HTTP Server) Allows you to choose between HTTP and HTTPS Signed URL / Signed Cookie This CloudFront functionality makes available content for a certain period of time: Signed URL: provides a file to any user with this URL Signed Cookie: makes several files available to any user who has this cookie The validity time depends on the content you want to share: Paid content, 24 hour rental,… Storage space reserved for 1 year,… Do not confuse CloudFront Signed URLs with S3 Pre-Signed URLs Generating from Trusted Key Groups Now, AWS recommends using Trusted Key Groups to generate Signed URL / Cookie. Indeed: Management (creation, rotation,…) of Trusted Key Groups is done entirely using AWS APIs The use of these APIs is protected by an IAM Role A Trusted Key Groups consists of: A private key used to sign a URL or Cookie A public key used to verify that the signature is valid CloudFront Caching The content can be cached according to 3 different criteria: Header Session Cookie URL parameter The Time To Live (TTL) can range from 0 sec to 1 year and depends on the type of content: For static content: TTL can be large because content does not change much and it is a good way to reduce latency. The criteria of Headers and Cookie should not be taken into account but only the URL. For dynamic content: TTL should be low and based on Headers and Cookies to maximize cache without the risk of delivering obsolete content. It is also possible to disable specific content from caches (depending on patterns) so that all Edge Locations update their cache with the new content version. Price Classes Because there are more than 230 Edge Locations, the cost of CloudFront can quickly increase. It is possible to reduce this cost by selecting Edge Locations according to their price per Region. There are 3 price classes that can be selected: Price Class All: all Regions, high cost but better performance Price Class 200: most Regions but removes those with the highest cost Price Class 100: cheapest Regions"
    } ,
  
    {
      "title"    : "Amazon S3",
      "url"      : "/en/aws-amazon-s3/",
      "img"      : "PHBpY3R1cmU+CiAgPHNvdXJjZSBzcmNzZXQ9Ii9hc3NldHMvaW1nL2F3cy1zMy1kb2N1bWVudGF0aW9uLTMwMC1mOGMzMDBiMzQuYXZpZiAxLjB4LCAvYXNzZXRzL2ltZy9hd3MtczMtZG9jdW1lbnRhdGlvbi02MDAtZjhjMzAwYjM0LmF2aWYgMi4weCIgdHlwZT0iaW1hZ2UvYXZpZiI+CiAgPHNvdXJjZSBzcmNzZXQ9Ii9hc3NldHMvaW1nL2F3cy1zMy1kb2N1bWVudGF0aW9uLTMwMC1lYWYwNDhjYjMuanBnIDEuMHgsIC9hc3NldHMvaW1nL2F3cy1zMy1kb2N1bWVudGF0aW9uLTYwMC1lYWYwNDhjYjMuanBnIDIuMHgiIHR5cGU9ImltYWdlL2pwZWciPgogIDxpbWcgc3JjPSIvYXNzZXRzL2ltZy9hd3MtczMtZG9jdW1lbnRhdGlvbi0zMDAtZWFmMDQ4Y2IzLmpwZyIgYWx0PSJBbWF6b24gUzMiPgo8L3BpY3R1cmU+Cg==",
      "isencoded" : "y",
      "tags"     : "AWS, S3, Standard, Infrequent-Access, One-Zone, One-Intelligent-Tiering, Glacier, Glacier-Deep-Archive, Versioning, Replication, Encryption, Policy, ACL, Athena, webSite, Documentation",
      "content"  : "Amazon Simple Storage Service (Amazon S3) is a storage for the Internet. You can use Amazon S3 to store and recover any amount of data at any time, anywhere on the web. Its scale is almost infinite, a considerable number of websites use Amazon S3 as well as many AWS services. Therefore, it is an essential component of AWS. Buckets et Objects Bucket Object Versionning Stockage Classes Transition and Life Cycle Versioning Replication Encryption Methods Forcing encryption Security Access Management Pre-signed URL Others Logging and Audit Logging Bucket Audit with Athena S3 Website S3 CORS (Cross-Origin Resource Sharing ) Buckets et Objects In Amazon S3, objects (files) are stored in buckets (directories) Bucket A Bucket must have a unique name at the global level of the AWS network even if it is defined at the Region level It follows a naming convention: No uppercase letter No underscore No IP And starts with a lowercase letter or a number Object As a file content It has a maximum size of 5 TB (as a 5 GB multi-parts) You can attach meta-data, tags and a version ID to it Objects are accessible by their Key A Key is composed of a prefix and the object name: Prefix: company/department/ Object Name: users.json Key: company/department/users.json For a Bucket named referential, the object will be accessed via the URL: s3://referential/company/department/users.json Even if there is no notion of directory in S3, we see that the naming of Prefix with /`s allows to simulate a tree structure. Versionning To implement the version management of objects, you must first enable the versioning at the Bucket level. Version number is generated by Amazon S3 The removal of an object is then soft-delete and the object will be marked with a delete marker. It will no longer be displayed in the list of objects but it will still exist with its different versions. Stockage Classes There are several categories of S3 storage (S3 Classes) that should be used depending on your use case: Amazon S3 Standard: General Use Highly durable data (10,000 years) 99.99% availability over 1 year (eleven 9) Resilient to AZ disaster (supports 2 concurrent failures) Amazon S3 Standard-Infrequent Access (SIA): For less frequently used data Backup Disaster Recovery Highly durable data (10,000 years) 99.9% availability over 1 year (un 9) Resilient to AZ disaster (supports 2 concurrent failures) Cheaper than S3 Standard Amazon S3 One Zone-Infrequent Access: For less frequently used and lost data: Secondary backup Data that can be recreated Highly durable data (10,000 years) BUT on a single AZ (risk of data loss) 99.5% availability over 1 year Cheaper than S3 SIA Amazon S3 One Intelligent Tiering: Same low latency and high throughput as S3 Standard Moves Objects between 2 thirds (e.g. between standard S3 and S3 IA) Highly durable data (10,000 years) 99.9% availability over 1 year Resilient to AZ disaster (supports 2 concurrent failures) Additional cost due to monitoring required Amazon Glacier: For long-term data retention (minimum 90 days) of up to 10 years, which do not require access: Archives or backups Very low cost storage BUT with cost recovery Recovery: Expedited: 1-5 min Standard: 3 to 5 hours Bulk: 5-12 hours Highly durable data (10,000 years) Archive with a size of 40 TB Storage is done in Vaults Amazon Glacier Deep Archive: Like Amazon Glacier For long-term data retention (minimum 180 days) up to 10 years, which do not require quick access Recovery: Standard: 12 hours Bulk: 48 hours Transition and Life Cycle It is possible to create rules so that the data is automatically migrated to a more suitable storage or even deleted: Transition: moves objects to less expensive storage after some time Expiration: deletes an object after some time Note The supported transition cycle is constrained and it is not possible to switch from Amazon Glacier to S3 Standard directly The ultimate goal is to reduce storage costs Versioning To implement the version management of objects, you must first enable versioning at the bucket level. Version number is generated by Amazon S3 Deleting an object is then soft-delete and the object will be marked with a Delete Marker. It will no longer be displayed in the list of objects but it will still exist with its different versions. Replication It is possible to replicate a S3 Bucket between 2 Regions (Cross Region Replication) or in the same Region (Same Region Replication): Versioning must be enabled on the 2 Buckets They can belong to 2 different accounts Permissions are managed by an IAM Role Replication is asynchronous but fast Possible use cases are: For CRR: regulatory compliance, latency reduction, AWS cross-region replication For SCR: data aggregation, live replication between environments Good to know Once enabled, replication is only performed on new or modified objects An option allows to replicate deletions (only Delete Markers) It is not possible to replicate a Replication Bucket Encryption Methods There are 4 methods of encrypting objects in S3: SSE-S3: Key managed by AWS Server Side Encryption (SSE) Algorithm AES-256 Activates by passing the Header “x-amz-server-side-encryption”:”AES256” when uploading the object Can use HTTP or HTTPS SSE-KMS: Uses KMS (Key Management Service) to manage the key Server Side Encryption (SSE) Activates by passing the Header “x-amz-server-side-encryption”:”aws:kms” when uploading the object Uses the Customer Master Key defined in KMS for encryption Can use HTTP or HTTPS SSE-C: Allows you to provide your own key (but it is up to you to store it) Server Side Encryption (SSE) but the key is not stored in AWS! Activates by passing the key in the Header when uploading the object but also when reading it Uses only HTTPS protocol (to protect key) Client-rated encryption: Encryption of objects is the responsibility of the Client Side Encryption Client (CSE) Encryption / decryption is done on the Client side Forcing encryption There are 2 ways to force encryption of an Object in its Bucket: Force encryption with a S3 Bucket Policy that only accepts PUT requests with an encryption header (and otherwise refuses the request) Enable Default Encryption on a Bucket: If the object is sent with an encryption method in the request, it will be applied If the object is sent without an encryption method, it will be encrypted with the default encryption method To be noted The Default Encryption option therefore ensures that objects are always encrypted but does not guarantee the encryption method Bucket Policy will always be evaluated before Default Encryption Encryption In Transit only encrypts an object in SSL/TLS when it is transferred to/from AWS. He doesn’t encrypt the object in its bucket. Security Access Management Access to S3 is managed at different levels: User: IAM Policy: Defines the calls allowed to S3 APIs for each IAM user Resource: Bucket Policy: S3 Bucket Policy: Configuration in JSON format Allows you to configure a public access to a Bucket, to force the encryption of objects or to give access to another account (Cross-Account) Block Public Access: Blocks public access to a Bucket Prevents leakage of data stored in a Bucket Object Access Control List: ACL for each object Bucket Access Control List: ACL at each bucket level Pre-signed URL A Pre-signed URL allows to generate a valid URL a time lapse (default 1H) to allow a user to download or upload a file into a Bucket: It can be generated with AWS CLI or SDK The user of the Pre-signed URL inherits the same rights (GET / PUT) as the one who created it Use case Generation of unique URLs and temporary downloads Generation of temporary URLs to upload into specific Bucket locations Others Networking: Supports VPC Endpoints (EC2 instances without Internet access) MFA for deletion: Must be enabled under Root Account with the following AWS CLI command: aws s3api put-bucket-versioning --bucket &amp;lt;bucket-name&amp;gt; --versioning-configuration Status=Enabled,MFADelete=Enabled --mfa &quot;&amp;lt;mfa-device-arn&amp;gt; &amp;lt;mfa-code&amp;gt;&quot; Reserved for the owner of the Bucket, requires a Multi Factor Authentication (MFA) token to delete a versioned Object or delete the versioning of a Bucket Logging and Audit Logging Bucket It is possible to log all access to a Bucket S3 in another Bucket S3: This bucket is called a Logging Bucket All accesses, authorized or not, will be logged with a lot of information about the Client who accessed them (Log Format) It will then be possible to analyse these requests (see Athena below) S3 APIs calls can be logged into AWS CloudTrail WarningYou should never configure the Logging Bucket as the same as the monitored Bucket, otherwise you will cause endless loops of logs and see its AWS bill explode! Audit with Athena Athena is a service that allows to perform analysis queries directly on S3 objects (without going through a BDD): It uses the SQL language It provides JDBC or ODBC drives which allows interfacing with other BI software for example It supports many formats: files: CSV, TSV, delimited, JSON related to Hadoop: ORC, Apache Avro, Parquet log files: Logstash, AWS CloudTrail, Apache WebServer S3 Website S3 can host websites’ static content The bucket must be activated in this way The access URL is of the form: &amp;lt;bucket&amp;gt;.s3-website.&amp;lt;region&amp;gt;.amazonaws.com &amp;lt;bucket&amp;gt;.s3-website-&amp;lt;region&amp;gt;.amazonaws.com S3 CORS (Cross-Origin Resource Sharing ) A website that refers to resources on a S3 Bucket may need to configure a Header CORS The bucket’s DNS name must be autorized in the HTTP Header Access-Control-Allow-Origin"
    } ,
  
    {
      "title"    : "Using AWS CLI (and AWS SDK)",
      "url"      : "/en/aws-cli-sdk/",
      "img"      : "PHBpY3R1cmU+CiAgPHNvdXJjZSBzcmNzZXQ9Ii9hc3NldHMvaW1nL2F3cy1jbGktc2RrLWRvY3VtZW50YXRpb24tMzAwLTU4OWUwNjMyMy5hdmlmIDEuMHgsIC9hc3NldHMvaW1nL2F3cy1jbGktc2RrLWRvY3VtZW50YXRpb24tNjAwLTU4OWUwNjMyMy5hdmlmIDIuMHgiIHR5cGU9ImltYWdlL2F2aWYiPgogIDxzb3VyY2Ugc3Jjc2V0PSIvYXNzZXRzL2ltZy9hd3MtY2xpLXNkay1kb2N1bWVudGF0aW9uLTMwMC00OGUyNTRjMDcuanBnIDEuMHgsIC9hc3NldHMvaW1nL2F3cy1jbGktc2RrLWRvY3VtZW50YXRpb24tNjAwLTQ4ZTI1NGMwNy5qcGcgMi4weCIgdHlwZT0iaW1hZ2UvanBlZyI+CiAgPGltZyBzcmM9Ii9hc3NldHMvaW1nL2F3cy1jbGktc2RrLWRvY3VtZW50YXRpb24tMzAwLTQ4ZTI1NGMwNy5qcGciIGFsdD0iVXNpbmcgQVdTIENMSSAoYW5kIEFXUyBTREspIj4KPC9waWN0dXJlPgo=",
      "isencoded" : "y",
      "tags"     : "AWS, CLI, Profile, STS, MFA, SigV4, Instance-Metadata, SDK, Exponential-Backoff, Documentation",
      "content"  : "The AWS CLI is an open source tool built using the AWS SDK for Python (Boto3) that provides commands to interact with AWS services. With minimal configuration, you can use all the features provided by the AWS management console from your favorite terminal. AWS SDKs provide an API for different programming languages (Python, Java, JavaScript, C++, .NET, GO, PHP, Ruby,…) to programmatically build and use AWS services. In this article, we will see some tips to make the best use of these 2 tools. CLI AWS Installation AWS CLI Profiles Temporary session Priorities of Credentials For AWS CLI For AWS SDK Good Practices Signing HTTP Request Debugging Policy Simulator Dry Run Message AWS EC2 Instance Metadata Examples of use Data Types Services Some instance metadata AWS SDKs AWS Limits and Backoff Limits / Quotas Exponential Backoff CLI AWS Installation AWS CLI is available in 2 versions: Version 2: the latest and supports the latest features Version 1: original version, should no longer be used To install AWS CLI Version 2 on Docker, Linux, macOS or Windows, refer to the AWS documentation https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html After a successful installation, you should be able to run the following commands: % which aws /usr/local/bin/aws % aws --version aws-cli/2.2.16 Python/3.8.8 Darwin/20.6.0 exe/x86_64 prompt/off Zsh AWS CLI Profiles It is possible to register multiple user accounts in AWS CLI. This is called Profiles. To do this, once your default account is configured, you can add a new one by executing the command: % aws configure --profile &amp;lt;name-of-other-profile&amp;gt; Zsh Now you can run an AWS CLI command under this new Profile by adding the parameter: % aws iam get-user --profile &amp;lt;name-of-other-profile&amp;gt; Zsh Temporary session When an AWS account is protected by an MFA code, it is possible to create a temporary session using AWS Security Token Service (AWS STS) to request temporary identifiers with limited privileges. To do this, run the following command: % aws sts get-session-token --serial-number &amp;lt;arn-of-your-mfa-device&amp;gt; --token-code &amp;lt;token-code-from-your-mfa-device&amp;gt; --duration-seconds &amp;lt;session-duration-in-seconds&amp;gt; { &quot;Credentials&quot;: { &quot;AccessKeyId&quot;: &quot;ASIH9FS4N0N76VVTDFDG5C&quot;, &quot;SecretAccessKey&quot;: &quot;4h0f4j88H9L/HT4I0xjvE9-jrf72jkISjklbjEvRHfn&quot;, &quot;SessionToken&quot;: &quot;EJu6g3JpZ2luX2VjEDoaCWV.../...wvA5a5mfFWUOrH62fDFYbmUW5j31k1r/igIgP14W374njGO+mbO5+MeYKGoaLvHAC0SFdXh9sjopz2wq7gEIQxABGgw3OTc1MTc4.../...k+4b6sGGym82jlaPI=&quot;, &quot;Expiration&quot;: &quot;2021-07-26T10:10:59+00:00&quot; } } Zsh You get new Credentials, valid, the time of the session. Priorities of Credentials It is possible to set Credentials in several places. There is therefore a priority order that must be known to fully understand the adverse effects that this can cause. For AWS CLI Passed in command line (–region, –output, –profile) Passed in ENVIRONMENT VARIABLES (AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, AWS_SESSION_TOKEN) Saved in the /. aws/credentials file generated by the aws configure command Saved in the /. aws/config­ file generated by the aws configure command Recorded in Container Credentials (for ECS Tasks) Saved in the EC2 Instances Profiles For AWS SDK Passed in Language System Properties Passed in ENVIRONMENT VARIABLES (AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, AWS_SESSION_TOKEN) Saved in the default /. aws/credentials file in many SDKs Recorded in Container Credentials (for ECS Tasks) Saved in the EC2 Instances Profiles Good Practices To avoid any pitfalls with Credentials, here are some rules to follow: NEVER save Credentials in the code!! But rather define the Credentials in the best place in the priority chain: If you are inside AWS, use Roles IAM (EC2 Instance Roles, ECS Roles, Lambda Roles) If you are outside the AWS network, use environment variables or Profiles Signing HTTP Request When using AWS CLI or AWS SDK, HTTP calls to AWS are automatically signed. The protocol used is called Signature Version 4 (SigV4) and comes from AWS. It comes in two possible forms: HTTP headers (Authorization header) URL parameters (Query string) Debugging Here are some tools that will help you debug and understand what’s going on if there are problems: Policy Simulator It may be useful to check the access rights to an AWS resource based on a User, Group or Role. There is a tool in AWS that can run these tests, the Policy Simulator: https://policysim.aws.amazon.com/ Dry Run It may also be useful to test an AWS CLI command by simulating its execution. AWS CLI commands have an option for this: Example of simulated creation of an EC2 instance: % aws ec2 run-instances --dry-run --region eu-west-3 --image-id ami-062fdd189639d3e93 --instance-type t2.micro An error occurred (DryRunOperation) when calling the RunInstances operation: Request would have succeeded, but DryRun flag is set. Zsh Because the command is run in dry-run mode, if successful, it returns DryRunOperation. If failed, it would return UnauthorizedOperation. Message Some AWS CLI commands return a encoded authorization message describing the problem. This message must be decoded to be understandable. To do this, you can use the command: % aws decode-authorization-message --encoded-message &amp;lt;encoded-message&amp;gt; Zsh AWS EC2 Instance Metadata Instances Metadata are data relating to an EC2 instance: they are accessible from the instance and allow you not to have to use IAM Role since this data has already been loaded into the instance for its configuration or creation. They are available at: http://169.254.169.254/latest/meta-data/ Note that this is a local address and therefore only accessible from the EC2 instance. Examples of use Data Types ec2-user ~ % curl http://169.254.169.254/latest/ dynamic meta-data user-data Zsh There are 3 types of data accessible from an EC2 instance as we can see in the output of this command. Services You can access the instance metadata from a running instance using one of the following methods: Instance Metadata Service Version 1 (IMDSv1) – request/response method Instance Metadata Service Version 2 (IMDSv2) – session-oriented method When using session-oriented requests (IMDSv2), you create a session token that defines the duration of the session, which must be a minimum of one second and a maximum of six hours. During the specified period, you can use the same session token for subsequent requests. Here’s how to retrieve a session token: ec2-user ~ % TOKEN=`curl -X PUT &quot;http://169.254.169.254/latest/api/token&quot; -H &quot;X-aws-ec2-metadata-token-ttl-seconds: 21600&quot;` Zsh You can then use it in the following commands, the time of the session: ec2-user ~ % curl -H &quot;X-aws-ec2-metadata-token: $TOKEN&quot; -v http://169.254.169.254/latest/meta-data/ami-id Zsh Some instance metadata Here is a description of some metadata of instances that may be useful: Path de la Metadata Description ami-id The AMI ID used to launch the instance. ami-launch-index If you started more than one instance at the same time, this value indicates the order in which the instance was launched. The value 0 indicates the first instance launched. block-device-mapping/ami The virtual device that contains the root/boot filesystem. block-device-mapping/ebsN Virtual devices associated with any Amazon EBS volume. Amazon EBS volumes are only available in the metadata if they were present at the time of launch or when the instance was last started. The N indicates the Amazon EBS volume index (such as ebs1 or ebs2). events/recommendations/rebalance Approximate time, UTC, at which the rebalancing recommendation notification EC2 is issued for the instance. Here is an example of metadata for this category&amp;nbsp;: {&quot;noticeTime&quot;: &quot;2020-11-05T08:22:00Z&quot;}. This category is only available after the notification has been issued. hostname The private IPv4 DNS host name of the instance. In case of multiple network interfaces are present, this refers to the eth0 device (the device whose device number is 0). iam/info If an IAM role is associated with the instance, it contains information about the last update of the instance profile, among which the date of last update up-to-date (LastUpdated), InstanceProfileArn and InstanceProfileId of the instance. Otherwise, absent. iam/security-credentials/role-name If an IAM role is associated with the instance, role-name is the name of the role and role-name contains the temporary security identification information associated with the role. Otherwise, absent. instance-id The ID of this instance. instance-type The type of instance. kernel-id The kernel ID launched with the instance, if applicable. local-hostname The private IPv4 DNS host name of the instance. In case of multiple network interfaces are present, this refers to the eth0 device (the device whose device number is 0). local-ipv4 The private IPv4 address of the instance. In case several network interfaces are present, this refers to the eth0 device (the device whose number of device is 0). network/interfaces/macs/mac/ipv4-associations/public-ip Private IPv4 addresses that are associated with each public IP address and assigned at this interface. network/interfaces/macs/mac/local-hostname The local host name of the interface. network/interfaces/macs/mac/local-ipv4s Private IPv4 addresses associated with the interface. network/interfaces/macs/mac/public-hostname The public DNS of the interface (IPv4). This category is only returned if the enableDnsHostnames is defined as true. placement/availability-zone The availability zone in which the instance was launched. placement/region AWS region where the instance is launched. public-hostname The public DNS of the proceeding. This category is only returned if the enableDnsHostnames attribute is set as true. public-ipv4 The public IPv4 address. If an Elastic IP address is associated with the instance, the returned is the IP Elastic address. security-groups The names of the security groups applied to the proceeding. After launch, you can change the security groups of the instances. From such changes appear here and in network/interfaces/macs/mac/security groups. AWS SDKs An AWS SDK (Software Development Kit) is used to interact with AWS within an application. There are therefore many AWS SDKs depending on the different programming languages (Python, Java, C++, JavaScript, Ruby, PHP,…): The AWS CLI is itself encoded from the Python AWS SDK (also called Boto3). Some AWS services are only accessible by an AWS SDK: DynamoDB, Lambda Function,… Note that if you do not have a default Region, the AWS SDKs will interact with the default Region us-east-1. AWS Limits and Backoff Limits / Quotas There are Limits or Quotas in AWS that you must be aware of: API Rate Limits: depending on the AWS APIs, one cannot make more than a certain number of calls per second to an API Service Quotas (or Service Limits): According to the AWS Services, you cannot consume more than a certain number of AWS Services (e.g. 1152 vCPU per AWS account) Exponential Backoff When you get ThrottlingException errors when calling AWS Services, you must use the Exponential Backoff. It is a retry mechanism with a duration between each attempt that increases exponentially. It is described more precisely in this article https://aws.amazon.com/blogs/architecture/exponential-backoff-and-jitter/ AWS SDKs already implement it, so there is nothing to do But if you make calls to the AWS APIs by another means, you MUST set up such a mechanism: In case of ThrottlingException or 5xx errors Not in case of 4xx errors"
    } ,
  
    {
      "title"    : "Amazon Virtual Private Cloud",
      "url"      : "/en/aws-vpc-documentation/",
      "img"      : "PHBpY3R1cmU+CiAgPHNvdXJjZSBzcmNzZXQ9Ii9hc3NldHMvaW1nL2F3cy12cGMtZG9jdW1lbnRhdGlvbi0zMDAtNzVmNGFiNDViLmF2aWYgMS4weCwgL2Fzc2V0cy9pbWcvYXdzLXZwYy1kb2N1bWVudGF0aW9uLTYwMC03NWY0YWI0NWIuYXZpZiAyLjB4IiB0eXBlPSJpbWFnZS9hdmlmIj4KICA8c291cmNlIHNyY3NldD0iL2Fzc2V0cy9pbWcvYXdzLXZwYy1kb2N1bWVudGF0aW9uLTMwMC03YTU4ZjlhMDQuanBnIDEuMHgsIC9hc3NldHMvaW1nL2F3cy12cGMtZG9jdW1lbnRhdGlvbi02MDAtN2E1OGY5YTA0LmpwZyAyLjB4IiB0eXBlPSJpbWFnZS9qcGVnIj4KICA8aW1nIHNyYz0iL2Fzc2V0cy9pbWcvYXdzLXZwYy1kb2N1bWVudGF0aW9uLTMwMC03YTU4ZjlhMDQuanBnIiBhbHQ9IkFtYXpvbiBWaXJ0dWFsIFByaXZhdGUgQ2xvdWQiPgo8L3BpY3R1cmU+Cg==",
      "isencoded" : "y",
      "tags"     : "AWS, VPC, Subnet, Route, Gateway, NAT, Security-Group, NACL, ENI, Peering, Endpoint, PrivateLink, VPN, Documentation",
      "content"  : "Amazon Virtual Private Cloud (Amazon VPC) allows you to launch AWS resources into a virtual network that you define. This virtual network looks a lot like a traditional network but with the benefits of using the AWS infrastructure. Amazon VPC components The gateways Security Logs and Monitoring Other VPC-specific concepts Elastic Network Interfaces VPC Peering VPC Endpoint and AWS PrivateLink VPN Connections Amazon VPC components Amazon VPC components The key concepts of VPC are: Virtual Private Cloud (VPC): A virtual private network at a Region level Subnet: A subnet (a range of IP or CIDR addresses for Classless Inter-Domain Routing) at the AZ level: Public: accessible from the Internet Private: not accessible from the Internet Route table: A set of rules, called Routes, that are used to determine where network traffic is directed (from the Internet and between Subnets) The gateways Gateways architecture in Amazon VPC Internet Gateway: Network gateway that allows VPC instances to access the Internet and be accessible from the Internet Public Subnets are because they have a Route to an Internet Gateway NAT Gateway: A NAT gateway is a Network Address Translation (NAT) service It allows instances of a private Subnet to connect to services outside the VPC (but external services cannot initiate a connection to those instances) It is a service fully managed by AWS NAT Instance: AMI (Amazon Machine Image) used as NAT It is entirely under the responsibility of the user Security Security is a shared responsibility between AWS and the user. This is what Amazon calls the Shared Responsibility Model. It includes: Cloud Security: AWS is responsible for protecting the infrastructure that runs AWS services in the AWS Cloud Cloud Security: User responsibility is determined by the AWS service they are using Here are 2 important AWS services that apply the shared responsibility model when using Amazon VPC: Implementation of a shared responsibility model Security Group (Instance Level): A Security Group acts as a virtual firewall of an instance or ENI to control incoming and outgoing traffic Up to five Security Groups can be assigned to one instance Security Groups only act at instance level, not subnet level Rules may include IP addresses or other Security Groups and contain only ALLOWs (all that is not ALLOW is blocked) Network ACL (Subnet Level): A Network Access Control List (ACL) is an optional security layer for a VPC that acts as a firewall to control incoming and outgoing traffic from one or more subnets Network ACLs can be configured with rules similar to those of Security Groups to add an additional security layer to a VPC Rules can only include IP addresses and contain ALLOWs / DENYs Logs and Monitoring You can use the following automated monitoring tools to monitor your VPC components and report any issues: Flow Logs: Flow Logs capture IP traffic entering and exiting your VPC’s network interfaces You can create a flow log for a VPC, Subnet or Elastic Network Interface It also works for the network flows of services managed by AWS: ELB, ElastiCache, RDS, Aurora,… Stream log data is published in CloudWatch Logs or Amazon S3 to diagnose ACL security rules that are too restrictive or permissive NAT Gateways monitoring: Monitoring a NAT Gateway is done using CloudWatch, which collects information from your NAT gateway and creates near real-time measurements Other VPC-specific concepts Elastic Network Interfaces A ENI (also called the network interface in AWS) is a logical network component in a VPC that represents a virtual network card Each instance of a VPC has a default network interface (the primary network interface) that is assigned to a private IPv4 address from the VPC IPv4 address range You cannot detach a primary network interface from an instance. But you can create and attach an additional ENI to any instance of your VPC VPC Peering A VPC Peering connection is a network connection between two VPCs that allows traffic to flow between them and privately Instances in either VPC can communicate with each other as if they were in the same VPC You can create a peering VPC connection between your own VPCs, with a VPC in another AWS account, or with a VPC in another AWS region The CIDR (IP address range) of each VPC must not overlap AWS uses the existing infrastructure of a VPC to create a VPC Peering connection; it is neither a gateway nor a VPN connection from site to site AWS, and does not depend on separate physical hardware There is no single point of failure for communication or a bandwidth bottleneck. VPC Endpoint and AWS PrivateLink An Endpoint allows you to connect to an AWS service from a private address without going through the public network This connection is called an AWS PrivateLink Provides even more security and improves latency to access an AWS service There are 2 types of Endpoints VPCs: VPC Endpoint Gateway: S3 and DynamoDB VPC Endpoint Interface: other services VPN Connections AWS Site-to-Site VPN: It is an IPsec VPN connection between an Amazon VPC and an enterprise network (on-premise) On the AWS side, 2 VPN terminals (tunnels) allow an automatic failover switch Encrypted traffic goes through the Internet AWS Direct Connect (DX): AWS Direct Connect connects an enterprise network to an AWS Direct Connect location via a standard Ethernet fiber optic cable Traffic doesn’t go to the Internet and is private It takes at least 1 month to set up because there is a network infrastructure to set up (fiber optic, …)"
    } ,
  
    {
      "title"    : "Amazon Route 53",
      "url"      : "/en/aws-route-53-documentation/",
      "img"      : "PHBpY3R1cmU+CiAgPHNvdXJjZSBzcmNzZXQ9Ii9hc3NldHMvaW1nL2F3cy1yb3V0ZS01My1kb2N1bWVudGF0aW9uLTMwMC1mNTBhOTY3NDUuYXZpZiAxLjB4LCAvYXNzZXRzL2ltZy9hd3Mtcm91dGUtNTMtZG9jdW1lbnRhdGlvbi02MDAtZjUwYTk2NzQ1LmF2aWYgMi4weCIgdHlwZT0iaW1hZ2UvYXZpZiI+CiAgPHNvdXJjZSBzcmNzZXQ9Ii9hc3NldHMvaW1nL2F3cy1yb3V0ZS01My1kb2N1bWVudGF0aW9uLTMwMC04NmRmMTdjNjUuanBnIDEuMHgsIC9hc3NldHMvaW1nL2F3cy1yb3V0ZS01My1kb2N1bWVudGF0aW9uLTYwMC04NmRmMTdjNjUuanBnIDIuMHgiIHR5cGU9ImltYWdlL2pwZWciPgogIDxpbWcgc3JjPSIvYXNzZXRzL2ltZy9hd3Mtcm91dGUtNTMtZG9jdW1lbnRhdGlvbi0zMDAtODZkZjE3YzY1LmpwZyIgYWx0PSJBbWF6b24gUm91dGUgNTMiPgo8L3BpY3R1cmU+Cg==",
      "isencoded" : "y",
      "tags"     : "AWS, Route-53, Documentation",
      "content"  : "Amazon Route 53 is a highly available and scalable Domain Name System (DNS) web service. With Route 53, you can register domains, route traffic to the resources where your domains are hosted, and check the health of your resources. You can also route traffic based on the health of your resources. This guide explains how to register domains, configure DNS, and configure health checks using the Route 53 console. Record Types Time To Live AWS Features Health Check Routing policies Record Types In AWS, you can record 4 types of Records: A: Match a hostname to an IPv4 A record Can match a hostname with multiple IPv4 AAAA: Match a hostname to an IPv6 A record Can match a hostname with multiple IPv6 CNAME: Match a hostname to another hostname Only works for root domain names (e.g. www.example.com -&amp;gt; myserver.12345.mymachine.com) Alias: Match a hostname to an AWS resource Works for root domain names (apex) or not root (e.g. example.com -&amp;gt; server-12345.eu-west-3.alb.amazonaws.com) It is free (unlike CNAME) It integrates a native Health Check Hostnames can be public, accessible from a client browser, for example, or private, accessed by an AWS instance within a private network. Time To Live A DNS record is associated with a TTL that tells the client the validity time of a DNS record. It can be: high: for example 24 hours, risk of obsolete registration low: for example 60s, allows a record to be quickly updated AWS Features Route 53 brings additional features compared to a classic DNS: Health Check A resource is marked healthy once it has passed x Heath Checks (default 3) A resource is marked unhealthy once it has failed x Heath Checks (default 3) Route 53 returns only endpoints healthy About 15 Health Checkers are run to test the health of an endpoint They each perform a test every 30s by default (can be lowered to 10s) This means that they run a query every 2s as a whole Probes can be configured in TCP, HTTP or HTTPS (but do not check the validity of the certificate in the latter case) Health Check can be combined with CloudWatch Routing policies Simple: Redirection to a single resource (but may be multiple endpoints/IPs) No Health Check! The Client receives all IPs and selects one at random to access the resource Multi value: Like Simple but with a Healt Check: only Healthy endpoints will be returned (unlike Simple) Failover: As part of a Disaster Recovery with a Primary/Secondary server architecture Returns the IP of the Primary server as long as it is Healthy, the Secondary server otherwise However, there is no Health Check on the Secondary server Weighted: Assigns a weight to each endpoint of the resource Allows you to test 10% of traffic on a new version of an application Useful to redirect traffic between 2 Regions Supports Health Check Customer receives only one IP (and is not aware of the other endpoints) Latency: Directs to the AWS Region that has the lowest latency from the Client’s perspective This is not necessarily the nearest Region (although this should be the case for most of it) Client receives only one IP (and is not aware of the other endpoints) Geolocation: Directs traffic to the Region closest to the Client Geoproximity: Directs traffic to the Region closest to the Client but with a bias that allows to modify the target Regions Configurable in Route 53 Traffic Flow A positive bias (from 1 to 99) will artificially increase the size of a Region A negative bias (from -99 to -1) will reduce the size of a Region Resources can be either: An AWS resource (linked to an AWS Region) A non-AWS resource (this will require to set the latitude/longitude of the resource)"
    } ,
  
    {
      "title"    : "Amazon Aurora - The Relational Database optimized for the Cloud",
      "url"      : "/en/aws-aurora-documentation/",
      "img"      : "PHBpY3R1cmU+CiAgPHNvdXJjZSBzcmNzZXQ9Ii9hc3NldHMvaW1nL2F3cy1hdXJvcmEtZG9jdW1lbnRhdGlvbi0zMDAtZGViNWFiNTQ2LmF2aWYgMS4weCwgL2Fzc2V0cy9pbWcvYXdzLWF1cm9yYS1kb2N1bWVudGF0aW9uLTYwMC1kZWI1YWI1NDYuYXZpZiAyLjB4IiB0eXBlPSJpbWFnZS9hdmlmIj4KICA8c291cmNlIHNyY3NldD0iL2Fzc2V0cy9pbWcvYXdzLWF1cm9yYS1kb2N1bWVudGF0aW9uLTMwMC0wN2JjYmExYjUuanBnIDEuMHgsIC9hc3NldHMvaW1nL2F3cy1hdXJvcmEtZG9jdW1lbnRhdGlvbi02MDAtMDdiY2JhMWI1LmpwZyAyLjB4IiB0eXBlPSJpbWFnZS9qcGVnIj4KICA8aW1nIHNyYz0iL2Fzc2V0cy9pbWcvYXdzLWF1cm9yYS1kb2N1bWVudGF0aW9uLTMwMC0wN2JjYmExYjUuanBnIiBhbHQ9IkFtYXpvbiBBdXJvcmEgLSBUaGUgUmVsYXRpb25hbCBEYXRhYmFzZSBvcHRpbWl6ZWQgZm9yIHRoZSBDbG91ZCI+CjwvcGljdHVyZT4K",
      "isencoded" : "y",
      "tags"     : "AWS, Aurora, RDS, Documentation",
      "content"  : "Amazon Aurora is a relational database, compatible with MySQL and PostgreSQL and created by Amazon for the Cloud. Amazon Aurora is up to 5x faster than a standard MySQL database and 3x faster than a standard PostgreSQL database. It relies on Amazon RDS and extends its features. Features of Amazon Aurora High performance and scalability Availability Security Features of Amazon Aurora General architecture of Amazon Aurora High performance and scalability High Performance: It is connected to the Cloud It supports Low Latency reading with up to 15 Read Replicas in Multi-AZ Depending on the option chosen, one can have 1 Master in Writing or several connected to the same storage volume Scaling: Automatic Storage Auto Scaling increase in 10 GB increments up to 128 TB The number of Read Replicas can be managed by an Auto Scaling Group A Serverless option allows you to manage intermittent and unpredictable workloads Availability High Availability: 6x data replication in a shared storage volume on 3 AZs Continuous backup in Amazon S3 1 Endpoint for Writing and 1 for Reading Automatic failover in case of unavailability of an AZ in less than 30s Support the cross-Region (Database Location: Global option); Data are replicated across multiple Regions Write operations on the Primary Region are replicated in the Secondary Regions with latency &amp;lt; 1s Automatic failover in case of unavailability of a Region Data can be restored at any point of time whatever the time of the last backup (Backtrack option) Security Seamless maintenance of Aurora instances Network isolation with Amazon VPC Data encryption at rest and in transit with AWS KMS as well as storage volumes, backups, snapshots and replicas"
    } ,
  
    {
      "title"    : "Amazon RDS - Relational Databases in AWS",
      "url"      : "/en/aws-rds-documentation/",
      "img"      : "PHBpY3R1cmU+CiAgPHNvdXJjZSBzcmNzZXQ9Ii9hc3NldHMvaW1nL2F3cy1yZHMtZG9jdW1lbnRhdGlvbi0zMDAtMGRmOGMzNjYzLmF2aWYgMS4weCwgL2Fzc2V0cy9pbWcvYXdzLXJkcy1kb2N1bWVudGF0aW9uLTYwMC0wZGY4YzM2NjMuYXZpZiAyLjB4IiB0eXBlPSJpbWFnZS9hdmlmIj4KICA8c291cmNlIHNyY3NldD0iL2Fzc2V0cy9pbWcvYXdzLXJkcy1kb2N1bWVudGF0aW9uLTMwMC0xMjhkNGJmZjQuanBnIDEuMHgsIC9hc3NldHMvaW1nL2F3cy1yZHMtZG9jdW1lbnRhdGlvbi02MDAtMTI4ZDRiZmY0LmpwZyAyLjB4IiB0eXBlPSJpbWFnZS9qcGVnIj4KICA8aW1nIHNyYz0iL2Fzc2V0cy9pbWcvYXdzLXJkcy1kb2N1bWVudGF0aW9uLTMwMC0xMjhkNGJmZjQuanBnIiBhbHQ9IkFtYXpvbiBSRFMgLSBSZWxhdGlvbmFsIERhdGFiYXNlcyBpbiBBV1MiPgo8L3BpY3R1cmU+Cg==",
      "isencoded" : "y",
      "tags"     : "AWS, RDS, Documentation",
      "content"  : "There are 2 major Relational Databases offerings in AWS: RDS and Aurora. We will see in this article the characteristics of the first type of BDD, Amazon RDS. Amazon RDS Managed operations Backups Replica Read Replica Multi-ZA Replicas and Disaster Recovery Security Data Encryption at rest Data encryption in transit Network IAM Amazon RDS Amazon Relational Database Service (Amazon RDS) is a web-based service that facilitates the configuration, operation and scaling of SQL relational databases in the AWS Cloud (managed BDD). It supports different database engines: MySQL MariaDB PostgreSQL Oracle Microsoft SQL Server Which can be run on different types of instances called DB Instances: Available Instance classes: Standard, Memory Optimized or Burstable Types of Storage RDS: General Purpose SSD, Provisioned IOPS, Magnetic Managed operations The operations supported by AWS and proposed in Amazon RDS are: Maintenance of the instance (OS + BDD Engine) on programmable ranges, Automatic backups at time intervals and possible restoration of these backups A monitoring dashboard Creation of Read Replicas to speed up access to data (maximum 5) Multi-AZ configuration for failover management Vertical and horizontal scaling of DB Instances and storage volume (Storage Auto Scaling) On the other hand, it is not possible to access these instances in SSH. Backups Automatic: Daily Full Backup during the service period (as defined) Backup of transaction log every 5 minutes (restoration ensured of m-5) Retention possible from 0 (no retention!) to 35 days On request: As DB Snapshots Retention as long as desired Replica Read Replica Example of Read Replicas The replication is ASYNCHRONOUS, the data is not consistent: the replicates are accessible only in reading Up to 5 replicas can be created They can be in the same AZ, in different AZ or between different Regions (except for Microsoft SQL Server) Remarks: An application needs to update its connection string to take advantage of the Reads Replicas. A Replica can be promoted, again, into a Database, which makes it possible, for example, to carry out analysis treatments that cannot be envisaged on a BDD in Production. Data transfer between AZ or Regions is paid for on AWS. With regard to RDS Reads Replicas, only transfers between Regions are chargeable, those between AZs are included in the cost of the service. Multi-ZA Replicas and Disaster Recovery Example of Disaster Recovery Architecture with Multi-AZ RDS Replicas The creation of Replicas between AZ (multi-AZ) or between Regions (cross-Region) makes it possible to set up a disaster recovery plan (disaster recovery) in the event of a network failure, an RDS instance, an AZ or even a Region. The application architecture is then different from the Read Replica: the replication is SYNCHRONOUS and the Replicas are not accessible for reading. One can go from a single-AZ to a multi-AZ by a Snapshot and without unavailability of the database. Security Data Encryption at rest Amazon RDS instance data can be encrypted including DB Instances storage volume, backups, replicas and snapshots This encryption is configured when the BDD is created: An encrypted instance produces an encrypted Repica An unsolicited instance produces an unsolicited Replica But a snapshot is still unencrypted Encryption uses a standard AES-256 encryption key managed by AWS Key Management Service (AWS KMS). Oracle and Microsoft SQL Server can also use Transparent Data Encryption (TDE). Data encryption in transit It is done using SSL/TLS certificates with the root certificate of your AWS Region Each BDD engine has its own configuration to support on-the-fly encryption and may also depend on the version used (refer to the documentation https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/UsingWithRDS.SSL.html) For example for MySQL 5.7 and later: ALTER USER &#39;encrypted_user&#39;@&#39;%&#39; REQUIRE SSL; Network An Amazon RDS is always deployed in a private subnet Access to an RDS instance is therefore made through a Security Group IAM The connection to the BDD of an RDS instance is usually done with a login/password. IAM policies allow access rights to an RDS instance to be granted to a User IAM. In the case of MySQL and PostgreSQL, a User IAM can also connect to an RDS BDD. However, there are limitations (token valid 15min, no more than 256 connections per second) Identification in Amazon RDS using IAM Authentication"
    } ,
  
    {
      "title"    : "AWS Elastic Load Balancing",
      "url"      : "/en/aws-elb-documentation/",
      "img"      : "PHBpY3R1cmU+CiAgPHNvdXJjZSBzcmNzZXQ9Ii9hc3NldHMvaW1nL2F3cy1lbGItZG9jdW1lbnRhdGlvbi0zMDAtMzI5MGZjMWExLmF2aWYgMS4weCwgL2Fzc2V0cy9pbWcvYXdzLWVsYi1kb2N1bWVudGF0aW9uLTYwMC0zMjkwZmMxYTEuYXZpZiAyLjB4IiB0eXBlPSJpbWFnZS9hdmlmIj4KICA8c291cmNlIHNyY3NldD0iL2Fzc2V0cy9pbWcvYXdzLWVsYi1kb2N1bWVudGF0aW9uLTMwMC1jNmEzYzBhODkuanBnIDEuMHgsIC9hc3NldHMvaW1nL2F3cy1lbGItZG9jdW1lbnRhdGlvbi02MDAtYzZhM2MwYTg5LmpwZyAyLjB4IiB0eXBlPSJpbWFnZS9qcGVnIj4KICA8aW1nIHNyYz0iL2Fzc2V0cy9pbWcvYXdzLWVsYi1kb2N1bWVudGF0aW9uLTMwMC1jNmEzYzBhODkuanBnIiBhbHQ9IkFXUyBFbGFzdGljIExvYWQgQmFsYW5jaW5nIj4KPC9waWN0dXJlPgo=",
      "isencoded" : "y",
      "tags"     : "AWS, ELB, Auto-Scaling, ASG, SSL-TLS, SNI, Sticky-Session, Target-Group, Documentation",
      "content"  : "Elastic Load Balancing automatically distributes your incoming traffic across multiple targets, such as EC2 instances, containers, and IP addresses, in one or more Availability Zones. It monitors the health of its registered targets and routes traffic only to the healthy targets. You can select the type of load balancer that best suits your needs. Let’s see how it works. Elastic Load Balancer ELB Types Classic Load Balancer Application Load Balancer Network Load Balancer Gateway Load Balancer AWS Auto Scaling Related concepts Target Group Sticky Session Cross-Zone Load Balancing SSL/TLS and SNI Elastic Load Balancer Example of using an ELB An ELB is a central element of an applicative architecture. It allows to: Distribute the load across multiple servers Expose a single entry point for the application Separate external traffic from the internal one Provide SSL/TLS termination Perform health checks of the underlying instances The AWS ELB is fully managed, which means that AWS makes sure it is running smoothly, kept up to date, and is high availability. ELB Types At the moment, there are 4 that we will detail below. Classic Load Balancer ELB v1 generation, you should not use it Supports HTTP, HTTPS and TCP protocols Single DNS name port of entry Distributes load across multiple AZs (Multi-AZs) Supports a single application Application Load Balancer Latest ELB v2 generation Supports HTTP, HTTPS (HTTP/2) and WebSocket (Layer 7) protocols Single DNS name port of entry Can support multiple applications: Several applications (multi Target Groups/ Containers) using a routing table (depending on the Hostname, Path, Query String or Header) HTTP redirect -&amp;gt; HTTPS Port mapping Visibility of the Client IP: ALB terminates connection, targets only see ALB IP The Client IP is located in the X-Forwarded-For header (other Forwarded: X-Forwarded-Port, X-Forwarded-Proto) The Security Group of targets must integrate the ALB Security Group Network Load Balancer ELB Generation v2 It supports TCP, UDP and TLS (Layer 4) protocols His interest in ALB is that it is much more efficient and can handle millions of req/s with ultra-low latencies Point of entry, static IP via AZ or fixed IP (also via AZ) via Elastic IP Like ALB, it can support multiple applications through Target Groups Visibility of the Client IP: The NLB does not touch the connection, the targets receive the Client’s IP The Security Group of targets must integrate the IPs of the Clients (0.0.0.0/0 for public access) Gateway Load Balancer Generation v2 of ELB, this service was introduced this year. As its name suggests, it is a network gateway that redirects application flows to a virtual appliance. This redirection is completely transparent for the network flow and the target server. A virtual appliance is a VM image that replaces hardware equipment. It is generally used to do security analysis (WAF, NGFW, anti-DDoS) or to monitor or log application flows. AWS Auto Scaling Example of ASG Scale Out AWS Auto Scaling allows you to automatically size the number of our instances (Horizontal Scale). It allows to automatically decrease (Scale In) or increase (Scale Out) this number according to predefined criteria. To do this, you configure an Auto Scaling Group with a minimum, maximum and initial number of instances to be created as well as a Scaling policy. Its great advantage is that it automatically records these instances in the ELB. Related concepts Here are some notions to know and that are related to Load Balancers in general or specific to AWS. Target Group Target Groups allow multiple targets to be associated under a single reference. These targets may be: EC2 instances (which can be managed by an Auto Scaling Group) ECS tasks Lambda functions IP addresses (always private) An ALB performs its Health Check at the Target Group level. Sticky Session Sticky Sessions are a mechanism to route customer traffic to a single target in a Target Group. This is necessary for Stateful applications in order not to lose customer information between requests. It is insured by a cookie and is available for all ELB. Cross-Zone Load Balancing When this option is enabled, ELB distribute traffic fairly to all registered targets regardless of their AZ. If this option is disabled, the traffic is distributed among the AZ only regardless of the number of targets in each AZ and not of the underlying instances number. Depending on the type of ELB and how it was created (by API, CLI or Console Management), this option may or may not be enabled by default. SSL/TLS and SNI An SSL certificate encrypts Internet traffic and verifies the identity of a server. There are 2 protocols to ensure this encryption: SSL, Secure Sockets Layer, is the old encryption protocol and is no longer used at this time (even if its name remains) TLS, Transport Layer Security, is the new, more secure protocol. SNI, Server Name Indication, is part of the TLS protocol. It allows you to tell a server the name of the Hostname you want to connect to: This allows Load Balancers that handle multiple Hostnames to know which certificate is returned and to perform the correct routing. Only ALB and NLB, generation v2, know how to manage the SNI."
    } ,
  
    {
      "title"    : "Storage for AWS EC2 Instances",
      "url"      : "/en/aws-ec2-storage-documentation/",
      "img"      : "PHBpY3R1cmU+CiAgPHNvdXJjZSBzcmNzZXQ9Ii9hc3NldHMvaW1nL2F3cy1lYzItc3RvcmFnZS1kb2N1bWVudGF0aW9uLTMwMC01ZmMwMWZjNGMuYXZpZiAxLjB4LCAvYXNzZXRzL2ltZy9hd3MtZWMyLXN0b3JhZ2UtZG9jdW1lbnRhdGlvbi02MDAtNWZjMDFmYzRjLmF2aWYgMi4weCIgdHlwZT0iaW1hZ2UvYXZpZiI+CiAgPHNvdXJjZSBzcmNzZXQ9Ii9hc3NldHMvaW1nL2F3cy1lYzItc3RvcmFnZS1kb2N1bWVudGF0aW9uLTMwMC02OGU5YWYyMGIuanBnIDEuMHgsIC9hc3NldHMvaW1nL2F3cy1lYzItc3RvcmFnZS1kb2N1bWVudGF0aW9uLTYwMC02OGU5YWYyMGIuanBnIDIuMHgiIHR5cGU9ImltYWdlL2pwZWciPgogIDxpbWcgc3JjPSIvYXNzZXRzL2ltZy9hd3MtZWMyLXN0b3JhZ2UtZG9jdW1lbnRhdGlvbi0zMDAtNjhlOWFmMjBiLmpwZyIgYWx0PSJTdG9yYWdlIGZvciBBV1MgRUMyIEluc3RhbmNlcyI+CjwvcGljdHVyZT4K",
      "isencoded" : "y",
      "tags"     : "AWS, EC2, EBS, EFS, Documentation",
      "content"  : "Here we will see the different storage services that can be used by an EC2 instance as well as their characteristics and their use cases. EBS Volume Types of EBS gp2 or gp3 (SSD): io1 or io2 (SSD) or Provisioned IOPS SSD (PIOPS) st1 (HDD) or Throughput Optimized: sc1 (HDD) or Cold HDD: EBS Multi-attach EFS Modes and classes Instance Store EBS Volume Amazon Elastic Block Store (Amazon EBS) is a web service that provides block level storage volumes for use with EC2 instances. EBS volumes are highly available and reliable storage volumes that can be used as hard drive. It’s a Network drive: It uses the network to communicate with the instance There might be a bit of latency Can be dettached and attached quickly while the instance is running Can only be mounted to one instance at a time (except for multi-attached - see bellow) It’s locked to a specific Availibility Zone: You could not bound to an instance whitin another A.Z. But you can snapshot it and copy it to another A.Z. It can be deleted or not after instance termination: So data can be persistent or not Links to an EBS in the same AZ with and without multi-attach Types of EBS gp2 or gp3 (SSD): General purpose SSD volume with a balanced price and performance 1 Gib to 16 TiB gp2: IOPS is linked with the volume size Max IOPS is 16,000 at 5,333 GiB Small volume size can burst to 3,000 gp3: IOPS starts at 3,000 IOPS and throughput can increase independently up to 16,000 and 1000 MiB/s Use cases: Boot volume, non-Prod environment io1 or io2 (SSD) or Provisioned IOPS SSD (PIOPS) The Highest performance SSD for low-latency and high-throughput workloads Supports EBS Multi-attach io1 / io2: 4 GiB to 16 TiB Max IOPS 64,000 with Nitro EC2 otherwise 32,000 with others instance type PIOPS can increase independently of storage size io2 Block Express: 4 GiB to 64 TiB Sub-millisecond latency Max PIOS 256,000 with IOPS:GiB ratio of 1000:1 (that means max PIOPS is reached from 256 GiB) Use cases: Databases st1 (HDD) or Throughput Optimized: Low cost HDD volume designed for frequently accessed and throughput intensive workloads Can NOT be a Boot volume 125 MiB to 16 TiB Max IOPS is 500 and throughput 500 MiB/s Use cases: Big Data, Data Warehouse, Log Processing sc1 (HDD) or Cold HDD: The Lowest cost HDD volume designed for less frequently accessed workloads Can NOT be a Boot volume 125 MiB to 16 TiB Max IOPS is 250 and throughput 250 MiB/s Use cases: Data that is infrequently accessed where lower cost is important EBS Multi-attach Only for io1 and io2 family You can attach the same EBS volume to multiple EC2 instances within the same A.Z. Each instance has full read / write permissions to the volume But must use a clustered file system to preserve data consistency EFS Amazon Elastic File System (Amazon EFS) provides file storage for your Amazon EC2 instances. It’s a managed NFS (Network File System) that can be mounted to multiple EC2 instances: It’s increasing automatically, so you don’t have to provision anything and you pay-per-use Is multi-AZ You have to use Security Group to access to EFS Uses NFSv4.1 protocol and is only compatible with POSIX Operating System (including Linux, macOS but not Windows) Use cases: Content management, web server, data sharing. Links to EFS from multi-AZ with Security Group with Inbound Rule on Port NFS Modes and classes Scaling: 100s of concurrent NFS clients with 10+ GiB/s throughput Can grow to Petabyte Performance mode: General Purpose: latency sensitive apps like web server, CMS, … Max I/O: higher latency, throughput but highly parallel : Big Data, Media Processing, … Throughput mode: Bursting: depends on the storage size (e.g. 1 TB = 50 MiB/s with burst up to 100 MiB/s) Provisioned: you set your desired throughput (e.g; 1 TB with 1 GiB/s) Storage Tiers: Standard: for frequently accessed files Infrequent Access: move file after n-days to EFS-IA which costs less but cost to retrieve the files Instance Store Hight performance harware disk with high I/O But ephemeral: Data is lost when the instance is stopped You have to manage backup and restoration yourself if you don’t want it to be deleted Good for cache, buffer and temporary data"
    } ,
  
    {
      "title"    : "AWS Elastic Cloud Compute",
      "url"      : "/en/aws-ec2-documentation/",
      "img"      : "PHBpY3R1cmU+CiAgPHNvdXJjZSBzcmNzZXQ9Ii9hc3NldHMvaW1nL2F3cy1lYzItZG9jdW1lbnRhdGlvbi0zMDAtYmUzZWM2YTRhLmF2aWYgMS4weCwgL2Fzc2V0cy9pbWcvYXdzLWVjMi1kb2N1bWVudGF0aW9uLTYwMC1iZTNlYzZhNGEuYXZpZiAyLjB4IiB0eXBlPSJpbWFnZS9hdmlmIj4KICA8c291cmNlIHNyY3NldD0iL2Fzc2V0cy9pbWcvYXdzLWVjMi1kb2N1bWVudGF0aW9uLTMwMC00MjIxMDQzZjEuanBnIDEuMHgsIC9hc3NldHMvaW1nL2F3cy1lYzItZG9jdW1lbnRhdGlvbi02MDAtNDIyMTA0M2YxLmpwZyAyLjB4IiB0eXBlPSJpbWFnZS9qcGVnIj4KICA8aW1nIHNyYz0iL2Fzc2V0cy9pbWcvYXdzLWVjMi1kb2N1bWVudGF0aW9uLTMwMC00MjIxMDQzZjEuanBnIiBhbHQ9IkFXUyBFbGFzdGljIENsb3VkIENvbXB1dGUiPgo8L3BpY3R1cmU+Cg==",
      "isencoded" : "y",
      "tags"     : "AWS, EC2, AMI, Security-Group, Documentation",
      "content"  : "Amazon Elastic Compute Cloud (Amazon EC2) is a web service that provides resizable computing capacity—literally, servers in Amazon’s data centers—that you use to build and host your software systems. Instance Types Naming convention Amazon Machine Image User data Security Group Instance Purchasing options Instance Types EC2 instances come with different configurations dedicated to different use cases. You can check out the list of the different configurations on: https://aws.amazon.com/ec2/instance-types/: General Purpose Compute Optimized Memory Optimized Accelarated Computing Storage Optimized Naming convention Instances’ names follow a naming convention with pattern like: &amp;lt;type&amp;gt;&amp;lt;hardware-generation&amp;gt;.&amp;lt;instance-size&amp;gt; Code For example, the instance type t4g.medium represents: Type t which is a general purpose instance, Hardware generation 4g which is based on AWS Graviton 2 ARM processor for this specific type, Instance size medium which stands for medium memory and CPU ressources, 2vCPU / 4Gio here. Amazon Machine Image An AMI is a template that contains the software configuration (Operating System, Application Server and application) required to launch your instance. You can select an AMI provided by AWS, the user community, the AWS Marketplace, or one of your own pre-built AMI. User data Script that will be launch at the first boot of the instance and will be run only once. Security Group It is fundamental in AWS network security: It controls how traffic is allowed into and out of the EC2 instances. It only contains ALLOW rules. So it acts as a firewall in front of the EC2 instance. It filters access to: Port number, IPv4 and IPv6 range, Inbound network traffic (into the instance), Outbound network traffic (out of the instance). That means when traffic is blocked, the EC2 instance won’t even see it: If you end up with a time-out issue when accessing your application, it’s probably a Security Group issue. If you encounter a connection error, then the traffic went through the Security Group and that’s an application error issue (the application did not launch, for example). A Security Group can reference rules, but also others Security Groups: Linking another Security Group (SG-DB) allow inbound traffic without knowing specific IP rule nor Port number The Security Group SG-DB attached to the Database allow traffic inbound to our EC2 instance because we also have attached the SG-DB Security Group to our EC2 instance. Instance Purchasing options AWS offers different kind of purchasing when it comes to instances and some of them can reduce cost: On-Demand instances: For short workload, with a predictable price, you pay what you use (no cost saving) Reserved and with a minimum of 1 or 3 years(up to 75% discount): Reserved instances: For long workloads (a database for example) Convertible Reserved instances: Long workloads, but you can change the instance type (from t2-2xlarge to c5-small for example)(up to 54% of discount) Scheduled Reserved instances: When you don’t need continuously an instance (for example you need an instance every Sunday per week but during one year at least) Spot instances: For short workloads with resilience but less reliable as you can lose your instance if the price you’re willing to pay for them is less than the current Spot price (provide the highest discount in AWS that can be up to 90%). Useful for batch jobs, distributed workloads, image processing, … Dedicated Host (for a period of 3 years): Reserves an entire physical server fully dedicated to your use in AWS datacenter. When you have compliance requirements or use a Bring Your Own License model (BYOL)"
    } ,
  
    {
      "title"    : "AWS Identity and Access Management",
      "url"      : "/en/aws-iam-documentation/",
      "img"      : "PHBpY3R1cmU+CiAgPHNvdXJjZSBzcmNzZXQ9Ii9hc3NldHMvaW1nL2F3cy1pYW0tZG9jdW1lbnRhdGlvbi0zMDAtNWI1NDMxMzg5LmF2aWYgMS4weCwgL2Fzc2V0cy9pbWcvYXdzLWlhbS1kb2N1bWVudGF0aW9uLTYwMC01YjU0MzEzODkuYXZpZiAyLjB4IiB0eXBlPSJpbWFnZS9hdmlmIj4KICA8c291cmNlIHNyY3NldD0iL2Fzc2V0cy9pbWcvYXdzLWlhbS1kb2N1bWVudGF0aW9uLTMwMC05NjhiODM4MmQuanBnIDEuMHgsIC9hc3NldHMvaW1nL2F3cy1pYW0tZG9jdW1lbnRhdGlvbi02MDAtOTY4YjgzODJkLmpwZyAyLjB4IiB0eXBlPSJpbWFnZS9qcGVnIj4KICA8aW1nIHNyYz0iL2Fzc2V0cy9pbWcvYXdzLWlhbS1kb2N1bWVudGF0aW9uLTMwMC05NjhiODM4MmQuanBnIiBhbHQ9IkFXUyBJZGVudGl0eSBhbmQgQWNjZXNzIE1hbmFnZW1lbnQiPgo8L3BpY3R1cmU+Cg==",
      "isencoded" : "y",
      "tags"     : "AWS, IAM, IAM-User, IAM-Group, IAM-Role, MFA, AWS-Identity, AWS-Policy, IAM-Service-Role, AWS-Trust-Policy, AWS-Principale, Documentation",
      "content"  : "AWS Identity and Access Management (IAM) is a web service for securely controlling access to AWS services. With IAM, you can centrally manage users, security credentials such as access keys, and permissions that control which AWS resources users and applications can access. AWS account root user IAM User and Group How to use Access Key? IAM Role The summury version (but not entirely accurate!) The long version (but more complex!) IAM Identity AWS Service Role Trust Policy What it does AWS account root user The role of the root account is to create AWS users It is created by default when registering on AWS. It shouldn’t be used except for creating AWS accounts setup. We can even imagine that it is used to create the first AWS account with administrator rights, and that’s it. IAM User and Group An IAM User is one physical person and only one : AWS user accounts must be protected by a strong Password Policy and Multi-Factor Authentication (MFA) to access AWS Management Console. For programmatic access via CLI (AWS Command Line Interface) from a console or SDK (AWS Software Development Kit) from an application, users can use Access Keys (an Access Key ID + an Access Key Secret) to access AWS Services. A IAM Policy grants a specific set of permissions and can be linked to any IAM identity: User, Group or Role. User permissions (IAM Policies) are attached either at the user level directly or even better, at the Groups level to which users belong. Multiple Groups can be attached to Users You should never EVER share your AWS User account or your Access Key!! How to use Access Key? Let’s take the connection example to an EC2 instance. Set the file permissions To secure the PEM file containing your Access Key ID and its Secret Key, AWS verifies that your PEM file permissions are secure. That means you should always set these permissions before using it: chmod 0400 &amp;lt;ACCESS-KEY-FILE&amp;gt;.pem Bash Connect to your instance On Linux instances, username is ec2-user. Let’s ssh into it: ssh -i &amp;lt;ACCESS-KEY-FILE&amp;gt;.pem ec2-user@&amp;lt;PUBLIC-IP-SERVER&amp;gt; Bash IAM Role All security in AWS is based on IAM Roles and this is probably the most difficult part to understand. Let’s see the concepts of IAM Roles IAM through a progressive approach. The summury version (but not entirely accurate!) A IAM Role authorizes an AWS Service to access information from another AWS Service. In the example below, an EC2 Instance uses an IAM Role to access a S3 Bucket in Read-Only: An IAM Role grants access to an EC2 instance to access an S3 bucket The long version (but more complex!) In order to fully understand the concepts behind IAM Roles, we need to define some terms specific to AWS. IAM Identity IAM User and IAM Role are both IAM Identities It has Permissions Policies that determine what identity can and cannot do in AWS So, User and Role are the same concept in AWS. What differentiates them: A User is associated with an individual in a unique way and has Long Life Identifiers, such as a password or access keys A Role is for anyone who needs it (so it may be a User) and has temporary identifiers, for the duration of the Role session AWS Service Role This is a Role for a Service, which is a set of permissions that allow this Service to access, in your account and on your behalf, the AWS Services it needs It is therefore a Role intended for a Service Trust Policy A Trust Policy defines the Principales you trust to endorse a Role. A Principale can be User, Role, AWS account or Service. Therefore one can exactly define to whom a Role is intended to What it does Some examples of using Roles (not exhaustive and in no particular order!): Allow a Developer to temporarily access, in read-only, a Production environment Allow a Load Balancer to (1) read CloudWatch metrics and (2) create new EC2 instances as required Allow a certain application to have read/write access to a specific directory of a S3 Bucket What to rememberIt is always best to use a Role to manage access to AWS resources"
    } ,
  
    {
      "title"    : "AWS documentation extraction",
      "url"      : "/en/aws-documentation/",
      "img"      : "PHBpY3R1cmU+CiAgPHNvdXJjZSBzcmNzZXQ9Ii9hc3NldHMvaW1nL2F3cy1kb2N1bWVudGF0aW9uLTMwMC05NzI4MmU1YTIuYXZpZiAxLjB4LCAvYXNzZXRzL2ltZy9hd3MtZG9jdW1lbnRhdGlvbi02MDAtOTcyODJlNWEyLmF2aWYgMi4weCIgdHlwZT0iaW1hZ2UvYXZpZiI+CiAgPHNvdXJjZSBzcmNzZXQ9Ii9hc3NldHMvaW1nL2F3cy1kb2N1bWVudGF0aW9uLTMwMC02NjI2NGM3NjUuanBnIDEuMHgsIC9hc3NldHMvaW1nL2F3cy1kb2N1bWVudGF0aW9uLTYwMC02NjI2NGM3NjUuanBnIDIuMHgiIHR5cGU9ImltYWdlL2pwZWciPgogIDxpbWcgc3JjPSIvYXNzZXRzL2ltZy9hd3MtZG9jdW1lbnRhdGlvbi0zMDAtNjYyNjRjNzY1LmpwZyIgYWx0PSJBV1MgZG9jdW1lbnRhdGlvbiBleHRyYWN0aW9uIj4KPC9waWN0dXJlPgo=",
      "isencoded" : "y",
      "tags"     : "AWS, Documentation, JSON, Scraper",
      "content"  : "Source codeAll sources are kept at https://github.com/scalastic/aws-documentation-scraper"
    } ,
  
    {
      "title"    : "JVM vs Native - Configuring Java containers in Kubernetes",
      "url"      : "/en/java-container-configuration/",
      "img"      : "PHBpY3R1cmU+CiAgPHNvdXJjZSBzcmNzZXQ9Ii9hc3NldHMvaW1nL2p2bS12cy1uYXRpdmUtcGFydC0yLTMwMC0yMGFhZDk0OWQuYXZpZiAxLjB4LCAvYXNzZXRzL2ltZy9qdm0tdnMtbmF0aXZlLXBhcnQtMi02MDAtMjBhYWQ5NDlkLmF2aWYgMi4weCIgdHlwZT0iaW1hZ2UvYXZpZiI+CiAgPHNvdXJjZSBzcmNzZXQ9Ii9hc3NldHMvaW1nL2p2bS12cy1uYXRpdmUtcGFydC0yLTMwMC1iYjlhMmQ2ZWEuanBnIDEuMHgsIC9hc3NldHMvaW1nL2p2bS12cy1uYXRpdmUtcGFydC0yLTYwMC1iYjlhMmQ2ZWEuanBnIDIuMHgiIHR5cGU9ImltYWdlL2pwZWciPgogIDxpbWcgc3JjPSIvYXNzZXRzL2ltZy9qdm0tdnMtbmF0aXZlLXBhcnQtMi0zMDAtYmI5YTJkNmVhLmpwZyIgYWx0PSJKVk0gdnMgTmF0aXZlIC0gQ29uZmlndXJpbmcgSmF2YSBjb250YWluZXJzIGluIEt1YmVybmV0ZXMiPgo8L3BpY3R1cmU+Cg==",
      "isencoded" : "y",
      "tags"     : "JVM, Spring-Boot, Spring-Native, Spring-WebFlux, GraalVM, Docker, Kubernetes, Prometheus, Grafana, Microservices",
      "content"  : "In a previous article, JVM vs Native - An effective comparison of performance, I showed how to install a full Kubernetes stack to measure Java microservices metrics. The configuration being long and tedious (the article also probably), I did not dwell on the configuration of the containers. In this article, we will see why, in a Java application, this configuration is essential and how it impacts the resources consumed by an application. Source codeAll sources are kept at github.com/scalastic/hotspot-vs-native-part2 Context reminder Kubernetes: how a Pod works The different types of resources in Kubernetes Soft and hard limits Resource unit CPU limit Memory limit Limits Operation in Kubernetes Example of a resource configuration in pod Java in Kubernetes Demonstration Creating the Kubernetes environment Dashboard Grafana Launch of the demo application and of its Bytecode microServices What do we observe? Remove the application Launch of the demo application and its native microservices What do we find? Conclusion Context reminder Our goal was to compare the execution of a Java application, between its Bytecode (JVM HotSpot) and native (compilation with GraalVM) versions. For this, we have set up a local Kubernetes cluster with Prometheus and Grafana to, respectively, collect and present the metrics. We have also equipped our Java microservices with Micrometer in order to expose the metrics of our applications to Prometheus. We obtained the following results in Grafana: Visualization of the roll-out between a JVM image and a Native image in Grafana And we find out about: Latency No change in responsiveness of microservices. CPU usage In its Bytecode version, CPU usage tends to decrease over time. This is due to the action of the HotSpot C2 compiler which produces native code that is increasingly optimized over time. On the other hand, the CPU usage in its native version is low from the start. RAM usage Surprisingly, native applications use more memory than Bytecode! Indeed, we had not made any particular configuration to our containers. So it’s time to rectify that now. Kubernetes: how a Pod works AttentionBy default, when you create a pod, it uses all the system resources of the host machine. It is said ! In order to guard against this, it is necessary to assign resource limits: Either at the pod level, Or at the namespace level, which will impact, by default, the pods it contains. Actually, under the hood, these are the cgroups of the Linux kernel that Docker and all Container Runtime Interfaces take into account when allocating resources. The different types of resources in Kubernetes Currently, they are of 3 kinds: CPU Memory Hugepages (since Kubernetes v1.14) The CPU and Memory type resources are called computing resources. Hugepages are virtual memory optimization mechanisms that reserve a large amount of memory rather than multiple chunks which increases system performance. Soft and hard limits In the OS, the resource limits are of 2 kinds: Soft limit: quantity of resource required Hard limit: maximum quantity authorized We also find these two limits in Kubernetes to manage pod resources: requests for the quantity needed limits for the maximum quantity Good to knowIf you only specify limits, Kubernetes will automatically assign the same value to requests. Resource unit The problem here is to specify a common CPU or memory unit while physical systems are heterogeneous. CPU limit It is expressed in terms of CPU core. Therefore it’s a vCPU / Core in a Cloud architecture and a hypertheaded core when it comes to bare-metal As a processor core can be shared by several pods, we also specify a fraction of the use of this core per pod. It can be expressed in core (e.g. 0.5 or half of a core) or in millicore (e.g. 250m or quarter of a core) We cannot go below 1m or 0.001 (implicitly in core unit) Memory limit It is expressed either in byte, or in its binary equivalent: 1024 bytes = 1000 bibytes It can be simplified with the suffixes K, M, G, T, P, E or in binary Ki, Mi, Gi, Ti, Pi, Ei Here is a summary table: Name Bytes Suffix Name Bibytes Suffix kilobyte 103 K kibibyte 210 Ki megabyte 106 M mebibyte 220 Mi gigabyte 109 G gibibyte 230 Gi terabyte 1012 T tebibyte 240 Ti petabyte 1015 P pebibyte 250 Pi exabyte 1018 E exbibyte 260 Ei Limits Operation in Kubernetes Kubernetes leaves to Container Runtime (e.g. Docker) the limits management: For the CPU, it calculates a quota in second that a pod is allowed to use every 100ms. When a pod goes throught its quota, Docker puts it on hold for 100ms and processes the following pods. If a Pod uses less than its quota, it processes the following pods as well. This CPU distribution method is called Completely Fair Scheduler. For memory, when limits is reached, the Container Runtime will delete the pod (which will then restart) with a Out Of Memory (OOM). Note also, that when a pod exceeds its requests, it becomes a candidate for eviction when the host lacks memory resources. It is therefore important not to underestimate the value of requests. Example of a resource configuration in pod Take the example of the Microservice hasher-java and set up its deployment. requests, the quantity of necessary resources, is configured in Kubertes with spec.containers[].Resources.requests. limits, the maximum allowable amount, is configured with spec.containers[].Resources.Limits. For the Microservice hasher-java, you now have this: apiVersion: apps/v1 kind: Deployment metadata: name: hasher namespace: demo labels: app: hasher spec: replicas: 1 selector: matchLabels: app: hasher strategy: type: RollingUpdate rollingUpdate: maxUnavailable: 1 maxSurge: 1 template: metadata: name: hasher labels: app: hasher spec: containers: - image: hasher-jvm:1.0.0 imagePullPolicy: IfNotPresent name: hasher resources: requests: memory: &quot;50Mi&quot; cpu: &quot;50m&quot; limits: memory: &quot;256Mi&quot; cpu: &quot;200m&quot; ports: - containerPort: 8080 name: http-hasher protocol: TCP readinessProbe: failureThreshold: 3 httpGet: path: /actuator/health port: 8080 scheme: HTTP initialDelaySeconds: 10 periodSeconds: 30 successThreshold: 1 timeoutSeconds: 2 Yaml Okay, so we are good now? Not sure, there are still elements to check like Java … Let’s see what it is. Java in Kubernetes The JVM questions the host OS to configure the number of threads for garbage collector and the memory to use. But in a containerized environment, OS information does not reflect container one. This problem has been addressed since 2017 and is managed properly since Java 10 b34 version as well as later versions. The correction was also reflected on JDKs 8 from the Java 8u191 version. It results in the addition of a parameter -xx:+UseContainerSupports which is activated by default and which allows extraction of good information from the containers. Other parameters appear along Java versions to refine operation in the containers: -XX:ActiveProcessorCount, -XX:PreferContainerQuotaForCPUCount, -XX:MaxRAMPercentage. But if you use versions of the JDK integrating UseContainerSupports, everything should be fine. Demonstration Let’s see what this new configuration brings to our microservices. Creating the Kubernetes environment Let’s start from a Kube environment that contains all the necessary components for our demo: A K8S cluster (local) Metrics Server Prometheus Grafana To do this, run the following commands in the root project: kubectl apply -f ./k8s/ Zsh It can take a few minutes before all components are up and running. Grafana is the one which is the most relevant to us. Dashboard Grafana Connect and login to the Grafana interface: http://localhost:3000/ The default login / password is admin / admin. Import the dashboard whose definition is placed under ./grafana/dashboard.json file from the root project. To do so, go to the Dashboards / Manage menu and click on the Import button. Then click on Upload JSON file and select the ./Grafana/dashboard.json file. In the prometheus field, select the datasource that has been created and which is called prometheus. Click Import. You should see the Dashboard for our demo: Empty Grafana dashboard Launch of the demo application and of its Bytecode microServices We will start the Bytecode-compiled application with 10 workers, 5 hashers and 5 rngs: kubectl apply -f ./app/demo-jvm.yaml Zsh Let’s wait before the application goes up and the docker images stabilizes. After a few minutes, you should see: Visualization of the demo application at startup with bytecode microservices What do we observe? For the CPU A peak at 700m when deploying Java microservices: the running C1 / C2 compilers. Next, there’s a gradual decrease of CPU consumption from 200m to 100m: the result of native code optimization produced by the C2 compiler. For RAM It quickly rises to 750MB and becomes stable. Remove the application Delete the application by launching the following command: kubectl delete -f ./app/demo-jvm.yaml Zsh Now, let’s see what happens in native compiled code during deployment. Launch of the demo application and its native microservices Launch the native version of the application: kubectl apply -f ./app/demo-native.yaml Zsh Let it run a few minutes to observe its behavior over time: Visualization of the demo application at startup with Bytecode microservices What do we find? For the CPU No peak of consumption at startup but instead a reduced and regular consumption by up to 35m: indeed, native code has already been compiled and optimized. For RAM It increases slightly but remains below 200MB. Conclusion In a constrained environment, the native code of our Spring Boot application, produced by Graalvm, consumes 3x less CPU than the same application compiled in Bytecode. With regard to memory, there’s also a 4-fold decrease with native code Spring Boot application. It differs completely from what we observed in our tests, without CPU and memory constraints onto the pods. As such, we can see the advantage that provides good configuration of pods. Note also, for the same Kubernetes cluster (and therefore for the same cost), it will be possible to execute 3x more microservices with native-compiled Spring Boot application. Therefore the advent of Graalvm marks the beginning of a deep change in the Java ecosystem. Spring’s GraalVM migration will allow our Legacy applications to benefit fully from constrained environments like Cloud. And at the same time, by mastering costs. Another important point: these tests were made with a non-optimized version of Spring Native, version 0.10.0-SNAPSHOT. Indeed, next iteration 0.11.0 will bring Spring new resources consumption optimizations but there is no doubt that this is already very promising. Cheers…"
    } ,
  
    {
      "title"    : "This week on the web",
      "url"      : "/en/review-2021-05-16/",
      "img"      : "PHBpY3R1cmU+CiAgPHNvdXJjZSBzcmNzZXQ9Ii9hc3NldHMvaW1nL3ByZXNzLXJldmlldy0wMS0zMDAtODdhOTAxZTZlLmF2aWYgMS4weCwgL2Fzc2V0cy9pbWcvcHJlc3MtcmV2aWV3LTAxLTYwMC04N2E5MDFlNmUuYXZpZiAyLjB4IiB0eXBlPSJpbWFnZS9hdmlmIj4KICA8c291cmNlIHNyY3NldD0iL2Fzc2V0cy9pbWcvcHJlc3MtcmV2aWV3LTAxLTMwMC03M2E0YjhjODIuanBnIDEuMHgsIC9hc3NldHMvaW1nL3ByZXNzLXJldmlldy0wMS02MDAtNzNhNGI4YzgyLmpwZyAyLjB4IiB0eXBlPSJpbWFnZS9qcGVnIj4KICA8aW1nIHNyYz0iL2Fzc2V0cy9pbWcvcHJlc3MtcmV2aWV3LTAxLTMwMC03M2E0YjhjODIuanBnIiBhbHQ9IlRoaXMgd2VlayBvbiB0aGUgd2ViIj4KPC9waWN0dXJlPgo=",
      "isencoded" : "y",
      "tags"     : "SpaceX, Spring-WebFlux, Lossless",
      "content"  : "This week, Artificial Intelligence to land SpaceX’s Starship rocket, reactive programming with a tutorial on Spring WebFlux and an open source project in the spotlight, ruby-vips, widely used on this site to convert images in lossless formats. Happy reading everyone. 1. SpaceX Starship landing The Starship landing algorithm, based on a Reinforcement Learning method: And its source code executable directly in Google Collab: 2. Reactive programming with Spring WebFlux Reactive Programming (a programming philosophy https://www.reactivemanifesto.org) in Spring : 3. Ruby VIPS, the image processor: A Ruby extension of the libvips library which can convert images into WebP or Avif format inter alia:"
    } ,
  
    {
      "title"    : "JVM vs Native - An effective comparison of performances",
      "url"      : "/en/jvm-vs-native/",
      "img"      : "PHBpY3R1cmU+CiAgPHNvdXJjZSBzcmNzZXQ9Ii9hc3NldHMvaW1nL2p2bS12cy1uYXRpdmUtMzAwLWZhMDYzMTg5My5hdmlmIDEuMHgsIC9hc3NldHMvaW1nL2p2bS12cy1uYXRpdmUtNjAwLWZhMDYzMTg5My5hdmlmIDIuMHgiIHR5cGU9ImltYWdlL2F2aWYiPgogIDxzb3VyY2Ugc3Jjc2V0PSIvYXNzZXRzL2ltZy9qdm0tdnMtbmF0aXZlLTMwMC02ZWRkNjMyMjkuanBnIDEuMHgsIC9hc3NldHMvaW1nL2p2bS12cy1uYXRpdmUtNjAwLTZlZGQ2MzIyOS5qcGcgMi4weCIgdHlwZT0iaW1hZ2UvanBlZyI+CiAgPGltZyBzcmM9Ii9hc3NldHMvaW1nL2p2bS12cy1uYXRpdmUtMzAwLTZlZGQ2MzIyOS5qcGciIGFsdD0iSlZNIHZzIE5hdGl2ZSAtIEFuIGVmZmVjdGl2ZSBjb21wYXJpc29uIG9mIHBlcmZvcm1hbmNlcyI+CjwvcGljdHVyZT4K",
      "isencoded" : "y",
      "tags"     : "Spring-Boot, Spring-Native, Spring-WebFlux, GraalVM, Docker, Kubernetes, Prometheus, Grafana, Microservices",
      "content"  : "To compare the execution of a Java application between its Bytecode (JVM) and native (GraalVM) versions, you must first decide on its architecture and the framewoks to use. Secondly, we must also ask ourselves what we are going to measure. Recently, I came across a very interesting course, containers and orchestration, by Jérôme Petazzoni . It uses different Python and Ruby applications which interact by means of Docker containers. They act as a mesh of microservices. The efficiency of the system is measured by the number of treatments performed per second. It seemed like a good example to serve as a basis for this comparison by: Transposing the code into Java language under the Spring Boot / WebFlux frameworks and using Spring Native for the build in Bytecode or in native, Playing on the number of containers in order to vary the load of the system. Let’s see this in detail. Source codeAll sources are kept at https://github.com/scalastic/hotspot-vs-native UPDATEThe configuration of containers is essential when it comes to measuring memory and CPU consumption. An update of this article is available at JVM vs Native - Configuring Java Containers in Kubernetes Requirements Application Architecture Build the app Requirements for the build The easy way Summary of executed commands From Docker Hub Check Kubernetes configuration Kubernetes Stack Architecture Grafana configuration Data source configuration Dashboard configuration Description of the Demo Dashboard Starting the application Play with Kubernetes configuration Overview Increase pods’ number Impact on the application Further increase the number of pods Let’s deploy the native version of the application Clean all Conclusion What next? What is missing for a more realistic evaluation? Useful links Requirements To implement this solution, we will need: A Kubernetes cluster to run our containers, Different processing measurements from microservices Prometheus and Grafana to collect and display these measurements, A Java application compilable in Bytecode and native Well, that’s not much and it already exists: In a previous article, I explain how to install a complete Kubernetes, Prometheus and Grafana stack - Locally install Kubernetes, Prometheus, and Grafana, By integrating Micrometer with a Java Spring Boot application, it is possible to expose the measurements of its services - HasherHandler.java, For a Python application, the prometheus_client library also allows to expose measurements - worker.py, By configuring the Maven POM with the org.springframework.experimental:spring-native dependency, it is possible to compile the application in Bytecode as well as in native. Spring version These are the latest versions of Spring Experimental that will be used to develop our Java microservices. Indeed, they continuously correct and improve the bugs and the performance of the native build. But keep in mind that these are beta versions: Spring 2.5.0-RC1 Spring Native 0.10.0-SNAPSHOT Application Architecture Let’s see what the application is made of: Demo application architecture The application is made up of 4 microservices: worker: the algorithm orchestrator [Python] which gets 1 a random number, 2 hash it and 3 increment a counter in the redis database, rng: the random number generator [Java], hasher: the hash processor [Java], redis: the database which records a processing cycle counter. Build the app The purpose of the compilation is to produce one Docker image by microservice. For Java microservices, there will be two images, the first in Bytecode, the second in native. Optional I have put these images in a public registry on Docker Hub, so you can skip this build step. Requirements for the build However, if you want to create these Docker images, you will need to install: GraalVM 21.1.0 Java 11 based GraalVM Native Images Docker The easy way Note It should work on Linux and macOS based systems - and on Windows with some small modifications It will take time……. 10-20 min depending on your internet connection and processor! That’s the price to compile to native code. To do so, execute this script in the project root: ./build_docker_images.sh Bash Summary of executed commands For a non-java app: docker build -t &amp;lt;app_docker_tag&amp;gt; ./&amp;lt;app_dir&amp;gt; Bash For JVM-based image: cd &amp;lt;app_dir&amp;gt; mvn clean package docker build -t &amp;lt;app_docker_tag&amp;gt; . Bash For a Java native image: cd &amp;lt;app_dir&amp;gt; mvn spring-boot:build-image Bash From Docker Hub You can pull pre-build images from Docker Hub by entering: docker pull jeanjerome/rng-jvm:1.0.0 docker pull jeanjerome/hasher-jvm:1.0.0 docker pull jeanjerome/worker-python:1.0.0 docker pull jeanjerome/rng-native:1.0.0 docker pull jeanjerome/hasher-native:1.0.0 Bash Check To list your local docker images, enter: docker images Bash At least, you should see these images in your local registry: REPOSITORY TAG IMAGE ID CREATED SIZE rng-jvm 1.0.0 f4bfdacdd2a1 4 minutes ago 242MB hasher-jvm 1.0.0 ab3600420eab 11 minutes ago 242MB worker-python 1.0.0 e2e76d5f8ad4 38 hours ago 55MB hasher-native 1.0.0 629bf3cb8760 41 years ago 82.2MB rng-native 1.0.0 68e484d391f3 41 years ago 82.2MB Bash Note Created time of native images seems inaccurate. It’s not, the explanation is here: Time Travel with Pack Kubernetes configuration First, we need to define the kubernetes configuration of our application and tell Prometheus where to find the metrics. Kubernetes Stack Architecture Let’s see how to install these microservices in our kubernetes cluster: The architecture of the application is deployed in a dedicated namespace, demo, The monitoring tools are in another namespace called monitoring. Our Kubernetes cluster architecture We want to manage the number of containers - pods in this case - for each microservice, We also want to be able to change the pod image (Bytecode or native) without needing to redeploy everything. =&amp;gt; Such Kubernetes resource already exists, Deployment We need our microservices to communicate with each other in the Kubernetes cluster. =&amp;gt; This is the job of the resource Service. The Redis database does not need to be accessible from outside, only from inside the cluster. =&amp;gt; This is already the case because, by default, Kubernetes Services are of type ClusterIP. We want application metrics to be collected by Prometheus. =&amp;gt; Here is how to configure it Take a look at the Hasher microservice configuration below: Kubernetes configuration of Hasher microservices apiVersion: apps/v1 kind: Deployment metadata: name: hasher namespace: demo labels: app: hasher spec: replicas: 1 selector: matchLabels: app: hasher template: metadata: name: hasher labels: app: hasher spec: containers: - image: hasher-jvm:1.0.0 imagePullPolicy: IfNotPresent name: hasher ports: - containerPort: 8080 name: http-hasher protocol: TCP readinessProbe: failureThreshold: 3 httpGet: path: /actuator/health port: 8080 scheme: HTTP initialDelaySeconds: 10 periodSeconds: 30 successThreshold: 1 timeoutSeconds: 2 --- apiVersion: v1 kind: Service metadata: name: hasher namespace: demo labels: app: hasher annotations: prometheus.io/scrape: &#39;true&#39; prometheus.io/scheme: http prometheus.io/path: /actuator/prometheus prometheus.io/port: &#39;8080&#39; spec: ports: - port: 8080 protocol: TCP targetPort: http-hasher selector: app: hasher Yaml Grafana configuration To display the metrics collected by Prometheus, Grafana needs: A data source to Prometheus, A dashboard describing the metrics to display and in what form. If you followed my previous article Locally install Kubernetes, Prometheus et Grafana, the data source is already configured and you can skip the next step. The Grafana interface is then accessible to http://localhost:3000/ Data source configuration Grafana uses YAML files to configure a data source. It can be defined using the Kubernetes ConfigMap resource: apiVersion: v1 kind: Namespace metadata: name: monitoring --- apiVersion: v1 kind: ConfigMap metadata: name: grafana-datasources namespace: monitoring data: prometheus.yaml: |- { &quot;apiVersion&quot;: 1, &quot;datasources&quot;: [ { &quot;access&quot;:&quot;proxy&quot;, &quot;editable&quot;: true, &quot;name&quot;: &quot;prometheus&quot;, &quot;orgId&quot;: 1, &quot;type&quot;: &quot;prometheus&quot;, &quot;url&quot;: &quot;http://prometheus-service.monitoring.svc:8080&quot;, &quot;version&quot;: 1 } ] } Yaml It remains to pass this resource to Grafana in the definition of its Deployment: apiVersion: apps/v1 kind: Deployment metadata: name: grafana namespace: monitoring spec: replicas: 1 template: spec: containers: - image: grafana/grafana:latest name: grafana .../... volumeMounts: - mountPath: /etc/grafana/provisioning/datasources name: grafana-datasources readOnly: false volumes: - name: grafana emptyDir: {} - name: grafana-datasources configMap: defaultMode: 420 name: grafana-datasources Yaml Dashboard configuration Connect to the Grafana web interface, Import the pre-defined dashboard demo-dashboard.json, Display the dashboard. You should see an empty dashboard as follows: The demo dashboard in Grafana Description of the Demo Dashboard Description of the demo dashboard in Grafana The rows of the table (labeled from A to C) represent the 3 microservices, respectively, Worker, Random Number Generator -RNG- and Hasher. The columns (numbered 1 to 4) represent different metrics: In column 1, we can see the number of pods running as well as the speed of the processes In column 2 is displayed the history of processing speeds, for each microservice, In column 3 the CPU consumption of each pod is displayed, In column 4, the RAM consumption of each pod. Starting the application A Kubernetes configuration was created with Replicas of 1 pod for each microservice and Java images compiled in Bytecode. To start the application in Kubernetes, enter: kubectl apply -f _kube/k8s-app-jvm.yml Bash You should see the output: namespace/demo created deployment.apps/hasher created service/hasher created deployment.apps/rng created service/rng created deployment.apps/redis created service/redis created deployment.apps/worker created service/worker created Bash Visualize the start of the pods in Grafana: Starting of the application in Grafana Result The observed processing speed, located in cell A1, gives us a basic measure of the efficiency for our application: 3.20 cycles / s. Depending on the resources allocated to your space, you may get a different result. Play with Kubernetes configuration Overview Let’s see the actual deployment’s situation by entering: kubectl get deployment -n demo Bash Which should return: NAME READY UP-TO-DATE AVAILABLE AGE hasher 1/1 1 1 13m redis 1/1 1 1 13m rng 1/1 1 1 13m worker 1/1 1 1 13m Bash Increase pods’ number To scale up worker pods to 2, enter: kubectl scale deployment worker --replicas=2 -n demo Bash Which returns: deployment.apps/worker scaled Bash Impact on the application Let’s take a look at the Grafana dashboard: 2-workers&#39; visualization in Grafana Results You will notice that the speed of the application is multiplied by x2. Further increase the number of pods Let’s go to 10 workers: kubectl scale deployment worker --replicas=10 -n demo Bash 10-workers&#39; visualization in Grafana Results The speed of the process increases, but does not reach exactly 10x more: the latency of the 2 microservices, rng and hasher, which increased slightly, explains this. Let’s increase hasher and rng pods’ number: kubectl scale deployment hasher rng --replicas=5 -n demo Bash RND and Hasher microservices visualization in Grafana Results The increase in the number of hasher and rng pods has reduced their latency, but it is still a little higher than at the beginning, Another factor is limiting but we do not see which one in the displayed data. Let’s deploy the native version of the application Replace the current image of the pods with their native version by updating their Deployment: kubectl set image deployment/hasher hasher=hasher-native:1.0.0 -n demo kubectl set image deployment/rng rng=rng-native:1.0.0 -n demo Bash Watch the rollout: kubectl rollout status deployment/hasher -n demo Bash And open the Grafana dashboard: Native images deployment visualization in Grafana Results Latency No change for the responsiveness of the microservices: undoubtedly, the code is too simple to benefit from a native build. CPU usage With Bytecode, CPU usage tended to decrease over time. This was due to the action of the HotSpot C2 compiler which produces native code that is increasingly optimized over time. In contrast, native processor usage is low from the start. RAM usage Surprisingly, native applications use more memory than Bytecode ones: it is all the more surprising that reducing the memory footprint is one of the advantages cited by the community. Is it because of the Beta versions used in this demo or a memory leak in the implementation? UPDATEThe configuration of containers is essential when it comes to measuring memory and CPU consumption. An update of this article is available at JVM vs Native - Configuring Java Containers in Kubernetes Clean all To simply remove the app and all its microservices, enter: kubectl delete -f _kube/k8s-app-jvm.yml Bash which will remove all the Kubernetes configuration created previously: namespace &quot;demo&quot; deleted deployment.apps &quot;hasher&quot; deleted service &quot;hasher&quot; deleted deployment.apps &quot;rng&quot; deleted service &quot;rng&quot; deleted deployment.apps &quot;redis&quot; deleted service &quot;redis&quot; deleted deployment.apps &quot;worker&quot; deleted service &quot;worker&quot; deleted Bash Conclusion We learned how to install a full Kubernetes stack so that we can measure application metrics. However, we are not getting the results we expect in the context of native applications. One explanation could be a lack of the Spring Beta version: Spring Native has just upgraded to version 0.10.0-SNAPSHOT and this is precisely the version where performance improvements are planned. I’ll open a ticket with the Spring Boot team to ask for their analysis. UPDATEThe configuration of containers is essential when it comes to measuring memory and CPU consumption. An update of this article is available at JVM vs Native - Configuring Java Containers in Kubernetes What next? What is missing for a more realistic evaluation? Kubernetes configuration should always include a resource limit which has not been done in this demo. I could have used Horizontal Pod Autoscaler (HPA) and even better HPA on custom metrics (read this post for more details). Question I wish I found something on self-regulating Scalers, able to maximize a metric but nothing about such a thing… Have you ever heard of something like that? Useful links Here are some links for further reading: Jérôme Patazzoni’s container training: https://github.com/jpetazzo/container.training Kubernetes Concepts : https://kubernetes.io/docs/concepts/ Monitoring your apps in Kubernetes with Prometheus and Spring Boot: https://developer.ibm.com/technologies/containers/tutorials/monitoring-kubernetes-prometheus/ Prometheus Python Client: https://github.com/prometheus/client_python Custom Prometheus Metrics for Apps Running in Kubernetes: https://zhimin-wen.medium.com/custom-prometheus-metrics-for-apps-running-in-kubernetes-498d69ada7aa Well, that’s your turn playing with native apps now! Cheers…"
    } ,
  
    {
      "title"    : "Locally install Kubernetes, Prometheus, and Grafana",
      "url"      : "/en/install-kubernetes-prometheus-grafana/",
      "img"      : "PHBpY3R1cmU+CiAgPHNvdXJjZSBzcmNzZXQ9Ii9hc3NldHMvaW1nL2luc3RhbGwta3ViZXJuZXRlcy0zMDAtYTNkZmMwZmYyLmF2aWYgMS4weCwgL2Fzc2V0cy9pbWcvaW5zdGFsbC1rdWJlcm5ldGVzLTYwMC1hM2RmYzBmZjIuYXZpZiAyLjB4IiB0eXBlPSJpbWFnZS9hdmlmIj4KICA8c291cmNlIHNyY3NldD0iL2Fzc2V0cy9pbWcvaW5zdGFsbC1rdWJlcm5ldGVzLTMwMC1iNDZiODMyNTQuanBnIDEuMHgsIC9hc3NldHMvaW1nL2luc3RhbGwta3ViZXJuZXRlcy02MDAtYjQ2YjgzMjU0LmpwZyAyLjB4IiB0eXBlPSJpbWFnZS9qcGVnIj4KICA8aW1nIHNyYz0iL2Fzc2V0cy9pbWcvaW5zdGFsbC1rdWJlcm5ldGVzLTMwMC1iNDZiODMyNTQuanBnIiBhbHQ9IkxvY2FsbHkgaW5zdGFsbCBLdWJlcm5ldGVzLCBQcm9tZXRoZXVzLCBhbmQgR3JhZmFuYSI+CjwvcGljdHVyZT4K",
      "isencoded" : "y",
      "tags"     : "Kubernetes, Docker, Prometheus, Grafana, macOS",
      "content"  : "Sometimes it may be useful for developers to have an effective monitoring toolset installed beside your Kubernetes cluster. The obvious choice would be to go to Prometheus and Grafana, two tools highly recognized. And that’s when you get into trouble. In this article I’m going to show you how to succesfully install a complete Kubernetes platform on your local machine particulary on macOS. Source codeAll scripts are kept in https://github.com/scalastic/local-k8s-installation Notice Kubernetes Dashboard Metrics Server Kube State Metrics Prometheus Grafana Import Grafana dashboard Notice This procedure is intended to work on macOS but should work on any other host. However I was not able to test it. Things are more tying up with the Kubernetes distribution (Docker Desktop) than the OS itself. Kubernetes We’ll be using Docker Desktop which comes with a ready-to-use Kubernetes cluster. Download the installer at https://www.docker.com/products/docker-desktop and follow the instructions. Once installed, activate Kubernetes in the Preferences pane / Kubernetes / Enable Kubernetes. You should now see all the Docker Desktop components in the About pane included Kubernetes: Docker Desktop interface with components version Grapple basic information from your cluster to verify everything’s working fine: % kubectl cluster-info Zsh % kubectl get nodes Zsh Or even: % kubectl get all -A Zsh -A parameter Since Kube 1.14, we don’t have to use the --all-namespaces anymore! So don’t… Dashboard Dashboard is a web UI of k8s API and provides an easy way to visualize and debug things. You can find more about Dashboard at https://github.com/kubernetes/dashboard By default, Dashboard is protected by a token, and each time you’ll access it, you’ll be asked to provide one. It could really be annoying in the long run. Fortunately, Dashboard allows you to bypass the login page by adding --enable-skip-login to the configuration. NoteThe provided configuration file is patched with this instruction (line 198) To deploy Dashboard - with no authentication - execute the following command: % kubectl apply -f ./k8s/dashboard-v2.2.0-recommended.yaml Zsh Then access your dashboard from your local workstation by creating a secure channel to your Kubernetes cluster. To do so, run the following command: % kubectl proxy Zsh The address is now: http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/ TipsI know so don’t forget to bookmark the URL! You’ll see this login page at first: Dashboard login screen Just push the skip button to bypass authentication. Dashboard main page Metrics Server Metrics-Server is used to crop metrics from k8s components and exposes them to k8s API. The official repository is https://github.com/kubernetes-sigs/metrics-server Metrics Server serves the goals of core metrics pipelines: CPU and RAM. The important thing is that Horizontal Pod Autoscaler uses this API to collect metrics. It’s a cluster level component which periodically scrapes metrics from all Kubernetes nodes served by Kubelet. When installed, Dashboard displays automatically this metrics too. NoteTo allow Metrics Server to collect its data over https, the original script has been modified to accept insecure TLS connections by adding - --kubelet-insecure-tls at line 133. Apply the configuration by entering : % kubectl apply -f k8s/metrics-server-components-v0.4.2.yaml Zsh When reloading the Dashboard, you should now see CPU and Memory Usages (after some time) 🌈 You can try the Pods section, this is my favorite! Dashboard page with metrics. Beautiful! Kube State Metrics Unlike Metrics Server, Kube State Metrics is focused on generating numerous metrics from Kubernetes’ object state (e.g. metrics based on deployments, replica sets, pods, etc.). For this, it holds an entire snapshot of Kubernetes state in memory and generates new metrics based off of it. Having Kube State Metrics enables access to these metrics from monitoring systems such as Prometheus, our concerne here. To install Kube State Metrics, launch the following: % kubectl apply -f k8s/kube-state-metrics-v2.0.0-rc.1.yaml Zsh Prometheus Prometheus is a collecting, querying, monitoring, and alerting system. It is useful when it comes to: Collects data identified by a metric name Stores time-series in memory and on local disk for efficiency Allows notifications and alerting depending on data queries Full documentation is accessible from https://prometheus.io. Prometheus’ maintainers provide binaries and Docker images for Prometheus components. With a bit of elbow grease, it is possible to create a k8s configuration file with everything we need: Resources access, dedicated role, configuration, deployment, and service exposition. To install the Prometheus configuration, run the command: % kubectl apply -f k8s/prometheus.yaml Zsh You can now access Prometheus interface at http://localhost:30000/ … but wait and see!! Grafana Grafana (https://grafana.com/grafana/) allows you to « query, visualize and alert on metrics through a powerful user interface » as the site puts. That&#39;s what Prometheus is already doing!?Let’s clarify : in practice, you will develop your query in Prometheus, and run on Grafana when you’re satisfied. To configure the Prometheus datasource and install Grafana, run the commands: % kubectl apply -f k8s/grafana-datasource.yaml % kubectl apply -f k8s/grafana.yaml Zsh Grafana will be listening on http://localhost:3000/. The default login is admin / admin. Import Grafana dashboard By default, Grafana comes with nothing specific and you’ll have to configure a dashboard. Fortunatly, you can easilly import pre-built ones via the Grafana interface. The principal dashboards’ source is at https://grafana.com/grafana/dashboards. I’ve made one specific to local Kubernetes cluster you can find here Dashboard model. It looks like this - as you can see, the Grafana graphical interface offers numerous possibilities - unlike Prometheus: Grafana interface after complete installation. Woah!! 👨🏻‍💻 Configuring a dashboard in Grafana could be long, costly, and time-consuming. Collecting metrics depends on your host, VM/Virtualization Framework and OS on your pod. You will have to get your hands dirty… Well, that’s your turn playing with the stack now! Cheers…"
    } ,
  
    {
      "title"    : "A Dive into Docker images",
      "url"      : "/en/dive-image-docker/",
      "img"      : "PHBpY3R1cmU+CiAgPHNvdXJjZSBzcmNzZXQ9Ii9hc3NldHMvaW1nL2RpdmUtaW1hZ2UtZG9ja2VyLTMwMC1kNGM4YTQ1ZGMuYXZpZiAxLjB4LCAvYXNzZXRzL2ltZy9kaXZlLWltYWdlLWRvY2tlci02MDAtZDRjOGE0NWRjLmF2aWYgMi4weCIgdHlwZT0iaW1hZ2UvYXZpZiI+CiAgPHNvdXJjZSBzcmNzZXQ9Ii9hc3NldHMvaW1nL2RpdmUtaW1hZ2UtZG9ja2VyLTMwMC02ZGY0ZmZmM2QuanBnIDEuMHgsIC9hc3NldHMvaW1nL2RpdmUtaW1hZ2UtZG9ja2VyLTYwMC02ZGY0ZmZmM2QuanBnIDIuMHgiIHR5cGU9ImltYWdlL2pwZWciPgogIDxpbWcgc3JjPSIvYXNzZXRzL2ltZy9kaXZlLWltYWdlLWRvY2tlci0zMDAtNmRmNGZmZjNkLmpwZyIgYWx0PSJBIERpdmUgaW50byBEb2NrZXIgaW1hZ2VzIj4KPC9waWN0dXJlPgo=",
      "isencoded" : "y",
      "tags"     : "Docker, Continuous-Integration, Tool",
      "content"  : "Building Docker images has become commonplace in the IT world. However and despite the use of reference images, our Docker images in their repository are like black boxes that nobody really knows what they really consist of. Worse yet, in an ongoing DevOps process, there are no metrics to measure the level of optimization of our container image. Of course, there are the commands docker inspect &amp;lt;image&amp;gt; or even docker history &amp;lt;image&amp;gt; but these are not very practical commands to handle. This is why the tool Dive will allow us to dive into the heart of our images Docker/OCI and allow us to finally simply know what they contain. Features Demonstration Conclusion Features Let’s review the features of the tool and see what it can do for us: Display of the content of the Docker image, for each layer of the image, it is possible to navigate in its filesystem Indication of modifications at the level of each layer (addition, modification, deletion) Estimation of a image efficiency score Integration into a CI possible to calculate the image efficiency and define a blocking limit Demonstration Let’s see a concrete case and analyze an image generated by Builpacks and Spring Native: What does the Dive tool teach us: Our image is made of 6 layers of which we can see the changes they operate in the filesystem Of the 82 MB of the image, 481 bytes can be recovered, which means not much, hence the image efficiency score of 99% The visual aspect of the tool makes the actions of the different layers more understandable: addition of a cnb user, addition of the executable in the worskpace directory, … Conclusion This simple tool should allow any developer to understand the impact of each line of their Dockerfile. In addition, as image size is increasingly critical in the context of deployments, it is also a way to impose a limit or at least a level of quality. And you what do you think ? Cheers …"
    } ,
  
    {
      "title"    : "Compile a Spring application natively with GraalVM",
      "url"      : "/en/spring-natif-graalvm/",
      "img"      : "PHBpY3R1cmU+CiAgPHNvdXJjZSBzcmNzZXQ9Ii9hc3NldHMvaW1nL3NwcmluZy1uYXRpdmUtZ3JhYWx2bS1wb3N0LTMwMC00ODRjNmU4MDUuYXZpZiAxLjB4LCAvYXNzZXRzL2ltZy9zcHJpbmctbmF0aXZlLWdyYWFsdm0tcG9zdC02MDAtNDg0YzZlODA1LmF2aWYgMi4weCIgdHlwZT0iaW1hZ2UvYXZpZiI+CiAgPHNvdXJjZSBzcmNzZXQ9Ii9hc3NldHMvaW1nL3NwcmluZy1uYXRpdmUtZ3JhYWx2bS1wb3N0LTMwMC01YTFiY2FhN2UuanBnIDEuMHgsIC9hc3NldHMvaW1nL3NwcmluZy1uYXRpdmUtZ3JhYWx2bS1wb3N0LTYwMC01YTFiY2FhN2UuanBnIDIuMHgiIHR5cGU9ImltYWdlL2pwZWciPgogIDxpbWcgc3JjPSIvYXNzZXRzL2ltZy9zcHJpbmctbmF0aXZlLWdyYWFsdm0tcG9zdC0zMDAtNWExYmNhYTdlLmpwZyIgYWx0PSJDb21waWxlIGEgU3ByaW5nIGFwcGxpY2F0aW9uIG5hdGl2ZWx5IHdpdGggR3JhYWxWTSI+CjwvcGljdHVyZT4K",
      "isencoded" : "y",
      "tags"     : "Spring-Native, GraalVM, Spring, Docker, Tutorial",
      "content"  : "With the release this week of Spring Native Beta in version 0.9.0, it is interesting to take stock of the compilation of Spring applications into native executables using GraalVM and its native mode. -image. The execution of an application in native code has, indeed, many advantages compared to that in Bytecode in a JVM: The start is instantaneous The performance is optimal from the start Memory consumption is greatly reduced The version of Spring Native is, however, in Beta which means that not all Spring components are still functional in native mode. Let’s see how it works in detail. Basic system requirements Generation of the application skeleton Spring Native Modules Remarks Adding a Web Controller Compilation in native code Using the Spring Boot Buildpack Prerequisites Compiling in native mode with Buildpacks Running the application Conclusion Basic system requirements First, you will need to install GraalVM and then its native code compiler native-image : For MacOS and Linux, it is recommended to install its JDKs with SDKMAN. Nothing complicated: refer to doc. official or find a previous article on this tool Install Java with SDKMAN For our friends on Windows, refer directly to the Installation on Windows Platforms Generation of the application skeleton The arrival of the Beta version implies that Spring Native is now supported by Spring Initializr, a web interface that allows you to compose your Spring application and then generate its skeleton. Let’s use it to define our demo app: Fill in the project metadata Select the Spring Native [Experimental] dependency to benefit from native compilation Add the Spring Web dependency as part of this demo Download the generated code by clicking on the Generate button Spring Initializr interface for demo application Spring Native Modules You will find, in the POM, the list of Spring modules configured as Maven dependencies: The Spring Native dependency and its version: &amp;lt;properties&amp;gt; &amp;lt;java.version&amp;gt;11&amp;lt;/java.version&amp;gt; &amp;lt;spring-native.version&amp;gt;0.9.1-SNAPSHOT&amp;lt;/spring-native.version&amp;gt; &amp;lt;/properties&amp;gt; .../... &amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.springframework.experimental&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;spring-native&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;${spring-native.version}&amp;lt;/version&amp;gt; &amp;lt;/dependency&amp;gt; Maven The Spring Boot Maven plugin and its configuration to run the build of a native image in a Buildpacks : &amp;lt;plugin&amp;gt; &amp;lt;groupId&amp;gt;org.springframework.boot&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;spring-boot-maven-plugin&amp;lt;/artifactId&amp;gt; &amp;lt;configuration&amp;gt; &amp;lt;image&amp;gt; &amp;lt;builder&amp;gt;paketobuildpacks/builder:tiny&amp;lt;/builder&amp;gt; &amp;lt;env&amp;gt; &amp;lt;BP_NATIVE_IMAGE&amp;gt;true&amp;lt;/BP_NATIVE_IMAGE&amp;gt; &amp;lt;/env&amp;gt; &amp;lt;/image&amp;gt; &amp;lt;/configuration&amp;gt; &amp;lt;/plugin&amp;gt; Maven The AOT Maven plugin which is used to configure Spring for its Ahead-Of-Time compilation as well as to generate code for the configuration and the classpath of the application: &amp;lt;plugin&amp;gt; &amp;lt;groupId&amp;gt;org.springframework.experimental&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;spring-aot-maven-plugin&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;${spring-native.version}&amp;lt;/version&amp;gt; &amp;lt;executions&amp;gt; &amp;lt;execution&amp;gt; &amp;lt;id&amp;gt;test-generate&amp;lt;/id&amp;gt; &amp;lt;goals&amp;gt; &amp;lt;goal&amp;gt;test-generate&amp;lt;/goal&amp;gt; &amp;lt;/goals&amp;gt; &amp;lt;/execution&amp;gt; &amp;lt;execution&amp;gt; &amp;lt;id&amp;gt;generate&amp;lt;/id&amp;gt; &amp;lt;goals&amp;gt; &amp;lt;goal&amp;gt;generate&amp;lt;/goal&amp;gt; &amp;lt;/goals&amp;gt; &amp;lt;/execution&amp;gt; &amp;lt;/executions&amp;gt; &amp;lt;/plugin&amp;gt; Maven Remarks Dependencies not supportedIn case you select a Spring dependency not yet supported in native mode, the HELP.md file will contain a warning: Warning in HELP.md file Supported dependencies In the case of dependencies supported by Spring, the initializr will configure all the plugins necessary for the build and execution of the Spring application to work out-of-the-box! In the example of Spring Data JPA, Maven will be configured so that the Hibernate classes are compiled when the application is built and not during its runtime as is the case for a JVM: &amp;lt;plugin&amp;gt; &amp;lt;groupId&amp;gt;org.hibernate.orm.tooling&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;hibernate-enhance-maven-plugin&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;${hibernate.version}&amp;lt;/version&amp;gt; &amp;lt;executions&amp;gt; &amp;lt;execution&amp;gt; &amp;lt;id&amp;gt;enhance&amp;lt;/id&amp;gt; &amp;lt;goals&amp;gt; &amp;lt;goal&amp;gt;enhance&amp;lt;/goal&amp;gt; &amp;lt;/goals&amp;gt; &amp;lt;configuration&amp;gt; &amp;lt;failOnError&amp;gt;true&amp;lt;/failOnError&amp;gt; &amp;lt;enableLazyInitialization&amp;gt;true&amp;lt;/enableLazyInitialization&amp;gt; &amp;lt;enableDirtyTracking&amp;gt;true&amp;lt;/enableDirtyTracking&amp;gt; &amp;lt;enableAssociationManagement&amp;gt;true&amp;lt;/enableAssociationManagement&amp;gt; &amp;lt;enableExtendedEnhancement&amp;gt;false&amp;lt;/enableExtendedEnhancement&amp;gt; &amp;lt;/configuration&amp;gt; &amp;lt;/execution&amp;gt; &amp;lt;/executions&amp;gt; &amp;lt;/plugin&amp;gt; Maven All of this is very reassuring! I had previously tested version 0.7.1 of Spring Native (named spring-graalvm-native at the time) and there were a lot of manual changes to be made. Posted goal of the team in charge of Spring Native Provide configuration automatically so that there is no need to modify the Java code, whether the application is running in native mode or in a JVM. Make the unit tests run in the same way in a native image or in a JVM. Further reduce the size of the native image generated in the next version 0.10 of Spring Native. Adding a Web Controller Unzip the file generated by Spring Initializr and open the directory with your preferred IDE. Create a new Controller at the root of your project’s package with the code below: package io.scalastic.demo.demo_spring_native; import org.springframework.web.bind.annotation.GetMapping; import org.springframework.web.bind.annotation.RestController; @RestController public class DemoSpringNativeController { @GetMapping(&quot;/&quot;) public String hello() { return &quot;Hello!&quot;; } } Java The project and its Controller in IntelliJ IDEA Compilation in native code There are two ways to compile a Spring application to native code: By using the Buildpack Spring Boot integrated in Spring and which will produce a container lightweight containing the native code of the application Using the Maven plugin native-image-maven-plugin which will produce a native executable NoteThe Maven configuration generated by Spring Initializr chooses Buildpacks: We will therefore only discuss this aspect in this article. We will see the native build using the Maven native-image plugin which requires significant POM modifications, in a future article. Using the Spring Boot Buildpack This procedure obtains a Docker container that contains the application compiled in native code. It is lightweight and can be deployed directly into a container orchestrator. Prerequisites Docker must be installed in order to be able to launch the Buildpack Spring Boot. It’s a container that contains everything you need to build a Spring application in native code. You can install Docker from Docker Installation For MacOS, it is recommended to allocate at least 8GB of memory to Docker For Windows, you must activate Docker WSL 2 Backend to have better their performances Compiling in native mode with Buildpacks The native application can be compiled by running the following command: % mvn spring-boot:build-image [INFO] Scanning for projects... [INFO] [INFO] ----------------&amp;lt; io.scalastic.demo:demo_spring_native &amp;gt;---------------- [INFO] Building demo_spring_native 0.0.1-SNAPSHOT [INFO] --------------------------------[ jar ]--------------------------------- [INFO] [INFO] &amp;gt;&amp;gt;&amp;gt; spring-boot-maven-plugin:2.4.4:build-image (default-cli) &amp;gt; package @ demo_spring_native &amp;gt;&amp;gt;&amp;gt; [INFO] [INFO] --- maven-resources-plugin:3.2.0:resources (default-resources) @ demo_spring_native --- [.../...] [INFO] Successfully built image &#39;docker.io/library/demo_spring_native:0.0.1-SNAPSHOT&#39; [INFO] [INFO] ------------------------------------------------------------------------ [INFO] BUILD SUCCESS [INFO] ------------------------------------------------------------------------ [INFO] Total time: 03:03 min [INFO] Finished at: 2021-03-21T20:57:29+01:00 [INFO] ------------------------------------------------------------------------ Process finished with exit code 0 Zsh This command will create, locally, a Linux container to compile the native application from the GraalVM native-image compiler. Let’s look at the images present, in the local Docker registry and which have just been implemented in this build: % docker images REPOSITORY TAG IMAGE ID CREATED SIZE paketobuildpacks/run tiny-cnb e85a0fe734d7 17 hours ago 17.3MB paketobuildpacks/builder tiny 1cbb20e3de7e 41 years ago 401MB demo_spring_native 0.0.1-SNAPSHOT a423116a12a8 41 years ago 81.9MB Zsh We can see that this process produces 3 Docker images: paketobuildpacks/run:tiny-cnb: The distroless bionic + glibc + openssl + CA certs based runner to run an application in native code. It is the basic container used to encapsulate an application in native code. paketobuildpacks/builder:tiny: The builder based on an ubuntu distroless stack: bionic + openssl + CA certs + compilers + shell utilities. It is a Buildpack used to compile most applications in Go and Java applications in native code with GraalVM. demo_spring_native:0.0.1-SNAPSHOT: The native code application encapsulated in a basic distroless runner. To go further The images from the Buildpack date from 1980, January 1, 1980 exactly! This is quite intended and the explanation can be found here: Time Travel with Pack The Distroless stacks are minimalist images, developed by Google and which improve security and container size by reducing the area of ​​attacks and the number of components they integrate. The notion of Runner and Builder in the Buildpacks . Running the application To start the application from the Buildpack, type the following command: % docker run -p 8080:8080 docker.io/library/demo_spring_native:0.0.1-SNAPSHOT 2021-03-21 19:32:54.188 INFO 1 --- [ main] o.s.nativex.NativeListener : This application is bootstrapped with code generated with Spring AOT . ____ _ __ _ _ /---- / ___&#39;_ __ _ _(_)_ __ __ _ -- -- -- -- ( ( )--___ | &#39;_ | &#39;_| | &#39;_ --/ _` | -- -- -- -- ----/ ___)| |_)| | | | | || (_| | ) ) ) ) &#39; |____| .__|_| |_|_| |_--__, | / / / / =========|_|==============|___/=/_/_/_/ :: Spring Boot :: (v2.4.4) 2021-03-21 19:32:54.190 INFO 1 --- [ main] i.s.d.d.DemoSpringNativeApplication : Starting DemoSpringNativeApplication using Java 11.0.10 on 91a2f0962a8e with PID 1 (/workspace/io.scalastic.demo.demo_spring_native.DemoSpringNativeApplication started by cnb in /workspace) 2021-03-21 19:32:54.190 INFO 1 --- [ main] i.s.d.d.DemoSpringNativeApplication : No active profile set, falling back to default profiles: default 2021-03-21 19:32:54.218 INFO 1 --- [ main] o.s.b.w.embedded.tomcat.TomcatWebServer : Tomcat initialized with port(s): 8080 (http) Mar 21, 2021 7:32:54 PM org.apache.coyote.AbstractProtocol init INFO: Initializing ProtocolHandler [&quot;http-nio-8080&quot;] Mar 21, 2021 7:32:54 PM org.apache.catalina.core.StandardService startInternal INFO: Starting service [Tomcat] Mar 21, 2021 7:32:54 PM org.apache.catalina.core.StandardEngine startInternal INFO: Starting Servlet engine: [Apache Tomcat/9.0.44] Mar 21, 2021 7:32:54 PM org.apache.catalina.core.ApplicationContext log INFO: Initializing Spring embedded WebApplicationContext 2021-03-21 19:32:54.220 INFO 1 --- [ main] w.s.c.ServletWebServerApplicationContext : Root WebApplicationContext: initialization completed in 29 ms 2021-03-21 19:32:54.231 INFO 1 --- [ main] o.s.s.concurrent.ThreadPoolTaskExecutor : Initializing ExecutorService &#39;applicationTaskExecutor&#39; Mar 21, 2021 7:32:54 PM org.apache.coyote.AbstractProtocol start INFO: Starting ProtocolHandler [&quot;http-nio-8080&quot;] 2021-03-21 19:32:54.240 INFO 1 --- [ main] o.s.b.w.embedded.tomcat.TomcatWebServer : Tomcat started on port(s): 8080 (http) with context path &#39;&#39; 2021-03-21 19:32:54.241 INFO 1 --- [ main] i.s.d.d.DemoSpringNativeApplication : Started DemoSpringNativeApplication in 0.057 seconds (JVM running for 0.06) Zsh Test its operation with: % curl http://127.0.0.1:8080 Hello! Zsh It works! Magnificent!! Buildpacks characteristics The compilation lasts 3 min (with Docker images and Maven artifacts locally) The application starts in 0.06 s The Docker image containing the Spring application and the OS, is 82 MB in size Conclusion The Spring Native 0.9.0 version allowed us to easily compile a Spring application in native mode. As expected, the benefits of native mode are instant startup and greatly reduced container size. Interesting points, this generates new uses: High Availability management can be done with a single instance, the start of a second being instantaneous. instant start also allows a web application to be serverless, without needing to be redeveloped. With Knative (a Kubernetes redesign that starts serverless containers), GraalVM Native is a very well suited solution. Spring Native will eventually be integrated into Spring Boot 3 and Spring Framework 6, the goal being to specify only in the Maven or Graddle build, the expected target (native or other). The remaining work consists of optimizing the size of the native code generated, taking into account more Spring APIs and improving the execution of the tests in the native image (JUnit 5, …) To be followed closely then! Previous Article GraalVM, the future of microservices applications in Java Cheers …"
    } ,
  
    {
      "title"    : "GraalVM, the future of microservices applications in Java",
      "url"      : "/en/graalvm-microservices-java/",
      "img"      : "PHBpY3R1cmU+CiAgPHNvdXJjZSBzcmNzZXQ9Ii9hc3NldHMvaW1nL2dyYWFsdm0tYnJhbmQtbmV3LWNvbnRhaW5lcnMtMzAwLWQwYjE0YzY4OS5hdmlmIDEuMHgsIC9hc3NldHMvaW1nL2dyYWFsdm0tYnJhbmQtbmV3LWNvbnRhaW5lcnMtNjAwLWQwYjE0YzY4OS5hdmlmIDIuMHgiIHR5cGU9ImltYWdlL2F2aWYiPgogIDxzb3VyY2Ugc3Jjc2V0PSIvYXNzZXRzL2ltZy9ncmFhbHZtLWJyYW5kLW5ldy1jb250YWluZXJzLTMwMC03ODYwMDk3MzUuanBnIDEuMHgsIC9hc3NldHMvaW1nL2dyYWFsdm0tYnJhbmQtbmV3LWNvbnRhaW5lcnMtNjAwLTc4NjAwOTczNS5qcGcgMi4weCIgdHlwZT0iaW1hZ2UvanBlZyI+CiAgPGltZyBzcmM9Ii9hc3NldHMvaW1nL2dyYWFsdm0tYnJhbmQtbmV3LWNvbnRhaW5lcnMtMzAwLTc4NjAwOTczNS5qcGciIGFsdD0iR3JhYWxWTSwgdGhlIGZ1dHVyZSBvZiBtaWNyb3NlcnZpY2VzIGFwcGxpY2F0aW9ucyBpbiBKYXZhIj4KPC9waWN0dXJlPgo=",
      "isencoded" : "y",
      "tags"     : "Java, GraalVM, Microservices",
      "content"  : "This article is the second in a series discussing the concepts introduced in GraalVM and how they are game-changing for using Java as a programming language in microservice architectures. GraalVM, a new generation of virtual machine What is that ? The architecture of GraalVM 1. The Grail compiler 2. The Truffle framework 3. The Virtual Machine 4. Runtimes So what ? GraalVM, a new generation of virtual machine The GraalVM ecosystem What is that ? GraalVM is a Virtual Machine (VM), Open Source, resulting from a research project at Oracle Labs. It is maintained by a community of major Internet players (Oracle, Amazon, Twitter, RedHat in particular with Quarkus, VMWare for the integration of its Spring framework, …). It is a new generation of VM, polyglot, that is to say that it supports many languages, even those which do not generate bytecode. Eventually, it could replace the current VM HotSpot. More details at https://www.graalvm.org/ In a few wordsThe GraalVM VM is coupled to a new compiler, Graal, written entirely in Java (which allows cyclic compilation): It aims to replace the C2 compiler used for the JIT of the HotSpot VM and which has reached the end of its life because it is too complex to upgrade (mix of assembler, C, Java ) The Grail compiler can also do AOT compilation (Ahead-Of-Time, in advance) also called advance compilation. The architecture of GraalVM The components of GraalVM 1. The Grail compiler It compiles in Bytecode languages for JVM. Integrated into the JVM, it is able to do JIT compilation. It can also do AOT compilation. 2. The Truffle framework It allows to describe, in the form of an Abstract Syntax Tree (AST or Syntax Tree), the grammar of languages unknown to the JVM. The Graal compiler, using the Truffle modules of each language, is able to interact with them. The Polyglot API, written in Java, allows messages to be passed from Java to other languages and to interact with them. Example of Truffle modules Graal.js defines the AST of JavaScript. Sulong implements the AST of the LLVM bitcode (C, C++, Rust, Objective-C, Fortran, Mono, …) Other modules being implemented: WebAssembly (for web applications), CUDA (for NVidia graphics cards), … 3. The Virtual Machine Two JVMs can be integrated into GraalVM as a Virtual Machine: By default, it is the JVM HotSpot which executes the bytecode of Java programs. But it is the Graal compiler which intervenes in the JIT compilation. You can also install the SubstrateVM virtual machine, included in the nativeimage module. It is this minimalist VM that offers the AOT compilation. 4. Runtimes They allow you to run languages other than those supported by the JVMs (the HotSpot JVM like the SubstrateVM). The node.js (ECMAScript compliant) and llvm runtimes are part of the standard installation. The other runtimes must be explicitly installed: the Python 3.7 interpreter, that of Ruby 2.6.5 or even of GNU R 3.6.1. The components of GraalVM: Core components: the JVM GraalVM, the Grail compiler, an LLVM runtime, a JavaScript runtime that supports Node.js Optional runtimes: Native Image which integrates the ahead-of-time (AOT) compiler LLVM toolchain Python interpreter Ruby interpreter R interpreter WebAssembly interpreter (Wasm) So what ? We are now entitled to wonder, how this is a game-changer for Java and microservices. Of course, we had to go through this theoretical presentation but already, we can understand the full potential of such an overhaul: A new, more efficient compiler, The ability to compile your application directly in native code, The ability to integrate multiple languages into its Java application, … It remains to be seen in practice the result. This is what we will do in the next article. Previous Article Java and microservices Next Article Compile a Spring application natively with GraalVM Cheers …"
    } ,
  
    {
      "title"    : "Java and microservices",
      "url"      : "/en/java-microservices/",
      "img"      : "PHBpY3R1cmU+CiAgPHNvdXJjZSBzcmNzZXQ9Ii9hc3NldHMvaW1nL2phdmEtbGlrZS1ydXN0eS1jb250YWluZXItMzAwLWFmOTZmZDA5Mi5hdmlmIDEuMHgsIC9hc3NldHMvaW1nL2phdmEtbGlrZS1ydXN0eS1jb250YWluZXItNjAwLWFmOTZmZDA5Mi5hdmlmIDIuMHgiIHR5cGU9ImltYWdlL2F2aWYiPgogIDxzb3VyY2Ugc3Jjc2V0PSIvYXNzZXRzL2ltZy9qYXZhLWxpa2UtcnVzdHktY29udGFpbmVyLTMwMC1lNmMwMDFkMzUuanBnIDEuMHgsIC9hc3NldHMvaW1nL2phdmEtbGlrZS1ydXN0eS1jb250YWluZXItNjAwLWU2YzAwMWQzNS5qcGcgMi4weCIgdHlwZT0iaW1hZ2UvanBlZyI+CiAgPGltZyBzcmM9Ii9hc3NldHMvaW1nL2phdmEtbGlrZS1ydXN0eS1jb250YWluZXItMzAwLWU2YzAwMWQzNS5qcGciIGFsdD0iSmF2YSBhbmQgbWljcm9zZXJ2aWNlcyI+CjwvcGljdHVyZT4K",
      "isencoded" : "y",
      "tags"     : "Java, Microservices",
      "content"  : "This article is the first in a series discussing the concepts of the new GraalVM virtual machine. We will discuss here basic notions on microservices, on Java and its JVM HotSpot. This will give us a first idea on the suitability of Java to the world of containers and especially microservices. What is a microservice? What is Java? How the JVM works The interpretation The Just-In-Time (JIT) compiler L’empreinte mémoire Architecture générale de la JVM Detail of 2 memory spaces How Java Frameworks work How do we do now? What is a microservice? We can, for the oldest among us, consider microservices as an extension of the concept of SOA (Service Oriented Architecture) with the advent of the cloud. A microservice can be characterized as: A service designed to manage a single feature (e.g. user management) Elastic, ie easily scalable. This involves rapid deployment, a lightweight service, all of this in a container, or even in the cloud. Automated, from the build process to deployment. Generally, it is maintained by a dedicated and small team, with automated tests to quickly deploy a new version. Microservice vs FaaSFaaS, for Functions-as-a-Service, goes a little further in this concept, adding the notions of serverless (the infrastructure necessary for the service is the responsibility of the supplier) and stateless (dear to functional programming, x -&amp;gt; f (x), always!) What is Java? Everyone knows Java! A language interpreted by a JVM and portable on different systems. However, let’s review a few notions about its internal functioning to fully understand its scope. How the JVM works The interpretation The stages of the interpretation of Bytecode by the JVM The JVM is an executable which reads bytecode and then interprets it. The bytecode is in .jar packages as .class files The JVM searches the .class file in the .jar packages, checks the .class file then loads it Once the bytecode is loaded, the JVM can execute it (semi-interpret it) Executing bytecode therefore has a cost: The bytecode is sought, verified then interpreted by the JVM which itself runs on the processor. The Just-In-Time (JIT) compiler The stages of the JIT optimization of Bytecode by the JVM     When executing a Java method, JIT’s C1 compiler (just-in-time, on the fly) will compile it to native code and the Profiler will start to collect information on its use. C1 is a light and fast compiler but it does not produce optimized native code.     When the profiler detects a widely used method, Hot, the C2 compiler will use information from the Profiler to produce native code, aggressive, optimized and very well suited to the context of use. C2 is a heavy and slow compiler but it produces very well optimized and very fast native code. There is actually a cycle between compilation C1 and C2. The C2 compiler will often recompile chunks of bytecode with new information from the profiler to produce an ever more optimal binary. After a while, when many pieces of bytecode have been compiled by the C2 compiler, the Java application will run very quickly. So it takes a heating time, warm-up, to a Java application to be fully responsive. This is a real problem for a microservice which must be able to be deployed and operational very quickly. L’empreinte mémoire Architecture générale de la JVM The different layers involved in a JVM When we look at the general architecture of a JVM, we can only see that there are a lot of components. We also see that its memory space is compartmentalized. Let’s focus on 2 of them. Detail of 2 memory spaces Details of 2 memory spaces of the JVM The JVM allocates memory for the application but also for its own metadata and its operation: The Java Heap stores instances of Java objects. It is divided into 2 parts: the Young Generation which contains the newly created objects and the Tenured Generation which contains items that have withstood the garbage collector (Garbage Collector). The Metaspace (formerly PermGen) contains class metadata (bytecode of methods, symbols, constant pools, annotations …). For a 10MB application, the JVM often occupies a size of 100MB. Again, this is a problem for a microservice which must have the smallest possible memory footprint. How Java Frameworks work It is now customary to embed several frameworks in a Java application in order to simplify certain technical aspects or to organize its application layers. Let us take the case of 2 frameworks, undoubtedly, the most used in the Java world: Spring and Hibernate. Here is what a Java application that uses these frameworks, will perform when it starts: Reading and parsing of configuration files, Complete class scan to retrieve metadata (annotations, accessors, …), Creation of a metamodel, Preparation of the reflection, Creation of proxies (lots of proxies!), … However, these are frameworks widely used by developers and, in reality, very well suited to monolithic applications. Java frameworks amplify the problems of startup time and memory consumption of the JVM. How do we do now? To summarizeWe have seen the problems of Java: High memory consumption Need for a warm-up time at start-up Native code optimization on the fly A priori, all that is not necessary for a microservice. So what do we do now? We forget about Java and we all get into C++ ?? None of that of course. The answer in the following article presenting GraalVM. And you will see that it moves! Next Article GraalVM, the future of microservices applications in Java Cheers…"
    } 
  
]
